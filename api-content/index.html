{"posts":[{"title":"向量数据库：AI 时代的创新引擎","content":"本期要点 初识向量数据库 案例引入，为什么需要有向量数据库 向量数据库的市场情况 向量数据库架构和实现 还有哪些应用场景 向量数据库的未来市场和技术展望 前置准备 什么是向量 类官方解释：向量是指在数学中具有一定大小和方向的量，文本、图片、音视频等非结构化数据， 通过机器学习/深度学习模型 Embedding 提取出来的“特征” 用数学中的向量来表示 与之相对应的，在机器学习领域，还有几个重要概念**(仅做对比了解)** 什么是特征向量 特征向量是包含事物重要特征的向量 例子1 大家比较熟知的一个特征向量是 RGB（红-绿-蓝）色彩，每种颜色都可以通过对红(R)、绿(G)、蓝(B)三种颜色的比例来得到，这样一个特征向量可以描述为：颜色 = [红，绿，蓝]。对于一个像素点，我们可以用数组 [255, 255, 255] 表示白色，用数组 [0, 0, 0] 表示黑色，这里 [255, 255, 255]、[0, 0, 0] 可以认为是该像素点的特征向量 例子2 如果我们想要区分小狗，很自然想到可以通过体型大小、毛发长度、鼻子长短等特征来区分。如下面这张照片按照体型排序，可以看到体型越大的狗越靠近坐标轴右边，这样就能得到一个体型特征的一维坐标和对应的数值，从 0 到 1 的数字中得到每只狗在坐标系中的位置 然而单靠一个体型大小的特征并不够，像照片中哈士奇、金毛和拉布拉多的体型就非常接近，我们无法区分。所以我们会继续观察其它的特征，例如毛发的长短 这样每只狗对应一个二维坐标点，我们就能轻易的将哈士奇、金毛和拉布拉多区分开来，如果这时仍然无法很好的区分德牧和罗威纳犬。我们就可以继续再从其它的特征区分，比如鼻子的长短，这样就能得到一个三维的坐标系和每只狗在三维坐标系中的位置 什么是Embedding 通过深度学习神经网络提取非结构化数据里的内容和语义，把图片、视频等变成特征向量，这个过程叫Embedding LLM输出的向量化是怎样的 这里以豆包最新的Doubao-embedding/text-240715向量模型和OpenAI第三代嵌入式向量模型text-embedding-3-small为例： 相似度计算 相似性计算方法 方法说明 内积（IP） 全称为 Inner Product，是一种计算向量之间相似度的度量算法，它计算两个向量之间的点积（内积），所得值越大越与搜索值相似。 欧式距离（L2） 全称为 Euclidean distance，指欧几里得距离，它计算向量之间的直线距离，所得的值越小，越与搜索值相似。L2在低维空间中表现良好，但是在高维空间中，由于维度灾难的影响，L2的效果会逐渐变差。 余弦相似度（COSINE） 余弦相似度（Cosine Similarity）算法，是一种常用的文本相似度计算方法。它通过计算两个向量在多维空间中的夹角余弦值来衡量它们的相似程度。所得值越大越与搜索值相似 内积（IP） 比较适合处理未归一化的数据或关注数据的大小和方向时 其中，a = (a1, a2,..., an) 和 b = (b1, b2,..., bn) ，是 n 维空间中的两个点。计算所得值越大，越与搜索值相似 两个 Embedding 向量间的 IP 距离计算公式： 余弦相似度（COSINE） 余弦相似度使用两组向量之间的夹角余弦来衡量它们的相似程度。可以将这两组向量想象成从相同起点 ([0,0,...]) 开始但指向不同方向的两条线段 要计算两组向量 A = (a(0), a(1),..., a(n-1)) 和 B = (b(0), b(1),..., b(n-1)) 之间的余弦相似度，计算公式： 余弦相似度始终在区间 [-1, 1] 内。例如，两个成比例的向量的余弦相似度为 1，两个正交向量的相似度为 0，两个相反向量的相似度为 -1。余弦值越大，表示两个向量之间的夹角越小，表明这两个向量彼此更相似。 反过来：通过将它们的余弦相似度从 1 中减去，也就可以得到两个向量之间的余弦距离 场景引入 prompt优化无法解决问题？ 在LLM的深入使用后，发现在某些场景prompt无论再怎样优化调教，仍然无法很好解决我们的问题，主要表现在 知识落后：涉及比较新的知识，或者是些特定专业领域的知识，大语言模型没有学习过，无法给到我们想要的答案，比如GPT-3.5的知识库截止日期是2021年9月 幻觉问题：大模型对于不了解的知识边界，对于欠缺知识的领域仍然尝试回答，容易造成错误回答(主打1个已读乱回)，当然这主要也受引入**“zero-shot”和本身的“Decoder-Only”**的Transformer架构 因此在这个问题背景下，业界提出了RAG(检索增强生成)技术 检索增强技术(RAG) Meta BLog: Streamlining the creation of intelligent natural language processing models 检索增强生成（RAG）是一种利用附加数据增强 LLM 知识的技术，使其能够在生成响应之前引用训练数据来源之外的权威知识库。相当于给大语言模型装上**“知识外挂”**。而其内部知识的修改方式也很高效，不需要对整个模型进行重新训练 业界表现：RAG 在 Natural Questions(opens in a new tab)、WebQuestions(opens in a new tab) 和 CuratedTrec 等基准测试中表现抢眼。用 MS-MARCO 和 Jeopardy 问题进行测试时，RAG 生成的答案更符合事实、更具体、更多样。FEVER 事实验证使用 RAG 后也得到了更好的结果 核心流程 这里基于LangChain实现一套RAG为例，LangChain是基于LLMs用于构建端到端语言模型应用的框架 加载：首先我们需要加载数据。这是通过 DocumentLoaders 完成的 分割：文本分割器将大文档分成更小的块。这对于索引数据和将其传递到模型都很有用，因为大块更难搜索并且不适合模型的有限上下文窗口 向量化：基于分割后的文本进行向量化处理，以便模型更好处理识别和召回 存储：我们需要某个地方来存储和索引我们的分割，以便以后可以搜索它们。这通常是使用 VectorStore 和 Embeddings 模型来完成的 检索：给定用户输入，使用检索器从存储中检索相关分割 生成：ChatModel / LLM 使用包含问题和检索到的数据的提示生成答案 模块实现 模块实现基于OpenAI最新API，国内大模型流程差不多 数据加载 基于langchain的Loader加载外部文件，如txt，当然也支持更多文本类型Loader，比如web链接的WebBaseLoader # 基于TextLoader加载外部文档，如.txt from langchain_community.document_loaders import TextLoader loader = TextLoader(&quot;./data/clue_faq.txt&quot;,encoding=&quot;utf8&quot;) faq = loader.load() 其中clue_faq.txt为清洗后的知识文档内容，比如 ... Q: 组件类型“智能电话”和“团购留资”是什么 A: &quot;智能电话&quot;是在留资组件中配置的商家联系电话、通过这种方式获取的线索会归类为智能电话，同样的，“团购留资”是指在团购中配置了“需要顾客留手机号”的订单上获取的客户联系电话 ... 文本分割 文档内容都比较长，类似ES分词，我们需要对文档进行切分后再向量化，存入向量数据库便于搜索 这里基于RecursiveCharacterTextSplitter进行文本分割 Embedding 切分文本片段后，即可通过向量模型对文本进行向量化，LLMs基本都会提供对应的向量模型 这里选用OpenAI最新的text-embedding-3-small from langchain.embeddings.openai import OpenAIEmbeddings embeddings = OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;) 数据存储(VectorDB) 文档转成向量后，需要作为向量存储至VectorDB，这样后续问题搜索时会将问题向量化后再从数据库中匹配相似向量的文本片段作为回答，以弥补LLM的知识盲区 这里以Meta提供的相似性搜索库FAISS为例： from langchain_community.vectorstores.faiss import FAISS vectors = FAISS.from_documents(documents, embeddings) 数据检索 现在需要考虑如何将原先的问题与加载后的文档进行串联，这里会构造1个documents_chain 关于更多Chain使用 &gt;&gt; from langchain_openai import ChatOpenAI from langchain.chains.combine_documents import create_stuff_documents_chain llm = ChatOpenAI( model=&quot;gpt-4o-mini&quot;, temperature=0.5 ) prompt_template = ChatPromptTemplate.from_template(&quot;&quot;&quot; 基于以下相关资料回答我的问题. ## 相关资料 {context} ## 我的问题 问题：{input} &quot;&quot;&quot;) document_chain = create_stuff_documents_chain(llm, prompt_template) 接下来的问题就是将向量数据库加入到检索流程 from langchain.chains import create_retrieval_chain retriever = vectors.as_retriever() # 创建新的调用链，为了将向量检索过程加入调用流中 rag_chain = create_retrieval_chain(retriever, document_chain) result = rag_chain.invoke({&quot;input&quot;: &quot;在线索经营中，组件类型“团购留资”是什么.&quot;}) print(result[&quot;answer&quot;]) 最终，来看看加入向量模型后，模型回答后的结果 业内市场 市场产品 随着生成式人工智能（GAI）应用以及大语言模型（LLM）的快速发展，在向量数据库如火如荼的市场中，各个服务商纷纷也推出了各自的解决方案 数据库排名网站 DB-Engines 列出了常见的一些向量数据库，包括专用的向量数据库和基于传统数据库的扩展功能 而目前向量数据库本质上有三种形态： 第一种是纯单机向量数据库，它不是分布式的(如上面提到的FAISS) 第二种是在传统数据库上加上一个具备向量检索能力的插件(如postgreSQL、mongoDB等) 第三种是独立的、专业的企业级向量数据库 各大厂商解决方案 腾讯云向量数据库自 2019 年开始内部研发，在今年不断升级和发展，多项核心性能得到提升，最高支持千亿级向量规模和 500 万 QPS 峰值能力，并与信通院一起联合 50 多家企业共同发布了国内首个向量数据库标准，推进向量数据库及大模型相关产业走向大规模应用。 阿里妈妈拥有自研的具有大规模、高性能、低成本且易开发优势的向量数据库 Dolphin VectorDB，在妈妈内容风控、营销知识问答、达摩盘人群 AI 圈人和 AI 经营分析师等场景中落地应用 |字节|产品概述--向量数据库VikingDB-火山引擎| |亚马逊|什么是向量数据库_向量数据库解决方案 - 亚马逊云科技| |阿里云|向量检索服务_向量搜索_大模型生成式检索_人工智能-阿里云| |腾讯云|cloud.tencent.com| |...|...| Milvus介绍 目前主流的开源的向量数据库Chroma、Milvus、Qdrant、Weaviate，这里以Milvus(star:28.5k)的架构实现为例 未开源的推荐Pinecone，主打卖点：支持排名跟踪、复杂搜索、数据去重 还有更多选型也可参考：2024年精选推荐的16个向量数据库:提升你的AI应用性能 - 掘金 Milvus在机器学习和数据科学领域获得了很高的声誉，在向量索引和查询方面拥有出色的能力。同时也是 LF AI &amp; Data 基金会 的毕业项目 利用功能强大的算法，Milvus提供闪电般的处理和数据检索速度以及GPU支持，即使在处理非常庞大的数据集时也是如此。 Milvus还可以与PyTorch和TensorFlow等其他流行的框架集成(当然也包括类似langChain这种LLM应用框架)，从而允许将其添加到现有的机器学习工作流中 Milvus架构 Milvus 是一款云原生向量数据库，采用存储与计算分离的架构设计，所有组件均为无状态组件，极大地增强了系统弹性和灵活性。整个系统架构分为四个层面： 接入层（Access Layer）。系统的门面，由一组无状态 proxy 组成。对外提供用户连接的 endpoint，负责验证客户端请求并合并返回结果 协调服务（Coordinator Service）。系统的大脑，负责分配任务给执行节点。协调服务共有四种角色，分别为 root coord、data coord、query coord 和 index coord 执行节点（Worker Node）。系统的四肢，负责完成协调服务下发的指令和 proxy 发起的数据操作语言（DML）命令。执行节点分为三种角色，分别为 data node、query node 和 index node 存储服务 （Storage）。系统的骨骼，负责 Milvus 数据的持久化，分为元数据存储（meta store）、消息存储（log broker）和对象存储（object storage）三个部分 操作对象 Database 与传统数据库引擎类似，可以在Milvus中创建数据库，并向某些用户分配权限来管理它们。其中一个 Milvus 集群最多支持 64 个数据库 from pymilvus import connections, db conn = connections.connect(host=&quot;127.0.0.1&quot;, port=19530) database = db.create_database(&quot;business_db&quot;) Schema 字段模式是字段的逻辑定义。因此在定义集合模式和管理集合之前需要先给定各字段的逻辑定义 from pymilvus import FieldSchema id_field = FieldSchema(name=&quot;id&quot;, dtype=DataType.INT64, is_primary=True, description=&quot;primary id&quot;) age_field = FieldSchema(name=&quot;age&quot;, dtype=DataType.INT64, description=&quot;age&quot;) embedding_field = FieldSchema(name=&quot;embedding&quot;, dtype=DataType.FLOAT_VECTOR, dim=128, description=&quot;vector&quot;) position_field = FieldSchema(name=&quot;position&quot;, dtype=DataType.VARCHAR, max_length=256, is_partition_key=True) 定义完成字段模式后，接下来需要定义集合模式collection schema(类似Create Table DML) from pymilvus import FieldSchema, CollectionSchema ... # 设置collection schema schema = CollectionSchema(fields=[id_field, age_field, embedding_field], auto_id=False, enable_dynamic_field=True, description=&quot;desc of a collection&quot;) Collections 生成的vector会嵌入存储在集合中。集合中的所有vector嵌入共享相同的维度和距离度量，从而用于测量相似性 在很多情况下大多数开发者没有过多的定制化诉求，只需要一个简单而动态的集合来开始，因此提供两种创建方式 通过官方包 MilvusClient 创建。 定制设置，也就是基于上面指定schema的方式 from pymilvus import MilvusClient, DataType # 实例化客户端，连接 Milvus 服务 client = MilvusClient( uri=&quot;http://localhost:19530&quot; ) # 方式一：快速创建1个collection client.create_collection( collection_name=&quot;demo_v2&quot;, dimension=5 ) # 方式二： 定制设置 # 1. 创建schema schema = MilvusClient.create_schema( auto_id=False, enable_dynamic_field=True, ) # 2. 定义schema中的字段 schema.add_field(field_name=&quot;my_id&quot;, datatype=DataType.INT64, is_primary=True) schema.add_field(field_name=&quot;my_vector&quot;, datatype=DataType.FLOAT_VECTOR, dim=5) # 3. 创建集合 client.create_collection( collection_name=&quot;consult_faq&quot;, schema=schema, index_params=index_params ) Index Milvus 提供多种索引类型来对字段值进行排序，以实现高效的相似性搜索。同时它还提供三种度量类型：余弦相似度 (COSINE)、欧几里得距离 (L2) 和内积 （IP）来测量向量嵌入之间的距离 索引类型 如何选型 索引类型 使用场景 适用向量规模 召回率 检索速度 写入速度 FLAT 暴力检索，召回率100%，但检索效率低。 10万以内 最高，可保证100%召回率 慢 慢 HNSW 基于图算法构建索引，可通过调整检索参数提升召回率。具体信息，请参见 配置索引参数。检索效率高，但数据量大后写入效率会变低 10万-1亿 95%+，可根据参数调整 快 慢 IVF系列 基于聚类算法构建的索引，可通过参数调整召回率，适用于上亿规模的数据集，检索效率高，内存占用低，写入效率高。 1亿以上 95%+，可根据参数调整 快 快（批量写入后统一构建索引） 索引使用 索引参数决定 Milvus 如何组织集合中的数据。我们可以通过调整特定字段的 metric_type 和 index_type 来设置特定字段的索引过程。对于矢量，可以灵活选择COSINE、L2或IP作为metric_type # 1. 设置索引的参数 index_params = MilvusClient.prepare_index_params() # 创建schema &amp; collection(同Collection) schema = MilvusClient.create_schema( auto_id=False, enable_dynamic_field=True, ) schema.add_field(field_name=&quot;my_id&quot;, datatype=DataType.INT64, is_primary=True) schema.add_field(field_name=&quot;my_vector&quot;, datatype=DataType.FLOAT_VECTOR, dim=5) client.create_collection( collection_name=&quot;consult_faq&quot;, schema=schema, index_params=index_params ) # 2. 在向量字段 vector 上面添加一个索引 index_params.add_index( field_name=&quot;my_vector&quot;, metric_type=&quot;COSINE&quot;, index_type=&quot;IVF_FLAT&quot;, index_name=&quot;vector_index&quot;, params={ &quot;nlist&quot;: 128 } ) # 3. 为集合添加索引(类似向表加索引) client.create_index( collection_name=&quot;consult_faq&quot;, index_params=index_params ) # 4. 查看collection索引信息 index_info = client.list_indexes( collection_name=&quot;consult_faq&quot; ) 变量索引分类 auto-index： Milvus 根据标量字段的数据类型自动决定索引类型。这适用于不需要控制具体索引类型的情况 变量字段就是除 vector 字段，id 字段之外的字段。在 Milvus 中，标量索引用于加速特定非向量字段值的元过滤，可以理解为传统的数据库索引 custom-index： 可以指定明确的索引类型，比如倒排索引。这就提供了对索引的类型的更多选择 # 这里以自定义索引类型为例 index_params = client.create_index_params() # 准备一个 IndexParams 对象 index_params.add_index( field_name=&quot;scalar_1&quot;, # 标量字段名称 index_type=&quot;INVERTED&quot;, # 明确索引类型 index_name=&quot;inverted_index&quot; # 索引的名称 ) client.create_index( collection_name=&quot;consult_faq&quot;, # 将索引添加到集合中 index_params=index_params ) Partitions Milvus中的分区代表集合的子分区。此功能允许将集合的物理存储分为多个部分，通过将焦点缩小到较小的数据子集而不是整个集合，有助于提高查询性能。 创建集合后，至少会自动创建一个名为**_default的默认分区。您可以在一个集合中最多创建4,096**个分区 向量搜索 更多向量搜索api，参考官方文档: Search、Query&amp;Get 结合以上操作对象，写入一批量数据后，就可以基于创建好的索引进行向量搜索 Milvus 支持两种类型的搜索，具体取决于集合中向量字段的数量 单向量搜索：如果您的集合只有一个向量字段，请使用search()方法查找最相似的实体。此方法将您的查询向量与集合中的现有向量进行比较，并返回最接近匹配的 ID 以及它们之间的距离。或者，它还可以返回结果的向量值和元数据。 多向量搜索：对于具有两个或多个向量场的集合，请使用hybrid_search()方法。此方法执行多个近似最近邻 (ANN) 搜索请求，并组合结果以在重新排名后返回最相关的匹配项 这样可以实现应用场景中最常用的搜索诉求 基本搜索：包括单向量搜索、批量向量搜索、分区搜索和指定输出字段搜索 过滤搜索： 应用基于标量字段的过滤条件来细化搜索结果 范围搜索： 查找距查询向量特定距离范围内的向量 分组搜索： 根据特定字段对搜索结果进行分组，以确保结果的多样性 # 分区搜索：指定返回和过滤标量 res = client.search( collection_name=&quot;consult_faq&quot;, # 集合名称 data=[[0.02174828545444263, 0.058611125483182924, 0.6168633415965343, -0.7944160935612321, 0.5554828317581426]], # 关键字的vector，比如&quot;灰色夹克&quot; limit=5, # 返回的搜索结果最大数量，top5 search_params={&quot;metric_type&quot;: &quot;IP&quot;, &quot;params&quot;: {}}, # 相似性搜索的度量类型：IP(内积) partition_names=[&quot;partition_1&quot;] # 这里指定搜索的分区以缩小集合分区 output_fields=[&quot;name_field&quot;] # 返回定义的字段 filter='color like &quot;gree%&quot;' # 模糊搜索过滤 ) 应用场景 模型知识库 这个场景其实就是上面「场景引入」的经典应用场景，目前很多LLM的知识库应用无不都是基于VectorDB来实现RAG的，这里推荐几个github比较受欢迎的基于 LLM 大语言模型的开源知识库问答系统 名称 项目地址 star数 dify https://github.com/langgenius/dify (LLM应用开发平台，类似字节的扣子) 39.2k FastGPT https://github.com/labring/FastGPT 15.8k MaxKB https://github.com/1Panel-dev/MaxKB?tab=readme-ov-file 8.5k 推荐系统 得益于企业级向量数据库的快速发展，在系统推荐场景下主要应有 搜索场景：试想下，在抖音搜索“灰色夹克”，得到的搜索结果基本都是与之相关的商品，结合向量的相似性搜索，推荐系统可以更好更准确地做信息推荐，这种比较适合推荐系统的冷启动阶段 大致的实现方式，可以参考我以前实现的demo：colab地址 用户推荐：将用户行为特征向量化存储在向量数据库。比如用户经常浏览数码产品，这种行为特征就可以作为特征向量。当发起推荐请求时，系统会基于用户特征进行相似度计算，最终筛选用户可能感兴趣的物品推荐给用户 文本图像检索 向量数据库可以存储大量的图像向量数据，并通过向量索引技术实现高效相似度计算，返回与检索图像最相似的图像结果 未来展望 ** 现状 ** 在国内，随着数字化转型的加速和人工智能应用的普及，对于高质量、高效率数据处理工具的需求日益旺盛。众多企业纷纷加大在人工智能领域的投入，加速推动了向量数据库在国内市场的发展。 融资规模 23年4月，向量数据库平台 Pinecone 获得 1 亿美元 B 轮融资，估值达到 7.5 亿美元 继 2023 年 4 月完成 750 万美元种子轮融资后，开源向量数据库公司 Qdrant 24年初完成 2800 万美元的 A 轮融资 Chroma 在23年4月宣布获得1800万美元的种子轮融资 市场展望 东北证券预测，到 2030 年，全球向量数据库市场规模有望达到 500 亿美元，国内向量数据库市场规模有望超 600 亿人民币 以 ChatGPT 为代表的生成式人工智能的快速发展，而模型的训练效果与数据源的质量和数量相关性愈发明显，因此数据成为 AIGC 应用产品的核心竞争壁垒之一和兵家必争之地 而向量数据库作为专门处理高维向量数据的重要工具，使得对于高效处理和存储大规模向量数据的需求急剧增加。各种新兴的应用场景，如计算机视觉、自然语言处理等领域，都对向量数据库的性能和功能提出了更高的要求，从而推动了市场规模的持续扩大 技术展望 随着AI技术的不断发展和大数据时代的来临，向量数据库将会迎来更多的应用场景和挑战(当然也是机会)，这其中就包括 扩展性：大模型的兴起，对嵌入（embedding）和向量化这些能力的需求急剧增加。大模型的普及也让向量数据的规模不断增大，从百万级别的数据体量已经变为千万级别，甚至更大。这就需要数据库能够有效地支持大规模向量数据的存储和检索，这对硬件资源提出了更高的要求，特别是在云上部署时成本可能成为一个重要问题 成本问题：在向量搜索中，索引的大小和存储是关键因素，而向量索引的成本通常较高。以前在数据量较小的情况下，可能只需要几台机器就足够了，成本并不是关键问题。但随着数据规模的增大，需要更多的资源来支持，这就涉及到成本的考虑 易用性问题：与传统的关系型数据库不同，向量搜索涉及到更多维度的考量，包括性能和召回率等。为了平衡性能和召回率，需要调整各种参数，但这可能对用户来说不太友好。因此，简化参数选择，优化用户体验是一个重要的挑战 就以上问题，讨论比较多的优化方向 边缘计算的支持：随着云计算和边缘计算的兴起，向量数据库也将会更加注重分布式处理和边缘计算的支持 一体化趋势：特别在“降本增效”的大环境下，目前，出现了单机分布式一体化、在离线一体化、多模态一体化。一体化技术使得数据库具备更强的适应性，并且能极大地降低用户使用和运维管理的复杂度。尤其在多模态技术方向上，通过对非结构数据向量化，也实现了多样性的数据检索管理能力。还有就是通过整合不同的数据库技术，实现一体化管理，也可以提高数据得处理效率 ","link":"https://GeekGhc.github.io/post/xiang-liang-shu-ju-ku-ai-shi-dai-de-chuang-xin-yin-qing/"},{"title":"Go 1.18 Feature应用分享","content":"开篇 终于在上个月Go官方发布了Go1.18，除了大家心心念念的泛型新的参数支持，还带来很多新的特性，在这一个月的体验感受下来，新版本对语言还是可以说做了有史以来最大的改变，无论是性能发挥还是特性支持都得到了较高的满意度 特性介绍 泛型 基本现状 在此之前版本很多时候如果想做一些通用数据类型的方法和操作时，基本都需要借助interface实现，有的内部甚至引入各种类型断言或者通过反射reflect机制实现 比如想实现基本快速排序 func quickSort(sequence []int64) { // sort logic } 这个方法的参数只能是int64的切片类型，如果需要实现通用化参数的方法，常见思路就是借助interface // 通用参数类型排序 func quickSort(sequence []interface{}) { switch sequence[0].(type) { case int64: // int64类型 logic case float64: // float类型 logic // other type default: panic(&quot;type not support!&quot;) } 另一种常见解决思路就是借助reflect反射来编写泛型函数，但是问题是这样不仅执行速度慢而且需要显示的类型断言，并且没有静态类型检查 泛型玩法 早在去年年底发布了1.18的beta版本里就开始启用泛型支持，很多人已经提前体验了一番。现在已经支持社区大多数用户的泛型需求，算是正式支持generic编程 泛型教程：https://go.dev/doc/tutorial/generics 首先Go支持泛型函数和泛型类型，还是基于上面的case 泛型函数 // 定义标准的泛型函数模板 // 其中[T any]即为参数列表，T为类型参数，any则为参数约束 func quickSort[T any](args T) { // sort logic } // 实际调用 quickSort[int]([]int{1,2,5,6}) // any 实际定义 V1.18.1 // https://github.com/golang/go/blob/go1.18.1/src/builtin/builtin.go#L95 type any = interface{} 再来实现一个经典的多类型相加 func add[T any](a, b T) T { return a + b } 但是会发现报错提示：Invalid operation: a + b (the operator + is not defined on []T) 这是因为问题出现在any这个参数约束，和C++乃至Java中类似，T受限于数值的运算符操作，以此对一些不支持的类型进行规避 // 修改为想要的数值比较类型 func add[T int64 | float64](a, b T) T { return a + b } 类型别名 type SelfInt int64 func add[T ~int64 | float64](a, b T) T { // ~限制参数底层实现的某种具体类型的别名 return a + b } add[SelfInt](2, 4) 泛型类型 除了对方法的泛型抽象，也可以定义符合数据类型的结构，以常见链表节点为例 type DataType interface { // 接口实现类型约束 int64 | float32 | string } type Element[V DataType] struct { key string value V // 节点类型只能为DataType指定的int64&amp;float32&amp;string } Go 1.18将移除用于泛型的类型约束constraints包，主要原因是很多约束类型使用场景太少，基本围绕any和comparable这2个类型约束足够 悲伤故事 尽管已经支持了泛型函数和泛型类型，但是在Go的泛型提案中：no-parameterized-methods也表示并不会支持泛型方法 主要原因Go泛型的处理是在编译的时候实现的，泛型方法在编译的时候，如果没有上下文的分析推断，很难判断泛型方案该如何实例化，甚至判断不了 最终导致目前Go 1.18实现中不支持泛型方案，不过是可以期待下在之后的版本会支持上 详细说明可以看下 https://colobu.com/2021/12/22/no-parameterized-methods/ 泛型库应用 Lodash 泛型工具库 &gt;&gt; https://github.com/samber/lo 模糊测试 基本介绍 模糊测试是一种自动化测试，通过不断创建输入以检查输出结果是否符合预期。作为常见单测的补充。单测往往是对静态输入的最终预期结果验证，而模糊测试通常也更擅长发现程序安全漏洞问题 发展历程 关于模糊测试的issue问题还在持续迭代 &gt;&gt; 2015年在GopherCon上Google工程师Dmitry Vyukov介绍了相关第三方解决方案： go-fuzz (4.4kstar)已实现了相关功能 2016年Dmitry Vyukov在Go官方issue列表中创建“cmd/compile: coverage instrumentation for fuzzing”的issue来说明三方工具无法实现的问题，并开始推动Fuzzing进入Go原生工具链 而在新版本中基本也是借鉴了实现思路，从而集成到标准工具链的testing pkg 2021年在官方正式进行Fuzzing提案，提议为 Go 添加模糊测试支持， issue &gt;&gt; 2022年3月发布Go1.18将fuzz testing纳入了go test工具链，与单元测试、性能基准测试等一起成为了Go原生测试工具链中的重要成员 基本规则 方法介绍 f.Add函数把指定输入作为模糊测试的种子语料库(seed corpus)，fuzzing基于种子语料库生成随机输入 f.Fuzz函数接收一个fuzz target函数作为入参。fuzz target函数有多个参数，第一个参数是*testing.T，其它参数是被模糊的类型(注意：被模糊的类型目前只支持部分内置类型, 列在 Go Fuzzing docs，未来会支持更多的内置类型) 官方给出的模糊测试基本规则 模糊测试必须是一个名称类似FuzzXxx的函数，仅仅接收一个*testing.F类型的参数,没有返回值 模糊测试必须在*_test.go文件中才能运行 Fuzz target(模糊目标)必须是对(testing.F).Fuzz的方法调用，参数是一个函数，并且此函数的第一个参数是testing.T,然后是模糊参数(fuzzing argument)，没有返回值 一个模糊测试中必须只有一个模糊目标 所有的种子语料库(seed corpus)必须具有与模糊参数相同的类型,顺序相同。对(*testing.F).Add的调用也是如此, 同样也适用模糊测试中的testdata/fuzz中的语料文件 模糊参数只能是下面的类型 string, []byte int, int8, int16, int32/rune, int64 uint, uint8/byte, uint16, uint32, uint64 float32, float64 bool 场景玩法 编写一个简易的字符串反转函数 // 字符串反转 func Reverse(s string) string { b := []byte(s) for i, j := 0, len(b)-1; i &lt; len(b)/2; i, j = i+1, j-1 { b[i], b[j] = b[j], b[i] } return string(b) } 单元测试 func TestReverse(t *testing.T) { testCases := map[string]string{ &quot;Hello&quot;: &quot;olleH&quot;, &quot;ByteDance&quot;: &quot;ecnaDetyB&quot;, } for in, out := range testCases { rev := Reverse(in) if rev != out { t.Errorf(&quot;rev:%s, expecte:%s&quot;, rev, out) } } } 模糊测试 func FuzzReverse(f *testing.F) { testCases := []string{&quot;Hello&quot;, &quot;World&quot;, &quot;ByteDance&quot;} for _, tc := range testCases { f.Add(tc) // 指定输入作为种子语料库 } f.Fuzz(func(t *testing.T, orig string) { rev := Reverse(orig) doubleRev := Reverse(rev) if orig != doubleRev { // check t.Errorf(&quot;Before: %q, after: %q&quot;, orig, doubleRev) } }) } 使用种子语料库 // 可以理解为mock后的单测 $ go test -run=FuzzReverse PASS ok 基于种子语料库生成随机测试数据 // 新增-fuzz参数 $ go test -fuzz=FuzzReverse 这次测试失败了，而失败的输入case则入了testdata以方便问题排查 打开后是这样的 go test fuzz v1 // 语料库的编码版本，目前是v1 string(&quot;Ғ&quot;) // 引入测试错误的输入数据 代码用例修复 错误的原因则是因为待测试的Reverse函数是基于字节(byte)进行的反转，因此对于本身就占用多字节的字符就会出现无效字符，常见的就比如中文字符 因此思路也很简单按照rune进行字符反转以得到有效的UTF-8编码的字符串 修改后指定错误用例继续测试 ok 但是继续用新的随机测试数据会发现新的问题 原因也是因为非法的unicode数据输入导致编码字节切片后无法还原，限定输入合法最终即可通过 // 修改后测试函数 func Reverse(s string) (string, error) { if !utf8.ValidString(s) { return s, errors.New(&quot;input is not valid UTF-8&quot;) } r := []rune(s) for i, j := 0, len(r)-1; i &lt; len(r)/2; i, j = i+1, j-1 { r[i], r[j] = r[j], r[i] } return string(r), nil } 定制化参数 上面的默认测试已经可以满足大部分模糊测试场景，不过官方还是提供了其他定制参数 fuzztime: 执行的模糊目标在退出的时候要执行的总时间或者迭代次数，默认是用不结束 fuzzminimizetime: 模糊目标在每次最少尝试时要执行的时间或者迭代次数，默认是60秒。你可以禁用最小化尝试，只需把这个参数设置为0 parallel: 同时执行的模糊化数量，默认是$GOMAXPROCS即调用器Processor数量。当前进行模糊化测试时设置-cpu无效果 架构原理 gofuzz 是一个多进程的fuzzer，其组件可分为协调进程Coordinator、工作进程Worker和RPC Coordinator Coordinator的职责是运行和唤醒工作进程、命令工作进行去fuzz下一个输入、如果发生crash则将interesting data 写入语料库等，实现代码 &gt;&gt; CoordinateFuzzingOpts &gt;&gt; // 定义了 CoordinateFuzzing 的一系列参数 type CoordinateFuzzingOpts struct { Log io.Writer Timeout time.Duration // 语料库加载后的挂钟时间 Limit int64 // 生成的随机值数量 MinimizeTimeout time.Duration MinimizeLimit int64 Parallel int // 并行运行的工作进程数，对应执行参数parallel Seed []CorpusEntry // 测试种子列表，对应f.Add() Types []reflect.Type // 语料库条目类型列表 CorpusDir string // 写入包含使正在测试的代码崩溃的值的文件 CacheDir string // interesting data的目录 } CoordinateFuzzing &gt;&gt; CoordinateFuzzing函数基于传入的CoordinateFuzzingOpts一系列参数管理多个worker进程的生命周期，如果遇到测试崩溃则返回崩溃信息以及确保crash后写入语料库 Coordinator &gt;&gt; 定义Coordinator与worker进程的channel,比如如coordinator传递fuzz数据到worker的channel inputC以及worker传递fuzzing结果到coordinator的channel resultC等 Worker worker的功能主要包括种子变异、最小化、运行fuzz函数、收集覆盖率、返回crash等 工作流程 执行go -test -fuzz=xx 启动模糊测试 Coordinator唤醒工作进程并分配worker对象 Coordinator从种子语料库和缓存语料库选择输入来进行模糊测试 workerClient作为RPC客户端调用worker进程的方法 RPC基于workerComm提供进程间通信的管道和共享内存 workerServer作为RPC服务端处理请求任务 fuzzIn()读取序列化后的RPC消息 fuzz()则在共享内存中根据随机输入在有限的持续时间或迭代次数内来运行测试函数 遇到crash则返回 覆盖率&amp;最小化 gofuzz采用覆盖率反馈的方式引导fuzzing，Go 编译器已经对libFuzzer(覆盖引导的模糊测试库)提供了检测支持 ，所以在gofuzz中重用了该部分。 编译器为每个基本块添加一个 8 位计数器用来统计覆盖率 实现思路 当coordinator接收到产生新覆盖范围的输入时，它会将该worker进程的覆盖范围与当前组合的覆盖范围数组进行比较： 如果另一个worker进程已经发现了提供相同覆盖范围的输入，则把该输入丢弃。 如果新的输入确实提供了新的覆盖，则coordinator将其发送回worker进程(可能是不同的worker）以进行最小化处理 coordinator收到导致错误的输入时，它会再次将输入发送回worker进程以进行最小化。在这种情况下会尝试找到仍然会导致错误的较小输入。输入最小化后，将其保存到 testdata/corpus/$FuzzTarget最终退出 最小化实现代码 &gt;&gt; 变异 为了达到模糊测试的效果，对于其支持的基本类型会进行变异操作，如int类型，通过加上或减去一个随机数，并判断其变异后的返回值不能超高int支持的最大范围以此来不断产生新的输入用例提升覆盖 变异实现代码 &gt;&gt; References https://go.dev/blog/go1.18 https://go.dev/blog/intro-generics 官方教程：如何开始使用泛型 Go语言单元测试最佳实践手册 https://colobu.com/2021/08/30/how-is-go-generic-implemented/ https://colobu.com/2022/01/03/go-fuzzing/ https://colobu.com/2021/12/22/no-parameterized-methods/ https://rakyll.org/generics-facilititators/ https://tip.golang.org/doc/fuzz/ ","link":"https://GeekGhc.github.io/post/go-118-feature-ying-yong-fen-xiang/"},{"title":"Ubuntu 搭建k8s集群","content":"简介 本文主要介绍基于Ubuntut通过官方社区推出的快速部署kubeneters集群的工具kubeadm来快速搭建k8s集群。 官方地址：https://kubernetes.io/docs/reference/setup-tools/kubeadm/ 安装条件 在开始之前，部署K8s集群需要做以下环境准备 准备两台虚拟机，这里我用的Ubuntu20.0.5LTS版本 集群角色 服务IP 主机名 控制节点 192.168.32.32 master-k8s 工作节点 192.168.32.32 worker-k8s 机器环境准备 网络配置 这里假设用的是一个新的Liunx Ubuntu环境 首先为虚拟机设置固定服务IP，这里取决于虚拟机工具设置的共享网络&quot;Host-only&quot;,这里需要新增网络配置 这里可以参考在Parellels的设置 接着在两个虚拟机中设置固定网络 比如在Master节点的机器上配置网络Method为Manual 最终通过ip addr查看两台机器网卡ip信息 软件安装 网络配置后，安装基本的软件 sudo apt update sudo apt install -y git vim curl jq 开始安装并启动Docker sudo apt install -y docker.io #安装Docker Engine sudo service docker start #启动docker服务 另外需要把当前的用户加入 Docker 的用户组，这是因为操作 Docker 必须要有 root 权限，而直接使用 root 用户不够安全，加入 Docker 用户组是一个比较好的选择，这也是 Docker 官方推荐的做法 sudo usermod -aG docker ${USER} #当前用户加入docker组 执行成功后退出系统重新登录使修改的用户组生效 最终通过执行docker version查看系统信息 如果遇到Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock，refer https://linuxhandbook.com/docker-permission-denied/ 退出系统重新登录即可 Docker完成安装后需要对配置进行一点修改，在“/etc/docker/daemon.json”里把 cgroup 的驱动程序改成 systemd ，然后重启 Docker 的守护进程。 daemon.json 内容如下 { &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;100m&quot; }, &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;registry-mirrors&quot;: [&quot;https://6kx4zyno.mirror.aliyuncs.com&quot;,&quot;https://registry.docker-cn.com&quot;] } 修改完成后重启docker daemon sudo systemctl enable docker sudo systemctl daemon-reload sudo systemctl restart docker 为了让 Kubernetes 能够检查、转发网络流量，你需要修改 iptables 的配置，启用“br_netfilter”模块： cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 # better than modify /etc/sysctl.conf EOF sudo sysctl --system 接着你需要修改“/etc/fstab”，关闭 Linux 的 swap 分区，提升 Kubernetes 的性能： sudo swapoff -a sudo sed -ri '/\\sswap\\s/s/^#?/#/' /etc/fstab 安装kubeadm 现在我们就要安装 kubeadm 了，在 Master 节点和 Worker 节点上都需要这么做 kubeadm 可以直接从 Google 自己的软件仓库下载安装，但国内的网络不稳定，很难下载成功，需要改用其他的软件源 sudo apt install -y apt-transport-https ca-certificates curl curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt update 更新完成后，就可以开始下载kubeadm相关的kubeadm、kubelet 和 kubectl工具，默认会下载最新版本，这里指定固定版本 #查看有哪些版本 sudo apt-cache madison kubeadm #指定版本 sudo apt install -y kubeadm=1.23.3-00 kubelet=1.23.3-00 kubectl=1.23.3-00 如果需要防止意外升级工具版本，可以锁定相关软件的版本 sudo apt-mark hold kubeadm kubelet kubectl 查看安装信息 kubeadm version Kubernetes 组件镜像 首先我们需要查看安装kubernetes所需要的镜像列表，其中参数--kubernetes-version可以指定版本号 kubeadm config images list --kubernetes-version v1.23.3 k8s.gcr.io/kube-apiserver:v1.23.3 k8s.gcr.io/kube-controller-manager:v1.23.3 k8s.gcr.io/kube-scheduler:v1.23.3 k8s.gcr.io/kube-proxy:v1.23.3 k8s.gcr.io/pause:3.6 k8s.gcr.io/etcd:3.5.1-0 k8s.gcr.io/coredns/coredns:v1.8.6 接着就需要下载k8s相关镜像了，因为kubeadm把apiserver、etcd、scheduler相关组件都打包成了镜像，并以容器的方式启动kubernetes 但是这些镜像的还是在Google自己的仓库，因为众所周知的原因，我们可以从国内镜像网站下载这些镜像 # 镜像下载执行脚本 repo=registry.aliyuncs.com/google_containers for name in `kubeadm config images list --kubernetes-version v1.23.3`; do src_name=${name#k8s.gcr.io/} src_name=${src_name#coredns/} docker pull $repo/$src_name docker tag $repo/$src_name $name docker rmi $repo/$src_name done 下载完成后看下相关镜像信息即可 docker image list 集群配置 基本的软件准备工作完成后，就可以开始进行k8s集群配置了 Master节点 基于kubeadm只需要kubeadm init命令就可以把组件在Master节点上运行起来，不过在这过程中有很多参数来进行集群配置。更多参数可以使用 -h 查看 这里我们主要需要以下参数 --pod-network-cidr，设置集群里 Pod 的 IP 地址段。 --apiserver-advertise-address，设置 apiserver 的 IP 地址，对于多网卡服务器来说很重要（比如 VirtualBox 虚拟机就用了两块网卡），可以指定 apiserver 在哪个网卡上对外提供服务。 --kubernetes-version，指定 Kubernetes 的版本号。 那么在Mater节点上我们执行 sudo kubeadm init \\ --pod-network-cidr=10.244.0.0/16 \\ --apiserver-advertise-address=192.168.32.32 \\ --kubernetes-version=v1.23.3 最终的执行结果 值得注意的是，最后有个重要的执行命令表示其他节点要加入集群必须要用指令里的 token 和 ca 证书 kubeadm join 192.168.32.32:6443 --token 9xx5kx.dyjebdab12bukhja \\ --discovery-token-ca-cert-hash sha256:a7f1d7794e90796ad0a1792fa392e67ce7cd16bbddeb52bb5a3d43a630305c0a 若果没有记录下来可以使用以下命令查看: kubeadm token create --print-join-command 安装过程中也会提示接下来所需要执行的操作 To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 安装完成后，你就可以使用 kubectl version、kubectl get node 来检查 Kubernetes 的版本和集群的节点状态了 kubectl version kubectl get node 这时候发现集群节点还是NotReady的状态，因为还缺少网络插件，集群的内部网络还没有正常运作 安装Flannel网络插件 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 稍等会儿，再使用kubectl get node查看下，发现节点已经是Ready状态 Worker 节点 如果你成功安装了 Master 节点，那么 Worker 节点的安装就简单多了，只需要用之前拷贝的那条 kubeadm join 命令就可以了，记得要用 sudo 来执行 kubeadm join 192.168.32.32:6443 --token 9xx5kx.dyjebdab12bukhja \\ --discovery-token-ca-cert-hash sha256:a7f1d7794e90796ad0a1792fa392e67ce7cd16bbddeb52bb5a3d43a630305c0a 它会连接 Master 节点，然后拉取镜像，安装网络插件，最后把节点加入集群 现在让我们用 kubectl run ，运行 Nginx 来测试一下： kubectl run ngx --image=nginx:alpine kubectl get pod -o wide 可以看到Pod成功运行在了workre节点上 ","link":"https://GeekGhc.github.io/post/ubuntu-da-jian-k8s-ji-qun/"},{"title":"机器学习知识库","content":"[whimsical 知识图谱]https://whimsical.com/G4ea2GT8y6dN4E4tWT4YAi ","link":"https://GeekGhc.github.io/post/ji-qi-xue-xi-zhi-shi-ku/"},{"title":"TiDB 演进与架构分析","content":"基本介绍 分布式的产生 随着数据量和计算量的增长，集中式系统已无法承载，此时分布式系统则就是数据爆发增长的刚需。分布式本质上也是进行数据和计算的分制，再结合多副本的机制满足了可用性。最终展现出分布式系统的价值。 纵观分布式的发展历史，Google在06年推出了大数据的三驾马车(GFS、BigTable、MapReduce)。其中GFS解决了分布式文件系统问题，BigTable解决了分布式KV存储问题。MapReduce则解决了在之前两者的基础上如何做分布式计算和分析的问题。 数据库现状 如今各种模型的数据库非常多， 而关系型模型在在线交易场景一直占据很大的权重(72%)。而作为关系型数据库就一定会具备事务，事务的本质就是并发控制单元(操作序列作为工作单位不可分割) 。这里也就涉及了常见的四大特性ACID A原子性：事务包含的整体操作是不可分割 C一致性：事务的前后，所有的数据都保持一致的状态，不可违反数据的一致性检测 I隔离性：事务之间相互影响的程度，多个事务访问同一资源的行为在不同的隔离性下不同 D持久性：事务完成后将数据变更的记录存储下来，包括了数据本身和多副本的备份 所以有没有既是关系型数据库模型又能发挥分布式系统的机制的数据呢，这就是如今的NewSQL数据库也就是原生分布式关系型数据库(分布式+SQL+事务) NewSQL: 是一类关系数据库，它寻求为在线交易处理(OLTP)工作提供NoSQL系统的可扩展性，同时维护传统数据库系统的ACID保证 主角亮相 TiDB作为NewSQL分类的代表是一款PingCAP公司设计，兼容MySQL协议，底层采用RocksDB作为单机存储，通过Raft协议保障一致性高可用的分布式关系型数据库，同时支持在线事务处理，适合高可用、强一致要求较高、数据规模较大等各种应用场景。 NewSQL设计目标 在我们熟悉的面向业务场景中，可以总结出这么NewSQL需要支持以下几种能力： 扩展性 强一致高可用性 标准SQL支持ACID事务 云原生 混合数据服务(HTAP) 解决数据容量的前提下，OLTP &amp; OLAP的数据服务融合 行列混合 更彻底的资源隔离 需要开放的、兼容数据库与大数据生态 统一数据查询服务 兼容主流生态与协议 计算存储分离 计算与存储强绑定意味着两种资源总有一种是浪费的，另外在对服务器选型时，计算型or存储型也会增加选择的复杂度和通用性。而在云计算的背景下，弹性的颗粒度是机器，并不能做到真正的资源弹性。 因此TiDB在设计之初就将弹性作为架构核心的主要考量点，所以选择了更为主流的面向未来的计算与存储分离的架构。 在架构逻辑上，TiDB主要分为三层： 支持标准SQL的计算引擎：TiDB-Server 本身并不存储数据，只进行计算 分布式存储引擎：TiKV 元信息管理与调度的引擎：Placement Driver(PD) 集群的元信息管理，包括分片分分布，拓扑结构等 分布式事务ID的分配 调度中心，每个TiKV节点会在一个周期内定向发送元信息给PD，包括所在节点的分数量、Leader数量等 系统构建 构建分布式存储系统 对于分布式存储引擎，设计的目标总结为 更细粒度的弹性扩缩容 支持高并发读写 数据准确，不错的容错性(不丢不错) 多副本保障一致性以及高可用 支持分布式事务 而TiKV是如何实现保证以上这些重要的特性和目标的呢 数据结构 数据结构是计算机存储和祖师数据的方式，是数据库的核心基础技术，首先选择了Key-Value数据模型 而数据结构在传统的OLTP得系统里，写入是最昂贵的成本。在传统的MySQL中也是最常见的索引结构是B-Tree。 B-Tree在一次写入需要写两次数据，一次是预写日志(WAL)，一次是写入树本身。而B-Tree是严格平衡的数据结构，它的设计本身就是对读友好。数据的写入触发的B-Tree的分裂和平衡的成本是非常高的。而在传统的主从架构里，集群的写入容量都是无法扩展的，集群的写入容量完全由主库的机器配置决定。在扩容上只能通过非常昂贵的集群拆分(分库)。 在分布式的场景，对于写入的容量需求会越来越大，因此TiKV节点选择了LSM-Tree结构作为RocksDB引擎的基础。而LSM结构本质上是一个用空间置换写入延迟，用顺序写替换随机写入的数据结构。 RocksDB引擎是一款非常成熟的LSM-tree存储引擎，它本身还具有其他重要特性，比如支持批量写入，无锁快照读(在数据副本迁移过程中会有用) 数据副本选择 数据的冗余决定了系统的可用性，在强一致的场景下，最终选择了强一致的复制算法，在一致性算法中，最成功的就属Raft和Paxos算法。而Raft是一种用于替换Paxos的共识算法，相比Paxos，Raft的目标是提供更加清晰的逻辑分工使用算法本身能被更好的理解，也就是说他更容易理解以及工程化实现，同时他的安全性也更高，，并提供了一些额外的特性。 因此就可以基于RocksDB构建一个多副本的集群 如何实现扩展 数据分片是分布式数据里面的关键设计，如果要实现扩展就是需要对数据做分片。而分片则需要注意是预先分片(静态分片)还是自动分片(动态分片)。 传统的分库分表或者分区的方案都是预先分片，这种分片只解决表容量的问题。因此我们需要自动的分片算法，另外分片总是需要一个维度和算法。 常见的分片算法有哈希、范围、列举。在TiKV系统中采用了范围Range算法。主要考虑到 Range分片可以更加高效的扫描数据记录，这种在OLTP业务中是非常常见的 范围分片可以简单的实现自动完成分裂与合并 弹性优先，分片需要可以自由调度 在TiKV中有个重要实现就是Region来做数据的分散。在系统中通过范围的方式将整个K-V空间分成若干段。每一段也是一系列连续的Key-Value。而这每一段就是Region，实际上就是一个分片的概念。每个Region中保存的数据不超过一定的大小(默认96M，可配置)。 如何分离与调度 当Region的大小超过了一定的限制(默认144MB)。TiKV会将他分裂成两个或者更多个的Region以保证各个Regon的大小是大致接近的，这样也有利于PD进行调度决策。同样的，当某个Region有大量的删除请求导致Region过小时，TiKV会将两个小的相邻的Region合并为一个。也就是常说的自动Split 和自动Merge。 为了保证客户端能够访问所需要的数据，TiKV系统中存在组件提供了记录Region在节点上面的分布情况。换句话说，通过一个key就能查询到这个key所在哪个Region以及这个Region在哪个存储节点上。而对于Region通过Raft又进行了三副本的复制 分布式事务 TiKV的MVCC实现是通过在Key后面添加版本号来实现的。TiKV支持分布式事务后，用户可以一次性写入多个kv而不用关心这些kv是否在同一个Region上，是否在同一个物理节点上。 TiKV通过两阶段提交来保证这一批次的读写请求的ACID约束。分布式事务普遍采用了两阶段提交的方式，而两阶段提交往往需要事务管理器(GTM)，而事务管理器也会成为整个集群的性能瓶颈。 因此TiKV采取了一个去中心化的两阶段提交：在每个TiKV存储节点上会单独分配一个存储锁信息的地方，TiKV的锁基于列簇，因此这个锁信息我们称为CF Lock。 PD来保证所有的TiKV节点的顺序的一致性，TiDB的默认隔离级别的是SI，而SI和RR可重复度的隔离级别非常近，同时TiDB也支持RC(提交读)。 构建分布式SQL引擎 基于KV实现逻辑表 TiKV对于每个表分配了一个TableID，每个索引也会分配一个IndexID，每一行数据也会分配RowID。一张表如果有证书的Primary Key，则pk为RowID。因此可以简单的理解为RowID+indexID 作为kv中的Key Value则可以看成所有的列按照等位偏移的方式进行的connect进行连接 在数据查询时通过等位偏移量对Value进行反解析，然后再对应于Schema的元信息进行的列信息映射。 在TiKV中，二级索引也是全局有序的Key-Value map 它的Key就是索引的列信息，Value则是原表的Primary Key的主键。通过主键在原表的Key-Value map进行一次扫描找到Value，按照等位偏移量进行列解析。 SQL存储过程 SQL是一个非过程的声明性语言，只描述结果不解读过程。因此SQL引擎最重要的就是优化器。 TiDB采用了兼容MySQL的策略，也就是说会按照MySQL的语法进行解析以及语义解析。中间过程就包含了权限控制以及对表队列的合理性校验。 AST抽象语法树主要试讲SQL从文本解析成一个结构化的数据 SQL逻辑优化部分会将各种SQL等价改写以及优化，比如说将子查询改成表关联，也可以进行一些不必要的信息的裁剪，如列裁剪、分区裁剪、left join裁剪等 物理优化，优化器会基于统计信息与成本进行生产执行计划。这一步十分关键也是整个SQL优化中最重要，优化空间最大的部分。 执行殷勤会根据优化器定下来的执行路径进行相应的得数据寻址、数据的计算 分布式SQL引擎的优化策略 简单来说就是让数据表咋不同的存储节点的分片进行预计算完成本地的数据过滤以及统计，然后再将本地存储节点的临时结果、中间结果上报至Server进行集中计算 表DDL构建 首先在TiDB中没有分表的概念，所以整体DDL的完成过程是非常快的 再者，TiDB中会把DDL过程分成Public、Delete-only、Write-only 等几个状态，每个状态在多节点之间同步和一致，最终完成完整的DDL。 TiDB-Server 最上面是MySQL协议层，按照MyDQL的协议进行编码解析，下面是SQL的核心层也是TiDB-Server的最主要的部分会进行SQL的Parser分析、逻辑优化、物理优化、统计信息收集等 最后再按照数据收集的方式交给执行器进行执行 HTAP的应用分析 TiDB是一款支撑HTAP数据服务的数据库 HTAP的必然性 HTAP需要同时支持OLTP和OLAP场景，基于创新的计算存储架构，在同一份数据上保证了事务的同时又支持实时分析，省去了费时的ETL过程 分布式技术的发展逐步解决了数据容量爆炸的问题，而分布式关系型数据库同时满足了OLTP的需求也解决了数据容量的问题，在此基础上，很多传统的OLAP技术可以在此架构上进行再融合，实现了更大树容量的混合数据库(HTAP) TiDB应用HTAP 最早的TiDB是为了解决业务分录分表的问题。早期的定位也是OLTP系统。而之后提供的解决方案将多众多业务系统实时同步至一个TiDB集群，渐渐地也被应用在数据中台，主要源于以下特性： TiDB支持海量存储，允许多数据源汇聚，数据实时同步 支持标准的SQL,多边关联快速出结果 透明多业务模块、支持分表聚合后可以任务维度查询 最大下推策略以及并行hash join等算子决定了TiDB在表关联上的优势 对于TiDB面向OLAP用户场景需求，通过引入大数据生态的Spark，让Spark识别TiKV的数据格式、统计信息、索引、执行器最终构建了一个能跑在TiKV上的Spark的计算引擎(封装成TiSpark) 进而实现了一个分布式的技术的平台，在面向大数据量的报表以及重要级的AdHhoc里提供了可行的方案 但是Spark只能提供低并发的重量级查询，在很多中小规模的轻量AP查询，也需要高兵大，相对低延迟技术的能力，在这种需求场景下，Spark的技术模型过于厚重，资源消耗也大。这个时候OLTP的查询和TiSpark公用的一套底层存储系统显然是不太合理的，面向软件层面去优化OLAP和OLTP的资源隔离是很难的 从数据库资源隔离的角度看，依次是数据库软件层、副本调度、容器、虚拟机、物理机等。因此越接近物理机的隔离性会更好(物理隔离是最好的资源隔离)。因此列存天然对OLAP的查询友好，因此借鉴传统的主从架构分离的思想，可以将这个副本放到一个列式引擎上。 如何进行行列数据同步 因为要强调数据的时效性，因此采用了复制协议更底层更高效的raft复制 通过对raft进行进一步的改造，将一个副本的raft设置为只负责同步没有投票权的角色(Learner) 这样既保证了Raft Group的写入效率又保证了列存副本的极低延迟，同时还可以避免复制组脑裂问题 ","link":"https://GeekGhc.github.io/post/tidb-jia-gou-te-xing-fen-xi/"},{"title":"Sync.Pool性能优化解析","content":"场景 我们知道Go本身采用三色标记法自动进行垃圾回收，而在一些高性能的场景下，就不得不为“自动”进行严格控制。毕竟大量的在堆上创建堆对象的话，不仅影响对象标记的时间而且对于频繁的垃圾回收也会影响程序的性能。 因此在做性能优化时常用的思路就是对象池的方式：把不用的对象回收起来避免被垃圾回收后重新创建。 在Go的标准库中提供了Pool来创建池化的对象，不过值得注意的是采用同Pool创建的对象可能会被垃圾回收掉。所以对于一些类似TCP连接池、数据库连接池这些场景是不适合。不过这也是有办法的。 基本介绍 数据类型 Sync.Pool用来保存一组可独立访问的临时对象，所池化的对象会在未来的某个时间点被移除掉。不过核心还是会通过减少新对象的产生，从而提高性能 使用方法 Sync.Pool 提供了3个方法：New、Get、Put New New字段的类型是func interface{}，当调用Get从池中获取对象并且没有没有空闲元素可返回时会调用New方法来创建新的元素。而如果没有设置New字段，当没有空闲元素时会返回nil以表示没有可用的元素 值得注意的是New可变的字段，因此在程序运行中是可以改变元素的创建方法的，不过一般也没必要这么做。 Get 调用Get方式时，会从Pool取走元素并返回，不过当New没有设置，并且也没有可用元素时也会返回nil，因此在使用时需要判断下 Put Put方法会将一个元素给到Pool保存在池中，以此来进行复用 实现原理 在Go1.13之前Pool的实现有两大问题： 每次Gc都会回收创建的对象 如果缓存的元素太多，就会导致STW的耗时变长；而缓存元素被回收后也会导致Get的命中率下降，因此会创建很多新的对象 底层使用了Mutex锁，对这个多并发请求激烈时会导致性能的下降 在1.13中sync.Pool做了大量的优化：Go 对 Pool 的优化就是避免使用锁，同时将加锁的 queue 改成 lock-free 的 queue 的实现，给即将移除的元素再多一次“复活”的机会 当前，sync.Pool 的数据结构如下图所示： Pool 最重要的两个字段是 local 和 victim，因为它们两个主要用来存储空闲的元素 每次垃圾回收的时候，Pool 会把 victim 中的对象移除，然后把 local 的数据给 victim，这样的话，local 就会被清空，而 victim 就像一个垃圾分拣站，里面的东西可能会被当做垃圾丢弃了，但是里面有用的东西也可能被捡回来重新使用 victim 中的元素如果被 Get 取走，那么这个元素就很幸运，因为它又“活”过来了。但是，如果这个时候 Get 的并发不是很大，元素没有被 Get 取走，那么就会被移除掉，因为没有别人引用它的话，就会被垃圾回收掉 垃圾回收时的处理逻辑： func poolCleanup() { // 丢弃当前victim, STW所以不用加锁 for _, p := range oldPools { p.victim = nil p.victimSize = 0 } // 将local复制给victim, 并将原local置为nil for _, p := range allPools { p.victim = p.local p.victimSize = p.localSize p.local = nil p.localSize = 0 } oldPools, allPools = allPools, nil } 其中local字段会包含当前所有空闲的可用的元素，请求元素时也是优先从local中读取，local字段包含一个poolLocalInternal 字段，并提供 CPU 缓存对齐，从而避免 false sharing 缓存对齐：当读取某一个值x做读写操作时，并不是只读取一个值，而是按块来读取（因为cpu读取，很可&gt; 能会用到相邻的数据，比如把y也一起读取进去了），此时如果另一个cpu操作y，就会出现伪共享问题。解&gt; 决方式：在x, y插入一些无用的内存，将y排出当前的缓存行即可 poolLocalInternal 也包含两个字段：private 和 shared private，代表一个缓存的元素，而且只能由相应的一个 P 存取。因为一个 P 同时只能执行一个 goroutine，所以不会有并发的问题 shared，可以由任意的 P 访问，但是只有本地的 P 才能 pushHead/popHead，其它 P 可以 popTail，相当于只有一个本地的 P 作为生产者（Producer），多个 P 作为消费者（Consumer），它是使用一个 local-free 的 queue 列表实现的 Get方法实现 func (p *Pool) Get() interface{} { // 把当前goroutine固定在当前的P上 l, pid := p.pin() x := l.private // 优先从local的private字段取，快速 l.private = nil if x == nil { // 从当前的local.shared弹出一个，注意是从head读取并移除 x, _ = l.shared.popHead() if x == nil { // 如果没有，则去偷一个 x = p.getSlow(pid) } } runtime_procUnpin() // 如果没有获取到，尝试使用New函数生成一个新的 if x == nil &amp;&amp; p.New != nil { x = p.New() } return x } 主要执行流程： 优先从local的private取元素，这个过程没有锁，所以会很快 如果private没有则从shared中弹出一个 如果shared也没有元素，则通过getSlow方法去偷一个(类似GMP模型) 首先遍历所有的local尝试从shared中弹出一个返回 没有找到的话则开始遍历victim 遍历victim也是先从private查找，然后再从shared中查找 如果也没偷到则尝试New重新创建一个 Put方法实现 func (p *Pool) Put(x interface{}) { if x == nil { // nil值直接丢弃 return } l, _ := p.pin() if l.private == nil { // 如果本地private没有值，直接设置这个值即可 l.private = x x = nil } if x != nil { // 否则加入到本地队列中 l.shared.pushHead(x) } runtime_procUnpin() } Put执行时会优先设置本地private，如果有值就push到本地shared队列中 sync.Pool的坑 内存泄漏 在将sync.Pool作为buffer池的场景中，因为取出来的bytes.buffer在使用后容量会变得很大，这个时候再存放回池子中后，由于对象的容量在重置后还是很大，而这些可能并不会被回收就会一直占用比较大的内存空间。 常见的解决思路是：在将Buffer放回池子中时增加大小的判断，超过一定大小的buffer则直接丢弃 因此在回收buffer时一定要检查buffer对象的大小 内存浪费 池子中的 buffer 都比较大，但在实际使用的时候，很多时候只需要一个小的 buffer，这就是一种浪费现象 解决思路就是我们可以将buffer池根据元素大小分成几层 如net/http/server中提供了2k和4k两个writer的池子 连接池 在之前说过很少会使用sync.Pool去池化连接对象，是因为Pool会无通知的在某个时刻去除元素也就是连接对象，因此在需要持久化连接对象时会使用其他的方法 标准库http client 标准库的http client通过池化的方式来缓存一定数量的连接，以便后续对象的重复使用从而来提供性能 主要实现实在Transport类型，其中idleCoon用来保存持久化的可重用的长连接 TCP连接池 常用的TCP连接池如: fatih/pool 管理的则是更加通用的net.Conn // 工厂模式，提供创建连接的工厂方法 factory := func() (net.Conn, error) { return net.Dial(&quot;tcp&quot;, &quot;127.0.0.1:4000&quot;) } // 创建一个tcp池，提供初始容量和最大容量以及工厂方法 p, err := pool.NewChannelPool(5, 30, factory) // 获取一个连接 conn, err := p.Get() // Close并不会真正关闭这个连接，而是把它放回池子，所以你不必显式地Put这个对象到池子中 conn.Close() // 通过调用MarkUnusable, Close的时候就会真正关闭底层的tcp的连接了 if pc, ok := conn.(*pool.PoolConn); ok { pc.MarkUnusable() pc.Close() } // 关闭池子就会关闭=池子中的所有的tcp连接 p.Close() // 当前池子中的连接的数量 current := p.Len() 通过把 net.Conn 包装成 PoolConn，实现了拦截 net.Conn 的 Close 方法，避免了真正地关闭底层连接，而是把这个连接放回到池中： type PoolConn struct { net.Conn mu sync.RWMutex c *channelPool unusable bool } //拦截Close func (p *PoolConn) Close() error { p.mu.RLock() defer p.mu.RUnlock() if p.unusable { if p.Conn != nil { return p.Conn.Close() } return nil } return p.c.put(p.Conn) } 数据库连接池 标准库 sql.DB 还提供了一个通用的数据库的连接池，通过 MaxOpenConns 和 MaxIdleConns 控制最大的连接数和最大的 idle 的连接数 DB 的 freeConn 保存了 idle 的连接，这样，当我们获取数据库连接的时候，它就会优先尝试从 freeConn 获取已有的连接 conn Worker Pool 尽管goroutine的初始栈大小只有2048字节，但是在有些场景如果每次处理都需要创建goroutine的话，大量的goroutine无论是资源消耗还是垃圾回收都会有很大影响 因此通常会创建一定数量的Worker Pool去减少goroutine的使用 比如 比如 fasthttp 中的Worker Pool 大部分的 Worker Pool 都是通过 Channel 来缓存任务的，因为 Channel 能够比较方便地实现并发的保护，有的是多个 Worker 共享同一个任务 Channel，有些是每个 Worker 都有一个独立的 Channel ","link":"https://GeekGhc.github.io/post/go-syncpool-xing-neng-you-hua/"},{"title":"分布式共识算法","content":"开篇 首先提大分布式系统你的第一印象是什么 试想一下心目中的分布式系统的特点，大胆总结下： 系统可用角度：无单点故障，保证系统可用性 系统性能角度：通过负载均衡，可以充分发挥集群节点的性能 系统扩展角度：基于集群可扩展能力可以持续扩展横向能力，增加节点来支持更高需求的场景 这些都可以归纳为分布式系统出现的意义以及解决的问题，反推回来，随着数据规模和服务计算能力需求越来越高，之前的单个节点已经无法满足系统的计算和存储要求，并且随着摩尔定律面对自然法则越来越乏力这些现实的问题。基于这些就不难理解分布式系统出现的意义了 当然类似人类社会，当团队规模从几十增长至几万时，带来是生产能力的规模提升，同时也要面对这各种协作和沟通的问题。也就上学那会会告诉你1+1=2。所以在整个生产组织的提效上，制度和规则是必须的 分布式系统也是类似，计算存储能力提升的同时也带了这些问题： 节点通信延迟：服务连接、请求超时等各种未及时响应的问题 节点故障/恢复：节点甚至机房宕机导致服务不可用 网络分区-脑裂：机房网络问题，导致数据不一致 拜占庭问题：存在恶意节点，恶意行为 这些问题基于环境是否置信又可以分为两类。1，2，3都是在可置信情况下，4则是在非置信情况下 而在共识算法的出现完善后都可以得到解决，而在一些其他应用比如区块链的研究，共识算法也是非常重要的基础，在我看来，共识可以理解为两个层面： 点的层面：也就是多个节点就某个数据达成一致共识 先的层面：即多个节点对多个数据的顺序达成一致共识 接下来我们看看在算法的演进过程中又是如何解决上面的问题的 算法演变 拜占庭问题 拜占庭错误是莱斯利·兰伯特在《拜占庭将军问题》中提出的一个错误模型，这个模型的场景是在一个完全不可信的场景，也是在分布式领域中最为复杂以及经典的容错模型了。当然简单解释就是节点存在着恶意行为 故事背景 类似投票裁决的场景，在&lt;&lt;鱿鱼游戏&gt;&gt;里男主第一次在进入比赛后进行了一场投票选举：Y or N来决定是否继续游戏，大家私下各自商量着自己的决定，如果有人背叛了同伴，开始从中误导分离这个团队，自己最后临时改变了策略，那么就可能直接影响着最终结果 我们简化为最小可解释模型就是：二忠一叛问题 假设三人团队中A，B，C商量这是否继续游戏，按照“少数服从多数”来表决，所以有两人一致就可以了 算法流程 假设大家内部意见并不一致，在内部讨论时，我们正常的预期是这样的： 可以看到无论是选择继续游戏还是放弃游戏，结果都是：三人的最终达成一致投票结果是一样的 那么如果中间(假设是B)有人有起了小心思，就会是接下来这样： 我们从各自视角看下结果 A: 2Y + 1N C: 2N + 1Y 显然这直接干扰了A，C两位的最终选择，可想而知，结果肯定有个人是被骗了(原地心态爆炸) 解决方案 这个问题后来有了两种解决算法，分别是口头版和签名版，目的就是协调整个团队(集群)的一致 口信消息型 3人太少了，再拉一位D入伙，也就是变成了4人的协商 几人约定，如果没有收到其他人的结果，就执行统一默认的：比如放弃游戏(N) 最终进行两轮商议投票 这个方案是兰伯特论文里阐述的：[The Byzantine Generals Problem]https://www.microsoft.com/en-us/research/publication/byzantine-generals-problem/ 最终整体流程是这样的： Round 1 新入伙的D作为指挥官(proposer)，将作战信息发送发送其余三位 原本团队的成员A,B,C 将收到的消息作为结果，如果没收到执行默认 Round 2 原本团队成员A,B,C 分别发送消息给其他两位成员(除去新加入的D) 最终，原本团队按照之前原则“少数服从多数”，执行最终投票结果 对于一叛问题，在这个方案里，叛徒要么存在原来团伙，要么存在新加入的D，因此两种情况我们分别讨论下： 最终结果(不包含D的投票结果)： A：2Y+1N C: 2Y+1N 最终结果： A,B,C都是：2N+1Y 如果D给的信号是2Y+1N，则A,B,C的结果就是2Y+1N，也就是说还是会达成一致 可以看到无论B怎么操作A,C都是达成一致(Y or N) 方案总结 可以看到无论叛徒存在哪方都不能使得团队产生分歧，最终都可以达成一致 当然这个算法是有一定前提的： 叛徒个数x是已知的 叛徒为x，则忠诚人数不小于3x+1 叛徒个数x决定了需要进行多少轮的协商 显然这个算法存在几点问题： 需要引入额外人员(节点)参与，不过此我们可以通过签名消息也就是第二种解决方案来解决这个问题 并且对叛徒(不可信节点)是已知可控的，因此叛徒太多，需要O(n^(x+1))的消息，网络开销太高，因此实际应用中用的是PBFT(下面会介绍) 达成一致的结果也许并不是想要的，比如真是场景需要就Y达成一致，但最终还是会得到非预期的结果 签名消息型 这个算法也有一定前提： 每个人的签名无法伪造，而且就算伪造后可以被发现 每个人都可以验证别人的签名和本人是否匹配 在这样的前提下，再来分析上面的两种情况 对于A(忠)发起请求，由于B伪造了A的请求，对于C就可以校验后进行忽略，因此最终A,C结论都是Y 对于B(叛)发起请求，由于A,C接收到的不同指令但指令集合一样，所以可以根据一定规则(排序)进行选取 介绍了算法流程，那么又是如何实现签名消息的验证即加密解密的呢，毕竟这是这个算法的前提 具体可以参考https里数字签名、证书的实现 https://www.zhihu.com/question/52493697/answer/1600962734 总结来说就是使用非对称加密对消息内容进行摘要，其他成员基于已有公钥进行解密比对 PBFT 场景描述 实际上上面讨论的拜占庭问题的解决方案都是理论化的基础，也存在诸多问题，核心还是说在叛徒存在的场景下如何就达成一致，但最终是在有叛徒(伪造节点)情况下具体什么结果是无法控制的。 因此这个算法的真正落地是在PBFT的应用，在区块链(公有链场景)的实际场景就已经有很多的使用 PBFT也是在口信型和签名型的基础之上进行改进落地的，因此算法的消息也是签名消息 算法流程 这里我们以基本规模为例，1客户端+1主节点+3备份节点+1问题节点(节点3) 先看下宏观的基本流程 客户端发送消息给主节点 主节点广播请求给其他节点，节点执行pbft的三阶段共识算法 节点处理三阶段流程后，返回消息给客户端 客户端收到来自f(问题节点数)+1个节点的相同消息后，代表共识已经成功完成 核心三阶段 客户端向主节点发起请求，主节0收到客户端请求，接着向其他节点发送 pre_prepare阶段 主节点发送消息后，节点1，2，3接收到消息后进入Prepare阶段，并分别广播消息给其他节点 prepare阶段 进入prepare节点后，当某个节点接收到2f(f为问题节点数)个一致的消息后，会进入提交阶段，而问题节点的表现就是对其他节点的请求无影响 Commit 阶段 进入提交阶段后吗，各节点分别广播消息给其他节点，相当于说我已经准备开始执行指令了 最后，当节点收到2f+1的验证通过的消息(包括自己)，这时也就意味着大都数节点已经达成共识，那么就可以执行执行，并最终返回给客户端成功执行消息 而对于客户端而言，如果收到了f+1个相同的响应消息，说明各个节点已经达成了共识。当然，如果超过超时时间，客户端也可以重新发送请求 Paxos 解决了置信问题，没人捣乱了，那么接下来大家就可以坐下来商量下如何达成共识了 这里就不得不提Paxos算法了，现在我们接触到的共识算法都是基于此改进的 而在Lamport提出的Paoxs提出包含了两种，Basic Paxos &amp; Multi-Paxos Basic Paxos 算法：多节点如何就某个值达成共识 Multi Paxos 思想：执行多个 Basic Paxos ，就一系列的值达成共识 先来说Basic Paxos描述的是多个成员节点如何基于一个值达成共识，这可以被看作共识算法的最小实现 场景描述 为了方便理解，所以现在是这么个场景，在一个分布式集群中，集群有A,B,C 3个节点组件，基于集群的变量KV信息的更新需要达成一致 简单来说就是多节点更新某个Key，其他节点就此Key的Value达成一致 角色对象 在几乎所有的共识算法中，我们都会接触到这几类角色，他们存在的意义是一样的，因此需要提前在整个上下文中达成一定共识 提议者(Proposer): 提议想达成共识的值，在生产场景中可以客户端也可以是接受到客户端请求的节点 协调者(Coordinator): 转发提议消息以及协调和保存结果确认，通常存在于二阶段提交 接受者(Acceptor): 对于提议者的提案会参与协商并存储结果值最终更新自身的状态机 学习者(Learner): 接受最终协商后的结果，在一些算法中也叫做Folllwer，被动接受协商后结果 为了方便理解和描述，对于提案都有自己的编号以及提案内容，我们以P[n,v]标识一个提案 n标识提案编号(唯一标识)，通常是生成自增id，v标识提案内容比如{ K:'SEC_KV_AUTH，V:1} 问题思路 基于这样场景的需求，核心就是能达成结果一致，是不是可以立马联想到二阶段提交，这也是最简单直接的方式了，事实上Paxos也是这么做的 但是他们都会存在这样的问题： 同步阻塞： prepare阶段在资源预留期间，其他事务是无法修改此资源的，需要等待至事务结束 协调节点故障：如果prepare阶段结束后，协调节点宕机，那么其他节点资源将会被锁定无法释放 数据不一致： 还是协调节点故障，只不过是在提交commit部分节点后故障，那么就会存在节点收不到commit命令而和其他节点数据不一致 基于上面的问题，其实也有很多解决方案，比如三阶段提交或者TCC(Try-Confirm-Cancel)事务模式(事实上，Paxos也是这么做的)，具体可以参照阿里的Seata和公司内部的ByteTx 的TCC事务模式的实现 和数据库层面的事务操作所不同的是，TCC需要业务实现各自的确认(Confirm)和撤销(Cancel)操作，并且是幂等的，因为请求都会有失败重试 算法流程 Prepare阶段 基于以上背景，根据时间线准备阶段流程是这样的： 客户端1提交提案，版本号基于时间递增，假设版本好号为V1 节点A,B,C是参与协商节点，A,B 收到了客户端1提案消息，分别表示I'm ready，并返回当前接受的最大版本号V1，最终返回[Yes,V1] 客户端2提交提案，假设版本为V3 节点A,B收到了客户端2的提案，并且之前提案1还没通过，而且又大于之前的提案版本，因此开始接受客户端2的提案 节点C第一次收到客户端提案，版本为V3，因此就提案3进行回复并返回当前接受的最大版本号V3 节点C最后收到了客户端1的提案，但是因为已经接受了提案V3版本号更大的提案，因此不做处理 Accept阶段 接受阶段是在收到了大都数节点的成功回复后发起的，随着时间演进： 客户端1收到了A,B的回复[Yes,V1]，多数ready后发起请求[1,data1] 接收者A,B,C此时接受的最大提案都是V3，因此对于客户端1的请求进行拒绝 客户端2收到了来自A,B,C的所有回复[Yes,V3]，多数ready后发起请求[3,data3] 接收者A,B,C根据请求和自己接受的提案版本比对后，通过最终提案[3,data3] Multi-Paxos 场景描述 基于二阶段提交达成了单个值的共识，而在实际场景中，面对诸多的数据，我们需要对多个Value达成一致 所以是不是多次执行Paxos算法即可，在一定数据规模下讲是可以的，需要达成一致的值较多，多次的Paxos算法就会存在问题 多节点集群下，多节点多提案就会引发冲突，因此提案协商的成功率就会下降 基于二阶段，本身的通讯次数就会很多，多节点情况下，节点的通讯延时就会被进一步放大 算法思路 为了解决多提案者的情况，结合数据库的Master-Slave模式，可以引入领导者节点，由领导者作为提案者 至于领导者如何选举，这个则需要根据各自服务的需求来实现，因为每个系统对于领导者的判断标准不一样，而选举算法最直接的可以基于Basic Paxos来实现，而如果领导者节点故障，则进行新一轮选举 基于领导者，就可以对Paxos的二阶段进行优化的，也就是省去了准备阶段。因为不存在了提案冲突这个问题，节点通讯数量也降低了，离目标又进了一步 至于一致性实现就和Basic Paxos的二阶段的接收阶段就没什么区别了，依旧按照&quot;多数原则&quot; 算法总结 Multi-Paox是在之前的基础之上引入了领导者来进行优化，从而实现多次执行Basic Paxos的效果。这个思想在很多的变种算法和场景都有所运用，比如接下来要说的Raft ZAB 场景描述 在开篇一开始，我们说过共识问题的线的层面就是需要解决如何就多个节点的对多个数据的顺序达成一致， 很多场景不仅需要对值的一致性进行保证，而对于值的顺序也有一定的要求，比如ZK因为底层作为目录树的设计，目录是有父子关系的也就是说有一定的依赖关系，因此就必须保证一定的顺序性，此时基于Multi-Paxos的算法就无法满足了 因此zookeeper就是基于ZAB协议实现的(当然了Raft也可以实现)，那么在ZAB又是如何实现执行顺序的 算法流程 为了说明这个问题，我们模拟这个场景：集群中存在4个节点A，B，C，D，其中A作为领导进行提案X，Y对应的唯一事务标识符为&lt;1,1&gt;&amp;&lt;1,2&gt; ，格式为&lt;epoch,counter&gt; 其中类似raft中的任期思想&lt;1,2&gt;表示任期1，计数为2 主节点收到客户端请求，生成唯一事务id的消息请求&lt;1,1&gt;:X &amp; &lt;1,2&gt;:Y，并按循序广播给各节点 因为主节点基于TCP协议顺序发送提案消息，因为Node B,C,D接收到的也是顺序消息 主节点接收到基于某一提案，比如X的多数节点的成功响应(暂无提案)后开始提交提案X 因为在同一任期内的提案是递增的，因此提交时会根据事务大小进行提交 各节点收到提交提案消息，执行成功后返回给LeaderA 主节点LeaderA收到多数执行成功回复后响应给客户端 算法总结 ZAB在之前Multi-Paxos的思想上，对提案实现了消息的事务唯一和递增，通过提交提案时比较事务标识符的大小而保证了指令的顺序性，最终实现想要的共识线层面 Raft 作为现在共识算法的首选，相对也最为成熟，如今的云原生的大环境下，很多大型系统都选用了Raft作为核心共识算法，比如tikv、Etcd、Consul等 因此在掌握了以上的结论后，再分析下Raft的核心实现，会发现很多结论都得到了验证 领导者选举 上面说过领导者选举，完全可以基于Basic Paxos来实现，只是每个系统的定义标准不一样，在Raft的选举过程中，定义了几种状态 Leader： 处理写请求，管理日志复制和连接其他节点的心跳检测 Follwer： 接收Leader的消息，必要时需要推荐自己当选Candiator Candidate: 当领导者有故障后，发送选举消息，此时相当于选举提案人 选举的详细过程动画演示 http://thesecretlivesofdata.com/raft/ 大致概总结下就是： 初始状态，所有节点都是Follwer状态，并且每个节点的超时时间是随机的，这里的超时时间简单了说就是等待领导者的心跳请求的超时时间 节点A超时时间最少，因此最先变为候选人并增加自己的任期编号为1，开始向其他节点发起投票RPC请求，进行Leader选举 B,C节点接收到任期为1的消息，因为之前没有收到该任期内的消息，因此可以投票给节点A(认为可以理解为二阶段的提案版本) A在选举超时时间(相对会比较大)内收到多数投票，则成为Leader，并开始定时发送心跳信息给其他节点(证明自己活着，不要再选举了)，选举结束 如果网络等问题产生选举超时，重置超时时间，并开始新一轮选举 基于领导者开始发起指令提案，依据多数原则达成一致后Leader实现指令的共识并提交状态机返回客户端 其他follower节点根据心跳信息或者日志复制RPC消息应用Leader最新提交的日志项 当然在选举的过程中，可能会遇见这类问题(这两个问题在动画演示中其中也都解释了) 问题1：目前是一个candidate进行选举，如果有多个，比如2个候选人同时发起投票进行选举 此时会等待进行下一轮选举，按照超时时间，可能就是节点A发起了下一任期的投票了 问题2：遇到网络分区异常，也就是通常说的脑裂问题，会出现多leader情况，如何解决 如上图所示，CDE和AB被划分在两个网络分区，两个分区分别进行了选举，推选C&amp;B为各自Leader 分区故障前 不过由于AB分区提交的提案无法得到大多数节点响应，因此是无法进行commit的 CDE分区因为可以得到大多数节点影响，因此可以正常得到正常提交 故障恢复后 节点A,B由于LeaderB，发现比自己Term更高的消息，因此会退出Leader并和节点A同步最新的日志项，最终不同分区节点的Log Entry保持一致 问题3：ZAB &amp; Raft都可以实现顺序性，主要区别是什么 一致性：从系统本身实现角度出发，ZAB实现了最终一致性，Raft读写都从Leader实现强一致性 事务表示方式：zab用的是epoch和count的组合来唯一表示一个值, 而raft用的是term和index Leader选举方式：也就决定了选举方式不一样。Raft基于每个节点设置不同的超时时间来自行发起选举，并且每个节点每任期中只会投票一次。ZAB每个节点在任期内可以发起多次投票，遇到更大的则更新并分发给其他节点。综合来说，Raft的选举效率更高 超时选举方式：Raft只有follower会检测Leader的心跳超时，超时则触发选举，ZAB中Leader也会检测是否收到半数节点的心跳回复超时并触发选举 问题4：想实现这样的领导者模型的算法，需要怎么实现 根据Raft和ZAB的整体实现，不难得出，这类共识算法的实现条件： 领导者选举-&gt;指令共识(二阶段提交)-&gt;日志复制应用-&gt;日志恢复 Log entry复制 集群中选举好了Leader就可以进行正常的协调通信了，而这个过程就可以理解为对日志项的复制和应用到状态机的过程 日志项内容 指令： 用户指定的数据，也就是需要达成一致的数据，通常由客户端提供 索引值： 日志项对应的物理索引值，可以理解为基于索引可以在连续的日志中查找到具体日志内容 任期编号： 创建日志的领导者的编号(很关键，用于数据比对同步和数据修复) Hashicorp Raft Log Code type Log struct { // Index holds the index of the log entry. Index uint64 // Term holds the election term of the log entry. Term uint64 // Type holds the type of the log entry. Type LogType // Data holds the log entry's type-specific data. Data []byte // Extensions holds an opaque byte slice of information for middleware. It Extensions []byte // AppendedAt stores the time the leader first appended this log to it's AppendedAt time.Time } 日志复制同步 在Raft中领导者是强制跟随者直接复制自己的日志项用来处理不一致的日志。这也是强领导者模型的优势 当领导者和跟随者日志产生diff时 ，根据日志项的任期和索引来进行一致性检查，并通过日志复制RPC 复制并更新不一致的日志项，这里不做详细展开 Quorum NWR 场景描述 理解了Raft算法的整个过程，其实已经能够满足绝大多数的应用场景了，而在一些场景我们其实更想自定义的实现，而Quorum 机制是分布式场景中常用的，用来保证数据安全，并且在分布式环境中实现最终一致性的投票算法。在一些企业级应用的InfluxDB、Cassandra都有运用此类算法 算法流程 首先需要明白NWR的三要素 N：副本数，也就是一份数据集有多少副本，这些副本会分布在不同的节点 W：写一致性级别，表示写入W个副本才能算是完成写操作 R：读一致性级别， 表示读取一个数据对象需要读取R个副本并返回最新的那份数据 以读取Data2数据为例，此时副本数N=3，如果需要实现强一致性，则设置W=2&amp;R=2即可 此时满足W(2)+R(2)&gt;N(3) 因此如果W+R&lt;=N，可以推出，整个系统可以保证最终一致性，即可能返回旧数据 整体和抽屉原理一样，还是比较容易理解的 而在实际的生产环境中，一致性级别通常也会被定义为诸如all、one、quorum(大多数)这些级别来分别应对不同的数据需求场景 TimeLine 1987年 ACM论文提出Gossip protocol ，用于分布式数据库系统中各个副本节点同步数据 1989年 莱斯利·兰伯特 提出Paxos，使其获得2013年图灵奖 1999年 Miguel Castro和Barbara Liskov在发表的论文[2]中首次提出PBFT算法 2006年 Paxos算法在Google的研发团队才在生产环境中落地，但初期算法实现并并未成熟 2007年 ZAB随着zooKeeper的开发而应运而生 2013年 斯坦福大学迭戈·安加罗（Diego Ongaro）和约翰·奥斯特霍德（John Ousterhout）深入研究Paxos协议后提出了 Raft 维度总结 经过上面算法的慢慢演进，最终我们根据拜占庭容错、一致性、性能、系统可用性总结如下 拜占庭容错 一致性 性能 系统可用性 2PC 否 强一致性 低 低 TCC 否 最终一致性 低 低 Paxos 否 强一致性 中 中 PBEF 是 - 低 中 ZAB 否 最终一致性 中 中 Raft 否 强一致性 中 中 Quorum NWR 否 强一致性 中 中 场景实践 生产实践 Etcd 原生化 Etcd 本身作为云原生最热门存储的基础，应用场景十分广泛，包括服务发现、分布式锁、配置存储和分布式协调。而在Raft在Etcd中的作用也举足轻重，我觉得可以一起看下内部是如何结合使用的 整体架构 在etcd的架构设计中，etcd基于raft算法实现了节点间的数据复制、数据一致从而实现服务的高可用 指令执行 首先我们先看下在etcd中一条KV写请求的执行过程 client端通过负载均衡算法选择一个etcd节点并发起grpc调用 调用过程中会经过一系列的拦截器进行请求过滤 经过Quota配额模块，根据请求QPS、当前db限额、存储容量决定是否超限 1. 超限后集群产生告警并禁止继续写入(默认db配额2G，禁止请求写入日志并同步其余节点) 请求通过后，进入KVServer核心处理层，此时作为内部raft层的客户端向raft层提交提案 发起提案前会经过鉴权等检查并生成提案id 提案内容就是本次请求需要执行的指令，比如&quot;put key value&quot; 发起请求后等待raft层消息结果通知 raft层内部经过RaftHttp网络模块进行指令转发，多数节点通过后，提交日志状态 指令包含了任期编号、索引信息、提案内容、日志类型等 半数节点通过(持久化)后，通过channel返回可执行提案 etcdserver从channel读取提案内容，传递给Apply模块进行提案内容执行 Apply模块基于MVCC模块执提案内容并更新最终状态机 经过上面的流程，可以看到etcd内部是如何基于raft实现数据写入的，那么又是如何保证数据一致的 服务可用性保证 机制一：领导者选举 Leader节点crash后，Follower节点因为收不到心跳信息，开始发起投票，进行新Leader选举 为避免同时竞选，引入随机的等待发起选举时间 Crash的Leader节点启动后，再次成为Follower节点并同步Leader最新日志 机制二：日志复制规则 Leader收到提案消息为此生成日志条目，然后遍历Follower列表和日志的应用进度信息，并为每个Follower生成追加类型的RPC消息，消息中包含Follower需要复制的日志条目 同时，日志的复制还遵循以下规则： 如果日志在某个任期中已经被提交(提交后可能还未通知Follower)，这个条目不会删除并且会应用到新Leader 追加日志中包含应用的前日志条目信息来进行一致性检查，直到匹配到一致日志条目才会追加 机制三：选举规则保证安全 如果Leader Crash后，剩余Follower并不是都可能成为新Leader，选举投票规则还会基于 收到投票请求的前后顺序 发起投票的节点应用的日志数据是否是最新的 发起投票的节点应用的任期是否是最大的，每个节点在每个任期只能有一个投票权 最终投票需要半数以上节点支持 基于Raft的这些特性和实现机制最终保证了etcd服务的数据一致性和服务高可用 代码实践 Raft实现 [Hashicorp Raft] https://github.com/hashicorp/raft 相关资料 相关论文 [paxos] https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf [simple paxos] https://lamport.azurewebsites.net/pubs/paxos-simple.pdf [pbft] https://pmg.csail.mit.edu/papers/osdi99.pdf [raft] https://raft.github.io/raft.pdf [ZAB] http://www.cs.cornell.edu/courses/cs6452/2012sp/papers/zab-ieee.pdf 文章系列 https://www.microsoft.com/en-us/research/publication/byzantine-generals-problem/ https://time.geekbang.org/column/intro/100046101?tab=catalog https://tech.bytedance.net/articles/3786?from=net_app_search#heading4 https://www.zhihu.com/question/52493697/answer/1600962734 [Seata XA]https://developer.aliyun.com/article/783796?utm_content=g_1000267062 [raft和pbft算法]https://zhuanlan.zhihu.com/p/35847127 ","link":"https://GeekGhc.github.io/post/fen-bu-shi-gong-shi-suan-fa/"},{"title":"Go并发实践-Channel","content":"概念介绍 想必在Go的并发领域中都听说过一句话： Don’t communicate by sharing memory, share memory by communicating. 解释就是：不要用共享内存的方式进行通信，而是用通信的方式共享内存 而这里的通信方式也就是go所提供的channel，channel 作为Go中特有的数据结构。和其他的并发原语不一样的是，可以直接使用，无需引用额外的包。 通常结合Go中著名的goroutine使用一起提供更加轻便的并发方案，同时也演进了很多并发模式 基本使用 在channel定义上可以分为只能接受、只能发送和既可以发送和接收这三种： var xx chan struct{} // 可以发送接收 var xx chan&lt;- struct{} // 只能接收 var xx &lt;-chan struct{} // 只能发送 箭头的匹配遵循最左结合规则 通过make可以初始化channel,当然和切片一样，可以声明其容量。默认则为0。因为区分为buffered channel和 unbuffered channel ch1 := make(chan int) //unbuffered channel ch2 := make(chan int,123) //buffered channel 常用操作包含了发送、接收、关闭等 ch1 &lt;- 100 //发送数据 x &lt;- ch1 //接收数据 &lt;- ch1 //丢弃处理 close(ch1) //关闭chan 以上都是一些基本使用，有兴趣或者详情的可以查阅官方文档即可，不是本次重点 底层理解 如果要想掌握channel的功能和特性，就需要了解本身的底层实现。 数据结构 首先看下基本数据结构：https://github.com/golang/go/blob/master/src/runtime/chan.go#L32 type hchan struct { qcount uint // 循环队列的元素大小 == len() dataqsiz uint // 循环队列的大小 == cap() buf unsafe.Pointer // 循环队列的指针 elemsize uint16 // chan中元素大小和元素类型相关 closed uint32 // 是否close elemtype *_type // chan中的元素类型 sendx uint // send 在buf中的索引 recvx uint // recv 在buf中的索引 recvq waitq // receiver 等待队列 sendq waitq // send 等待队列 // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex //互斥锁，并发基础，资源保护 } type _type struct { size uintptr ptrdata uintptr // size of memory prefix holding all pointers hash uint32 tflag tflag align uint8 fieldAlign uint8 kind uint8 // function for comparing objects of this type // (ptr to object A, ptr to object B) -&gt; ==? equal func(unsafe.Pointer, unsafe.Pointer) bool // gcdata stores the GC type data for the garbage collector. // If the KindGCProg bit is set in kind, gcdata is a GC program. // Otherwise it is a ptrmask bitmap. See mbitmap.go for details. gcdata *byte str nameOff ptrToThis typeOff } type waitq struct { first *sudog last *sudog } 初始化 func makechan(t *chantype, size int) *hchan { elem := t.elem // compiler checks this but be safe. if elem.size &gt;= 1&lt;&lt;16 { throw(&quot;makechan: invalid channel element type&quot;) } if hchanSize%maxAlign != 0 || elem.align &gt; maxAlign { throw(&quot;makechan: bad alignment&quot;) } mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem &gt; maxAlloc-hchanSize || size &lt; 0 { panic(plainError(&quot;makechan: size out of range&quot;)) } // Hchan does not contain pointers interesting for GC when elements stored in buf do not contain pointers. // buf points into the same allocation, elemtype is persistent. // SudoG's are referenced from their owning thread so they can't be collected. // TODO(dvyukov,rlh): Rethink when collector can move allocated objects. var c *hchan switch { case mem == 0: // Queue or element size is zero. c = (*hchan)(mallocgc(hchanSize, nil, true)) // Race detector uses this location for synchronization. c.buf = c.raceaddr() case elem.ptrdata == 0: // Elements do not contain pointers. // Allocate hchan and buf in one call. c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: // Elements contain pointers. c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) if debugChan { print(&quot;makechan: chan=&quot;, c, &quot;; elemsize=&quot;, elem.size, &quot;; dataqsiz=&quot;, size, &quot;\\n&quot;) } return c } 上面的整体初始化流程可以概括为： channel初始化size检查 声明的是unbuffered channel，不创建buf。见 c.buf = c.raceaddr() 声明的不是指针类型，则分配一段连续的内存和buf给到chan 声明的为指针类型，单独申请buf：c.buf = mallocgc(mem, elem, true) 初始化结构参数 数据Send 这里以数据发送为例，最终编译时会调用chansend1: https://github.com/golang/go/blob/master/src/runtime/chan.go#L158 我们解析下期中的逻辑 func chansend1(c *hchan, elem unsafe.Pointer) { chansend(c, elem, true, getcallerpc()) } func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { if c == nil { if !block { return false } gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(&quot;unreachable&quot;) } ...... if !block &amp;&amp; c.closed == 0 &amp;&amp; full(c) { return false } } 判断chan是否为nil,如果是的话，通过gopark将调用者的groutine阻塞并休眠 if !block &amp;&amp; c.closed == 0 &amp;&amp; ((c.dataqsiz == 0 &amp;&amp; c.recvq.first == nil) || (c.dataqsiz &gt; 0 &amp;&amp; c.qcount == c.dataqsiz)) { return false } 如果本身chan没有close,并且已满时，如果不想阻塞(取决于block)当前的调用直接返回 lock(&amp;c.lock) if c.closed != 0 { unlock(&amp;c.lock) panic(plainError(&quot;send on closed channel&quot;)) } 这里是值得注意的一部分，经常会遇到这样的场景：如果chan已经被close了，再往里面发数据就会panic if sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() { unlock(&amp;c.lock) }, 3) return true } 如果等待的队列有等待的recevier,那么将数据交给他进行处理，这样就可以不用再放入buf中，减少操作开支 具体数据处理参考：memmove(dst, src, t.size) if c.qcount &lt; c.dataqsiz { // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) if raceenabled { raceacquire(qp) racerelease(qp) } typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(&amp;c.lock) return true } 到这里就是常见的发送数据场景：当前没有receiver,这时数据押入循环队列，返回成功 if !block { unlock(&amp;c.lock) return false } ... 最后就是处理buf满的情况，这时的goroutine就会加入到发送等待队列中，直到下次唤醒。 数据Receive https://github.com/golang/go/blob/master/src/runtime/chan.go#L454 Chan Close https://github.com/golang/go/blob/master/src/runtime/chan.go#L355 避坑指南 问题Case 无论是在开源的知名项目还是日常开发中，我们在引入channle这个新的概念都会遇到不少问题，最常见的无非就是panic和goroutine泄露 经过上面的底层结果分析，不难发现，panic的情况总结可以为三种 Close 为nil的channel Close 已经close的channel Send 已经被close的channel 主要原因也说的比较清楚：创建了unbuffered chan:applyConfChan，这个gorountine中只有一个receiver，但是send是在业务逻辑的一个loop(循环)中，这样子gorountine就会block因为没有接收和发送同步。 如果要解决的话也很简单：修改发送逻辑，业务循环处理完成后再send即可 总结建议 channel在不同状态时我们都需要注意，具体总结如下 nil empty Buf full Not empty&amp;full closed send block ok block ok receiver block block ok ok close panic block ok&amp;保留已有数据 ok&amp;保留已有数据 实践运用 综合channel的基本特性和原理，通常我们可以总结为如下的场景： 1-发布订阅 最典型的应该就是生产消费，是可以当做内部消息队列的。而其设计的也是多生产者和多消费者模型 2-数据传递 作为消息的通信渠道，在通信上可以将数据进行处理传递 3-信号处理 其本身也是基于生产和发布的模式再结合chan的特性，可以很好的实现基于信号的处理 4-锁 基于基本结构实现的互斥，因此也可以实现互斥锁的机制 数据传递 这个场景其实比较常见，举例来说就是：有4个goroutine，可以为其编号。如果需要定时顺序打印各自编号，这个就是个数据传递的过程。 应用到现有的设计模式就是责任链模式，在很多上下文处理都有所使用 那怎么实现这个过程，其实可以定义变量令牌作为标识，哪个goroutine拿到令牌就做任务处理，这里我们只做打印 //定义令牌 type token struct{} func newWorker(id int, ch chan token, nextCh chan token) { for { // 取得令牌 token := &lt;-ch // 打印处理，这里可以针对token做一系列的业务处理，最终再下发token fmt.Println(&quot;data = &quot;, id) time.Sleep(time.Second) nextCh &lt;- token } } func TestChannel(t *testing.T) { chs := []chan token{make(chan token), make(chan token), make(chan token), make(chan token)} // 创建n个worker goroutine for i := 0; i &lt; 4; i++ { go newWorker(i, chs[i], chs[(i+1)%4]) } //令牌开始传递，不用指定顺序，这里从0开始 chs[0] &lt;- struct{}{} //常驻执行 select {} } === RUN TestChannel data = 0 data = 1 data = 2 data = 3 data = 0 data = 1 ... 这样的场景需要注意的是，当前的goroutine只需要关注自己的token即可，内部的处理也会是基于接收到的token进行处理。处理完成之后进行传递即可 信号处理 想象一下传统的notify/wait的场景，其实利用channnel本身的特性也可以实现。主要就是基于chan为空，那么receiver就会进行阻塞等待，当有消息来时或者关闭chan会进行响应处理这个特性 举例来说就是通常我们需要在程序退出之前执行一些连接关闭、文件close等操作。实际上很多框架的优雅退出都是基于此来实现的 可以先看一个最简单的信号通知的case: func TestQuit(t *testing.T) { g := make(chan int) quit := make(chan struct{}) go func() { for { select { case v := &lt;-g: t.Log(v) case &lt;-quit: t.Log(&quot;service 退出&quot;) return } } }() for i := 0; i &lt; 3; i++ { g &lt;- i } quit &lt;- struct{}{} t.Log(&quot;TestQuit 退出&quot;) } 而在实现程序的优雅退出时，我们需要考虑这样的情况： 程序在关闭时会有一些流程需要处理，这些流程中间都会花费一些时间，比如说上面的的链接关闭，文件关闭等。这个时候程序是处理一个关闭中的状态 而这个关闭的过程如果太久肯定是不可接受的，因此需要设置超时时间，到达这个时间，可以直接退出。 程序完全退出时，这个时候才是真正的closed 因此针对以上考虑，我们可以设计优雅关闭的流程如下： func TestGracefulDown(t *testing.T) { var closing = make(chan struct{}) var closed = make(chan struct{}) go func() { // 模拟业务处理 for { select { case &lt;-closing: return default: // 业务处理流程 time.Sleep(100 * time.Millisecond) } } }() // 处理CTRL+C等中断信号 termChan := make(chan os.Signal) signal.Notify(termChan, syscall.SIGINT, syscall.SIGTERM) &lt;-termChan close(closing) // 执行退出之前的清理动作 go doCleanup(closed) select { case &lt;-closed: case &lt;-time.After(time.Second): t.Log(&quot;清理超时，不等了&quot;) } t.Log(&quot;graceful log out...&quot;) } func doCleanup(closed chan struct{}) { //中间处理执行一分钟 time.Sleep(1 * time.Minute) close(closed) } 锁实现 在底层结构的分析中就可以看到，chan的内部就有互斥锁的保护。因此是可以保证只有当数据进入队列后(不一定指进入buf中)才可以被消费 最常见基于channel的实现主要有两种，这两种都是基于chan的特性决定的 初始化一个buffer为1的channel。上面也说了内部可以保证只有进入channel的数据才可以被receive，因此获得这个元素的就等价于获得了锁 初始化一个buffer为1的channel。在发送给一个full chan的时候会block这个前提下，可以知道可以发送元素到这个chan的就代表了获得了锁 通常锁场景是需要重试和超时的，而chan结合select可以很好的实现这些特性 两种方式其实都大同小异，这里以第一个场景为例 // 定义互斥锁 type Mutex struct { ch chan struct{} } // 初始化 func NewMutex() *Mutex { // buffer为1 mu := &amp;Mutex{make(chan struct{}, 1)} // 放入元素 mu.ch &lt;- struct{}{} return mu } // 请求锁，直到获取到 func (m *Mutex) Lock() { &lt;-m.ch } // 释放锁 func (m *Mutex) Unlock() { select { case m.ch &lt;- struct{}{}: default: panic(&quot;unlock of unlocked mutex&quot;) } } // 尝试获取锁 func (m *Mutex) TryLock() bool { select { case &lt;-m.ch: return true default: } return false } // 加入一个超时的设置 func (m *Mutex) LockTimeout(timeout time.Duration) bool { timer := time.NewTimer(timeout) select { case &lt;-m.ch: timer.Stop() return true case &lt;-timer.C: } return false } // 锁是否已被持有 func (m *Mutex) IsLocked() bool { return len(m.ch) == 0 } func TestLock(t *testing.T) { m := NewMutex() ok := m.TryLock() t.Logf(&quot;locked %v\\n&quot;, ok) ok = m.TryLock() t.Logf(&quot;locked %v\\n&quot;, ok) } 整个流程就是: 初始化锁的时候，先将一个元素推入chan， 之后成功获取这个元素的就代表了获得这个锁。 完成操作后再将这个元素推入chan即可，推入的过程就是释放锁。 因为元素在没有再次推入chan之前是不会有任何goroutine可以拿到这个元素的。这也就实现了锁的互斥 ","link":"https://GeekGhc.github.io/post/go-bing-fa-shi-jian-channel/"},{"title":"Redis分布式集群","content":"前言 Redis在3.0以后的版本支持了Cluster 那么我们首先会想到Cluster是解决怎样的应用场景 为了应对大流量访问下提供稳定的业务，集群化是存储的必然形态 之前的单点存储势必会有诸多隐患 而未来的发展趋势肯定是大数据和云计算的紧密集合 分布式架构也就可以很好的体现他的优势 分布式是以缩短单个任务的执行时间来提升效率的，而集群则是通过提高单位时间内执行的任务数来提升效率 解决方案 目前的Redis的集群化方案有三种 Twitter开发的twemproxy 豌豆荚开发的codis redis官方的redis-cluster redis-cluster是三个里性能最强大的 因为他使用去中心化的思想 使用hash slot方式 将16348个hash slot 覆盖到所有节点上 对于存储的每个key值 使用CRC16(KEY)&amp;16348=slot 得到他对应的hash slot 并在访问key时就去找他的hash slot在哪一个节点上 然后由当前访问节点从实际被分配了这个hash slot的节点去取数据 节点之间使用轻量协议通信 减少带宽占用 性能很高 自动实现负载均衡与高可用 自动实现failover 并且支持动态扩展 官方已经玩到可以1000个节点 实现的复杂度低 因为他的去中心化思想免去了proxy的消耗 是全新的思路 基本介绍 Redis 集群是一个可以在多个 Redis 节点之间进行数据共享的设施installation Redis 集群通过分区partition来提供一定程度的可用性availability： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。 Redis集群提供了以下两个好处： 将数据自动切分split到多个节点的能力。 当集群中的一部分节点失效或者无法进行通讯时， 仍然可以继续处理命令请求的能力。 集群分区 常见的分区规则有哈希分区和顺序分区，redis集群使用了哈希分区。RedisCluster采用了哈希分区的'虚拟槽分区'方式(哈希分区节点取余，一致性哈希分区和虚拟槽分区) RedisCluster采用嘘嘘你槽分区，所有的键根据哈希函数(CRC16[key]&amp;16383)映射到0-16383槽内，一共16384个槽位。每个节点维护部分及槽所映射的键值数据 哈希函数： Hash() = CRC16[key]&amp;16383 按位与 其中槽与节点的关系如下: 环境安装 本地安装Redis服务 我因为是mac环境 所以直接可以通过homebrew安装 $ brew install redis 为了搭建 目录这里新建了6个节点目录 将redis安装包 执行bin目录分别copy到每个节点目录 可以看到redis bin目录下的一些redis执行命令文件 集群搭建 将本地redis的redis-trib.rb复制到redis-cluster目录 复制redis的配置文件到每个节点 注意本机的conf文件的地址为 /usr/local/etc/redis.conf 完毕之后 集群文件的目录是这样的： 接下来就是针对每个节点的配置了 这里我们进入节点目录下的配置文件redis.conf 这里我们需要编辑的配置信息如下: port 7001 //节点端口 daemonize yes //配置redis作为守护进程运行，默认情况下，redis不是作为守护进程运行的 bind 127.0.0.1 //默认127.0.0.1，需要改为其他节点可访问的地址 cluster-node-timeout 5000 //集群超时时间 cluster-enabled yes //redis 集群 cluster-config-file nodes-7001.conf //指定节点配置信息 appendonly yes //存储方式 dir /Users/gehuachun/Develop/redis-cluster/redis01 //指定本地数据库路径 节点启动 每个节点配置完毕之后开始依次启动每个节点 这里假设启动是06这个节点 启动完毕之后查看redis的服务我们可以看到 集群创建 节点启动完毕之后开始创建集群 集群文件目录终端执行 $ redis-trib.rb create --replicas 1 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 其中--replices 1指定了为每个节点分配一个从节点 安装过程如果遇到插件报错 可以尝试： 那么在创建的过程中你将会看到如下信息： 最终集群创建成功时 你会看到 注意看后面的返回信息 这里指示了急群众主从节点的关系信息 比如： 可以看到7005是7001这个节点的从节点 因为这在创建集群的6个节点所生成的主从关系 当然你也可以单独指定主从节点 那么创建集群完毕之后再去查看集群节点目录 你会发现多了日志存储文件 集群连接 既然集群创建爱好了 我们开始尝试连接 这里我们先从7001节点开始并查看集群的信息 $ redis-cli -c -h 127.0.0.1 -p 7001 $ cluster nodes //查看集群信息 连接上7001这个节点 可以测试下与其他节点的通信 即与节点握手 就像之前说的 我们是可以手动建立主从关系的 我们再查看节点信息时 看好节点的id 槽的重新分配 集群创建完后 我们可以对所给我们的hash进行重新分配 比如: all 表示节点的槽位全部重新洗牌 分配后的槽 可以看到7003这个节点多了500个槽 也就是 0-248 5461-5711 这之间的500个 集群是为了处理一块业务 比如数据的存储 不过是通过不同的机器 如果没有重新分配槽 那么我这里新起了集群 最终的槽点的分布式这样的 注意主节点分配的信息 集群正常启动后，在每个redis.conf里加上 masterauth “12345678” requiredpass “12345678” 当主节点下线时，从节点会变成主节点，用户和密码是很有必要的，设置成一致 集群节点之间的通信 1.节点之间采用Gossip协议进行通信，Gossip协议就是指节点彼此之间不断通信交换信息 当主从角色变化或新增节点，彼此通过ping/pong进行通信知道全部节点的最新状态并达到集群同步 Gossip协议 Gossip协议的主要职责就是信息交换，信息交换的载体就是节点之间彼此发送的Gossip消息，常用的Gossip消息有ping消息、pong消息、meet消息、fail消息 meet消息：用于通知新节点加入，消息发送者通知接收者加入到当前集群，meet消息通信完后，接收节点会加入到集群中，并进行周期性ping pong交换 ping消息：集群内交换最频繁的消息，集群内每个节点每秒向其它节点发ping消息，用于检测节点是在在线和状态信息，ping消息发送封装自身节点和其他节点的状态数据； pong消息，当接收到ping meet消息时，作为响应消息返回给发送方，用来确认正常通信，pong消息也封闭了自身状态数据； fail消息：当节点判定集群内的另一节点下线时，会向集群内广播一个fail消息， 集群扩容 这也是分布式存储最常见的需求，当我们存储不够用时，要考虑扩容 启动新节点 这里启动7007 和7008以备用 这里新增了两目录redis07和redis08 复制之前的节点配置文件 需要修改的就是port和dir数据存储的路径这两个参数 集群新增节点 ./redis-trib.rb add-node 命令中 7007是新增的主节点 7001则是集群中已存在的节点 添加完毕后再查看下集群的节点信息 发现多了7007这个新的主节点 添加从节点 现在将7008添加作为7007的从节点 我们需要7007 的nodeid 可以通过clutser nodes查看 可以看到7c22cf36b86bc9efcd5d4cd8cab300462090b087 这个是7007的节点id --slave表示添加从节点 7008为需要加入到集群的从节点 7001为集群中已经存在的节点 现在可以再去查看集群各节点信息 OK 添加完毕 但是还没有结束 新添加的7007节点并没有为他分配槽 所以他还算不上主节点 和上面提过的一样 为7007这个节点重新分配节点 7007的节点id为7c22cf36b86bc9efcd5d4cd8cab300462090b087 redis-trib.rb reshard 127.0.0.7 //为新主节点重新分配solt How many slots do you want to move (from 1 to 16384)? 1000 //设置slot数1000 What is the receiving node ID? 7c22cf36b86bc9efcd5d4cd8cab300462090b087 //新节点node id Source node #1:all //表示全部节点重新洗牌 新增完毕 再次查看集群信息 删除节点 删除已经占有hash槽的结点会失败 所以删除前需要把槽位分配出去即可 指定节点的id即可 比如这里删除7008 $ redis-trib.rb del-node 127.0.0.1:7008 2113ab21277cb952d77d269fe69f8a852000d949 查看删除后的集群信息 此时的节点也算是下线完成 故障处理 redis集群实现了高可用，当集群内少量节点出现故障时，通过故障转移可以保证集群正常对外提供服务。当集群里某个节点出现了问题，redis集群内的节点通过ping pong消息发现节点是否健康，是否有故障，其实主要环节也包括了 主观下线和客观下线； 主观下线：指某个节点认为另一个节点不可用，即下线状态，当然这个状态不是最终的故障判定，只能代表这个节点自身的意见，也有可能存在误判 下线流程: 节点a发送ping消息给节点b ,如果通信正常将接收到pong消息，节点a更新最近一次与节点b的通信时间； 2.如果节点a与节点b通信出现问题则断开连接，下次会进行重连，如果一直通信失败，则它们的最后通信时间将无法更新； 3.节点a内的定时任务检测到与节点b最后通信时间超过cluster_note-timeout时，更新本地对节点b的状态为主观下线(pfail） 客观下线：指真正的下线，集群内多个节点都认为该节点不可用，达成共识，将它下线，如果下线的节点为主节点，还要对它进行故障转移 假如节点a标记节点b为主观下线，一段时间后节点a通过消息把节点b的状态发到其它节点，当节点c接受到消息并解析出消息体时，会发现节点b的pfail状态时，会触发客观下线流程 当下线为主节点时，此时redis集群为统计持有槽的主节点投票数是否达到一半，当下线报告统计数大于一半时，被标记为客观下线状态。 故障恢复 故障主节点下线后，如果下线节点的是主节点，则需要在它的从节点中选一个替换它，保证集群的高可用；转移过程如下： 1.资格检查：检查该从节点是否有资格替换故障主节点，如果此从节点与主节点断开过通信，那么当前从节点不具体故障转移； 2.准备选举时间：当从节点符合故障转移资格后，更新触发故障选举时间，只有到达该时间后才能执行后续流程； 3.发起选举：当到达故障选举时间时，进行选举； 4.选举投票：只有持有槽的主节点才有票，会处理故障选举消息，投票过程其实是一个领导者选举（选举从节点为领导者）的过程，每个主节点只能投一张票给从节点，当从节点收集到足够的选票（大于N/2+1）后，触发替换主节点操作，撤销原故障主节点的槽，委派给自己，并广播自己的委派消息，通知集群内所有节点。 RedisCluster 缺陷 1.键的批量操作支持有限，比如mset、mget,如果多个键映射再不同的槽，就不支持了 2.键的事务支持有限，当多个key分布在不同的节点时无法使用事务，同一个节点时支持事务的 3.键是数据分区的最小粒度，不能讲一个很大的键值映射到不同的节点 4.不支持多数据库，只有0,select 0 5.复制结构只支持单层结构，不支持树状结构 ","link":"https://GeekGhc.github.io/post/redis-fen-bu-shi-ji-qun/"},{"title":"Redis基本高级特性以及性能调优(转载)","content":"概述 Redis是一个开源的，基于内存的结构化数据存储媒介，可以作为数据库、缓存服务或消息服务使用。 Redis支持多种数据结构，包括字符串、哈希表、链表、集合、有序集合、位图、Hyperloglogs等。 Redis具备LRU淘汰、事务实现、以及不同级别的硬盘持久化等能力，并且支持副本集和通过Redis Sentinel实现的高可用方案，同时还支持通过Redis Cluster实现的数据自动分片能力。 Redis的主要功能都基于单线程模型实现，也就是说Redis使用一个线程来服务所有的客户端请求，同时Redis采用了非阻塞式IO，并精细地优化各种命令的算法时间复杂度，这些信息意味着： Redis是线程安全的（因为只有一个线程），其所有操作都是原子的，不会因并发产生数据异常 Redis的速度非常快（因为使用非阻塞式IO，且大部分命令的算法时间复杂度都是O(1)) 使用高耗时的Redis命令是很危险的，会占用唯一的一个线程的大量处理时间，导致所有的请求都被拖慢。（例如时间复杂度为O(N)的KEYS命令，严格禁止在生产环境中使用 Redis的数据结构和相关常用命令 完整的Redis命令集，或了解某个命令的详细使用方法，请参考官方文档：https://redis.io/commands Key Redis采用Key-Value型的基本数据结构，任何二进制序列都可以作为Redis的Key使用（例如普通的字符串或一张JPEG图片） 关于Key的一些注意事项： 不要使用过长的Key。例如使用一个1024字节的key就不是一个好主意，不仅会消耗更多的内存，还会导致查找的效率降低 Key短到缺失了可读性也是不好的，例如u1000flw比起user:1000:followers来说，节省了寥寥的存储空间，却引发了可读性和可维护性上的麻烦 最好使用统一的规范来设计Key，比如”object-type:id:attr”，以这一规范设计出的Key可能是”user:1000″或”comment:1234:reply-to” Redis允许的最大Key长度是512MB（对Value的长度限制也是512MB） String String是Redis的基础数据类型，Redis没有Int、Float、Boolean等数据类型的概念，所有的基本类型在Redis中都以String体现。 与String相关的常用命令： SET：为一个key设置value，可以配合EX/PX参数指定key的有效期，通过NX/XX参数针对key是否存在的情况进行区别操作，时间复杂度O(1) GET：获取某个key对应的value，时间复杂度O(1) GETSET：为一个key设置value，并返回该key的原value，时间复杂度O(1) MSET：为多个key设置value，时间复杂度O(N) MSETNX：同MSET，如果指定的key中有任意一个已存在，则不进行任何操作，时间复杂度O(N) MGET：获取多个key对应的value，时间复杂度O(N) 上文提到过，Redis的基本数据类型只有String，但Redis可以把String作为整型或浮点型数字来使用，主要体现在INCR、DECR类的命令上： INCR：将key对应的value值自增1，并返回自增后的值。只对可以转换为整型的String数据起作用。时间复杂度O(1) INCRBY：将key对应的value值自增指定的整型数值，并返回自增后的值。只对可以转换为整型的String数据起作用。时间复杂度O(1) DECR/DECRBY：同INCR/INCRBY，自增改为自减。 INCR/DECR系列命令要求操作的value类型为String，并可以转换为64位带符号的整型数字，否则会返回错误。 也就是说，进行INCR/DECR系列命令的value，必须在[-2^63 ~ 2^63 – 1]范围内。 前文提到过，Redis采用单线程模型，天然是线程安全的，这使得INCR/DECR命令可以非常便利的实现高并发场景下的精确控制。 例1：库存控制 在高并发场景下实现库存余量的精准校验，确保不出现超卖的情况。 设置库存总量： SET inv:remain &quot;100&quot; 库存扣减+余量校验： `DECR inv:remain` 当DECR命令返回值大于等于0时，说明库存余量校验通过，如果返回小于0的值，则说明库存已耗尽。 假设同时有300个并发请求进行库存扣减，Redis能够确保这300个请求分别得到99到**-200**的返回值，每个请求得到的返回值都是唯一的，绝对不会找出现两个请求得到一样的返回值的情况。 例2：自增序列生成 实现类似于RDBMS的Sequence功能，生成一系列唯一的序列号 设置序列起始值： SET sequence &quot;10000&quot; 获取一个序列值： INCR sequence 直接将返回值作为序列使用即可。 获取一批（如100个）序列值： INCRBY sequence 100 假设返回值为N，那么[N – 99 ~ N]的数值都是可用的序列值。 当多个客户端同时向Redis申请自增序列时，Redis能够确保每个客户端得到的序列值或序列范围都是全局唯一的，绝对不会出现不同客户端得到了重复的序列值的情况。 List Redis的List是链表型的数据结构，可以使用LPUSH/RPUSH/LPOP/RPOP等命令在List的两端执行插入元素和弹出元素的操作。虽然List也支持在特定index上插入和读取元素的功能，但其时间复杂度较高（O(N)，应小心使用。 与List相关的常用命令： LPUSH：向指定List的左侧（即头部）插入1个或多个元素，返回插入后的List长度。时间复杂度O(N)，N为插入元素的数量 RPUSH：同LPUSH，向指定List的右侧（即尾部）插入1或多个元素 LPOP：从指定List的左侧（即头部）移除一个元素并返回，时间复杂度O(1) RPOP：同LPOP，从指定List的右侧（即尾部）移除1个元素并返回 LPUSHX/RPUSHX：与LPUSH/RPUSH类似，区别在于，LPUSHX/RPUSHX操作的key如果不存在，则不会进行任何操作 LLEN：返回指定List的长度，时间复杂度O(1) LRANGE：返回指定List中指定范围的元素（双端包含，即LRANGE key 0 10会返回11个元素），时间复杂度O(N)。应尽可能控制一次获取的元素数量，一次获取过大范围的List元素会导致延迟，同时对长度不可预知的List，避免使用LRANGE key 0 -1这样的完整遍历操作。 应谨慎使用的List相关命令： LINDEX：返回指定List指定index上的元素，如果index越界，返回nil。index数值是回环的，即-1代表List最后一个位置，-2代表List倒数第二个位置。时间复杂度O(N) LSET：将指定List指定index上的元素设置为value，如果index越界则返回错误，时间复杂度O(N)，如果操作的是头/尾部的元素，则时间复杂度为O(1) LINSERT：向指定List中指定元素之前/之后插入一个新元素，并返回操作后的List长度。如果指定的元素不存在，返回-1。如果指定key不存在，不会进行任何操作，时间复杂度O(N) 由于Redis的List是链表结构的，上述的三个命令的算法效率较低，需要对List进行遍历，命令的耗时无法预估，在List长度大的情况下耗时会明显增加，应谨慎使用。 换句话说，Redis的List实际是设计来用于实现队列，而不是用于实现类似ArrayList这样的列表的。如果你不是想要实现一个双端出入的队列，那么请尽量不要使用Redis的List数据结构。 为了更好支持队列的特性，Redis还提供了一系列阻塞式的操作命令，如BLPOP/BRPOP等，能够实现类似于BlockingQueue的能力，即在List为空时，阻塞该连接，直到List中有对象可以出队时再返回。针对阻塞类的命令，此处不做详细探讨，请参考官方文档（https://redis.io/topics/data-types-intro） 中”Blocking operations on lists”一节。 Hash Hash即哈希表，Redis的Hash和传统的哈希表一样，是一种field-value型的数据结构，可以理解成将HashMap搬入Redis。 Hash非常适合用于表现对象类型的数据，用Hash中的field对应对象的field即可。 Hash的优点 可以实现二元查找，如”查找ID为1000的用户的年龄” 比起将整个对象序列化后作为String存储的方法，Hash能够有效地减少网络传输的消耗 当使用Hash维护一个集合时，提供了比List效率高得多的随机访问命令 与Hash相关的常用命令 HSET：将key对应的Hash中的field设置为value。如果该Hash不存在，会自动创建一个。时间复杂度O(1) HGET：返回指定Hash中field字段的值，时间复杂度O(1) HMSET/HMGET：同HSET和HGET，可以批量操作同一个key下的多个field，时间复杂度：O(N)，N为一次操作的field数量 HSETNX：同HSET，但如field已经存在，HSETNX不会进行任何操作，时间复杂度O(1) HEXISTS：判断指定Hash中field是否存在，存在返回1，不存在返回0，时间复杂度O(1) HDEL：删除指定Hash中的field（1个或多个），时间复杂度：O(N)，N为操作的field数量 HINCRBY：同INCRBY命令，对指定Hash中的一个field进行INCRBY，时间复杂度O(1) 应谨慎使用的Hash相关命令： HGETALL：返回指定Hash中所有的field-value对。返回结果为数组，数组中field和value交替出现。时间复杂度O(N) HKEYS/HVALS：返回指定Hash中所有的field/value，时间复杂度O(N) 上述三个命令都会对Hash进行完整遍历，Hash中的field数量与命令的耗时线性相关，对于尺寸不可预知的Hash，应严格避免使用上面三个命令，而改为使用HSCAN命令进行游标式的遍历，具体请见 https://redis.io/commands/scan Set Redis Set是无序的，不可重复的String集合。 与Set相关的常用命令 SADD：向指定Set中添加1个或多个member，如果指定Set不存在，会自动创建一个。时间复杂度O(N)，N为添加的member个数 SREM：从指定Set中移除1个或多个member，时间复杂度O(N)，N为移除的member个数 SRANDMEMBER：从指定Set中随机返回1个或多个member，时间复杂度O(N)，N为返回的member个数 SPOP：从指定Set中随机移除并返回count个member，时间复杂度O(N)，N为移除的member个数 SCARD：返回指定Set中的member个数，时间复杂度O(1) SISMEMBER：判断指定的value是否存在于指定Set中，时间复杂度O(1) SMOVE：将指定member从一个Set移至另一个Set 慎用的Set相关命令： SMEMBERS：返回指定Hash中所有的member，时间复杂度O(N) SUNION/SUNIONSTORE：计算多个Set的并集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数 SINTER/SINTERSTORE：计算多个Set的交集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数 SDIFF/SDIFFSTORE：计算1个Set与1或多个Set的差集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数 上述几个命令涉及的计算量大，应谨慎使用，特别是在参与计算的Set尺寸不可知的情况下，应严格避免使用。 可以考虑通过SSCAN命令遍历获取相关Set的全部member（具体请见 https://redis.io/commands/scan ）如果需要做并集/交集/差集计算，可以在客户端进行，或在不服务实时查询请求的Slave上进行。 Sorted Set Redis Sorted Set是有序的、不可重复的String集合。Sorted Set中的每个元素都需要指派一个分数(score)，Sorted Set会根据score对元素进行升序排序。如果多个member拥有相同的score，则以字典序进行升序排序。 Sorted Set非常适合用于实现排名。 Sorted Set的主要命令： ZADD：向指定Sorted Set中添加1个或多个member，时间复杂度O(Mlog(N))，M为添加的member数量，N为Sorted Set中的member数量 ZREM：从指定Sorted Set中删除1个或多个member，时间复杂度O(Mlog(N))，M为删除的member数量，N为Sorted Set中的member数量 ZCOUNT：返回指定Sorted Set中指定score范围内的member数量，时间复杂度：O(log(N)) ZCARD：返回指定Sorted Set中的member数量，时间复杂度O(1) ZSCORE：返回指定Sorted Set中指定member的score，时间复杂度O(1) ZRANK/ZREVRANK：返回指定member在Sorted Set中的排名，ZRANK返回按升序排序的排名，ZREVRANK则返回按降序排序的排名。时间复杂度O(log(N)) ZINCRBY：同INCRBY，对指定Sorted Set中的指定member的score进行自增，时间复杂度O(log(N)) 慎用的Sorted Set相关命令： ZRANGE/ZREVRANGE：返回指定Sorted Set中指定排名范围内的所有member，ZRANGE为按score升序排序，ZREVRANGE为按score降序排序，时间复杂度O(log(N)+M)，M为本次返回的member数 ZRANGEBYSCORE/ZREVRANGEBYSCORE：返回指定Sorted Set中指定score范围内的所有member，返回结果以升序/降序排序，min和max可以指定为-inf和+inf，代表返回所有的member。时间复杂度O(log(N)+M) ZREMRANGEBYRANK/ZREMRANGEBYSCORE：移除Sorted Set中指定排名范围/指定score范围内的所有member。时间复杂度O(log(N)+M) 上述几个命令，应尽量避免传递[0 -1]或[-inf +inf]这样的参数，来对Sorted Set做一次性的完整遍历，特别是在Sorted Set的尺寸不可预知的情况下。 可以通过ZSCAN命令来进行游标式的遍历（具体请见 https://redis.io/commands/scan ），或通过LIMIT参数来限制返回member的数量（适用于ZRANGEBYSCORE和ZREVRANGEBYSCORE命令），以实现游标式的遍历。 Bitmap和HyperLogLog Redis的这两种数据结构相较之前的并不常用，在本文中只做简要介绍，如想要详细了解这两种数据结构与其相关的命令，请参考官方文档 https://redis.io/topics/data-types-intro 中的相关章节 Bitmap在Redis中不是一种实际的数据类型，而是一种将String作为Bitmap使用的方法。可以理解为将String转换为bit数组。使用Bitmap来存储true/false类型的简单数据极为节省空间。 HyperLogLogs是一种主要用于数量统计的数据结构，它和Set类似，维护一个不可重复的String集合，但是HyperLogLogs并不维护具体的member内容，只维护member的个数。 也就是说，HyperLogLogs只能用于计算一个集合中不重复的元素数量，所以它比Set要节省很多内存空间。 其他常用命令 EXISTS：判断指定的key是否存在，返回1代表存在，0代表不存在，时间复杂度O(1) DEL：删除指定的key及其对应的value，时间复杂度O(N)，N为删除的key数量 EXPIRE/PEXPIRE：为一个key设置有效期，单位为秒或毫秒，时间复杂度O(1) TTL/PTTL：返回一个key剩余的有效时间，单位为秒或毫秒，时间复杂度O(1) RENAME/RENAMENX：将key重命名为newkey。使用RENAME时，如果newkey已经存在，其值会被覆盖；使用RENAMENX时，如果newkey已经存在，则不会进行任何操作，时间复杂度O(1) TYPE：返回指定key的类型，string, list, set, zset, hash。时间复杂度O(1) CONFIG GET：获得Redis某配置项的当前值，可以使用*通配符，时间复杂度O(1) CONFIG SET：为Redis某个配置项设置新值，时间复杂度O(1) CONFIG REWRITE：让Redis重新加载redis.conf中的配置 数据持久化 Redis提供了将数据定期自动持久化至硬盘的能力，包括RDB和AOF两种方案，两种方案分别有其长处和短板，可以配合起来同时运行，确保数据的稳定性。 必须使用数据持久化吗？ Redis的数据持久化机制是可以关闭的。如果你只把Redis作为缓存服务使用，Redis中存储的所有数据都不是该数据的主体而仅仅是同步过来的备份，那么可以关闭Redis的数据持久化机制。 但通常来说，仍然建议至少开启RDB方式的数据持久化，因为： RDB方式的持久化几乎不损耗Redis本身的性能，在进行RDB持久化时，Redis主进程唯一需要做的事情就是fork出一个子进程，所有持久化工作都由子进程完成 Redis无论因为什么原因crash掉之后，重启时能够自动恢复到上一次RDB快照中记录的数据。这省去了手工从其他数据源（如DB）同步数据的过程，而且要比其他任何的数据恢复方式都要快 现在硬盘那么大，真的不缺那一点地方 RDB 采用RDB持久方式， Redis会定期保存数据快照至一个rbd文件中，并在启动时自动加载rdb文件，恢复之前保存的数据。可以在配置文件中配置Redis进行快照保存的时机： $ save [seconds] [changes] 意为在[seconds]秒内如果发生了[changes]次数据修改，则进行一次RDB快照保存，例如 save 60 100 会让Redis每60秒检查一次数据变更情况，如果发生了100次或以上的数据变更，则进行RDB快照保存。 可以配置多条save指令，让Redis执行多级的快照保存策略。 Redis默认开启RDB快照，默认的RDB策略如下: save 900 1 save 300 10 save 60 10000 也可以通过BGSAVE命令手工触发RDB快照保存。 RDB的优点： 对性能影响最小。如前文所述，Redis在保存RDB快照时会fork出子进程进行，几乎不影响Redis处理客户端请求的效率。 每次快照会生成一个完整的数据快照文件，所以可以辅以其他手段保存多个时间点的快照（例如把每天0点的快照备份至其他存储媒介中），作为非常可靠的灾难恢复手段。 使用RDB文件进行数据恢复比使用AOF要快很多。 RDB的缺点： 快照是定期生成的，所以在Redis crash时或多或少会丢失一部分数据。 如果数据集非常大且CPU不够强（比如单核CPU），Redis在fork子进程时可能会消耗相对较长的时间（长至1秒），影响这期间的客户端请求。 AOF 采用AOF持久方式时，Redis会把每一个写请求都记录在一个日志文件里。在Redis重启时，会把AOF文件中记录的所有写操作顺序执行一遍，确保数据恢复到最新。 AOF默认是关闭的，如要开启，进行如下配置： appendonly yes AOF提供了三种fsync配置，always/everysec/no， 通过配置项[appendfsync]指定 appendfsync no：不进行fsync，将flush文件的时机交给OS决定，速度最快 appendfsync always：每写入一条日志就进行一次fsync操作，数据安全性最高，但速度最慢 appendfsync everysec：折中的做法，交由后台线程每秒fsync一次 随着AOF不断地记录写操作日志，必定会出现一些无用的日志，例如某个时间点执行了命令SET key1“abc”，在之后某个时间点又执行了SET key1 “bcd”，那么第一条命令很显然是没有用的。 大量的无用日志会让AOF文件过大，也会让数据恢复的时间过长。 所以Redis提供了AOF rewrite功能，可以重写AOF文件，只保留能够把数据恢复到最新状态的最小写操作集。 AOF rewrite可以通过BGREWRITEAOF命令触发，也可以配置Redis定期自动进行： auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb 上面两行配置的含义是，Redis在每次AOF rewrite时，会记录完成rewrite后的AOF日志大小，当AOF日志大小在该基础上增长了100%后，自动进行AOF rewrite。同时如果增长的大小没有达到64mb，则不会进行rewrite。 AOF的优点： 最安全，在启用appendfsync always时，任何已写入的数据都不会丢失，使用在启用appendfsync everysec也至多只会丢失1秒的数据。 AOF文件在发生断电等问题时也不会损坏，即使出现了某条日志只写入了一半的情况，也可以使用redis-check-aof工具轻松修复。 AOF文件易读，可修改，在进行了某些错误的数据清除操作后，只要AOF文件没有rewrite，就可以把AOF文件备份出来，把错误的命令删除，然后恢复数据。 AOF的缺点： AOF文件通常比RDB文件更大 性能消耗比RDB高 数据恢复速度比RDB慢 内存管理与数据淘汰机制 最大内存设置 默认情况下，在32位OS中，Redis最大使用3GB的内存，在64位OS中则没有限制。 在使用Redis时，应该对数据占用的最大空间有一个基本准确的预估，并为Redis设定最大使用的内存。否则在64位OS中Redis会无限制地占用内存（当物理内存被占满后会使用swap空间），容易引发各种各样的问题。 通过如下配置控制Redis使用的最大内存： maxmemory 100mb 在内存占用达到了maxmemory后，再向Redis写入数据时，Redis会： 根据配置的数据淘汰策略尝试淘汰数据，释放空间 如果没有数据可以淘汰，或者没有配置数据淘汰策略，那么Redis会对所有写请求返回错误，但读请求仍然可以正常执行 在为Redis设置maxmemory时，需要注意： 如果采用了Redis的主从同步，主节点向从节点同步数据时，会占用掉一部分内存空间，如果maxmemory过于接近主机的可用内存，导致数据同步时内存不足。 所以设置的maxmemory不要过于接近主机可用的内存，留出一部分预留用作主从同步。 数据淘汰机制 Redis提供了5种数据淘汰策略： volatile-lru：使用LRU算法进行数据淘汰（淘汰上次使用时间最早的，且使用次数最少的key），只淘汰设定了有效期的key allkeys-lru：使用LRU算法进行数据淘汰，所有的key都可以被淘汰 volatile-random：随机淘汰数据，只淘汰设定了有效期的key allkeys-random：随机淘汰数据，所有的key都可以被淘汰 volatile-ttl：淘汰剩余有效期最短的key 最好为Redis指定一种有效的数据淘汰策略以配合maxmemory设置，避免在内存使用满后发生写入失败的情况。 一般来说，推荐使用的策略是volatile-lru，并辨识Redis中保存的数据的重要性。 对于那些重要的，绝对不能丢弃的数据（如配置类数据等），应不设置有效期，这样Redis就永远不会淘汰这些数据。 对于那些相对不是那么重要的，并且能够热加载的数据（比如缓存最近登录的用户信息，当在Redis中找不到时，程序会去DB中读取），可以设置上有效期，这样在内存不够时Redis就会淘汰这部分数据。 配置方法： maxmemory-policy volatile-lru #默认是noeviction，即不进行数据淘汰 Pipelining Redis提供许多批量操作的命令，如MSET/MGET/HMSET/HMGET等等，这些命令存在的意义是减少维护网络连接和传输数据所消耗的资源和时间。 例如连续使用5次SET命令设置5个不同的key，比起使用一次MSET命令设置5个不同的key，效果是一样的，但前者会消耗更多的RTT(Round Trip Time)时长，永远应优先使用后者。 然而，如果客户端要连续执行的多次操作无法通过Redis命令组合在一起，例如： SET a &quot;abc&quot; INCR b HSET c name &quot;hi&quot; 此时便可以使用Redis提供的pipelining功能来实现在一次交互中执行多条命令。 使用pipelining时，只需要从客户端一次向Redis发送多条命令（以rn）分隔，Redis就会依次执行这些命令，并且把每个命令的返回按顺序组装在一起一次返回，比如： $ (printf &quot;PINGrnPINGrnPINGrn&quot;; sleep 1) | nc localhost 6379 +PONG +PONG +PONG 大部分的Redis客户端都对Pipelining提供支持，所以开发者通常并不需要自己手工拼装命令列表。 Pipelining的局限性 Pipelining只能用于执行连续且无相关性的命令，当某个命令的生成需要依赖于前一个命令的返回时，就无法使用Pipelining了。 通过Scripting功能，可以规避这一局限性 事务与Scripting Pipelining能够让Redis在一次交互中处理多条命令，然而在一些场景下，我们可能需要在此基础上确保这一组命令是连续执行的。 比如获取当前累计的PV数并将其清0 &gt;GET vCount 12384 &gt; SET vCount 0 OK 如果在GET和SET命令之间插进来一个INCR vCount，就会使客户端拿到的vCount不准确。 Redis的事务可以确保复数命令执行时的原子性。也就是说Redis能够保证：一个事务中的一组命令是绝对连续执行的，在这些命令执行完成之前，绝对不会有来自于其他连接的其他命令插进去执行。 通过MULTI和EXEC命令来把这两个命令加入一个事务中： MULTI OK GET vCount QUEUED SET vCount 0 QUEUED EXEC 1) 12384 2) OK Redis在接收到MULTI命令后便会开启一个事务，这之后的所有读写命令都会保存在队列中但并不执行，直到接收到EXEC命令后，Redis会把队列中的所有命令连续顺序执行，并以数组形式返回每个命令的返回结果。 可以使用DISCARD命令放弃当前的事务，将保存的命令队列清空。 需要注意的是，Redis事务不支持回滚： 如果一个事务中的命令出现了语法错误，大部分客户端驱动会返回错误，2.6.5版本以上的Redis也会在执行EXEC时检查队列中的命令是否存在语法错误，如果存在，则会自动放弃事务并返回错误。 但如果一个事务中的命令有非语法类的错误（比如对String执行HSET操作），无论客户端驱动还是Redis都无法在真正执行这条命令之前发现，所以事务中的所有命令仍然会被依次执行。 在这种情况下，会出现一个事务中部分命令成功部分命令失败的情况，然而与RDBMS不同，Redis不提供事务回滚的功能，所以只能通过其他方法进行数据的回滚。 通过事务实现CAS Redis提供了WATCH命令与事务搭配使用，实现CAS乐观锁的机制。 假设要实现将某个商品的状态改为已售： if(exec(HGET stock:1001 state) == &quot;in stock&quot;) exec(HSET stock:1001 state &quot;sold&quot;); 这一伪代码执行时，无法确保并发安全性，有可能多个客户端都获取到了”in stock”的状态，导致一个库存被售卖多次。 使用WATCH命令和事务可以解决这一问题： exec(WATCH stock:1001); if(exec(HGET stock:1001 state) == &quot;in stock&quot;) { exec(MULTI); exec(HSET stock:1001 state &quot;sold&quot;); exec(EXEC); } WATCH的机制是：在事务EXEC命令执行时，Redis会检查被WATCH的key，只有被WATCH的key从WATCH起始时至今没有发生过变更，EXEC才会被执行。 如果WATCH的key在WATCH命令到EXEC命令之间发生过变化，则EXEC命令会返回失败。 Scripting 通过EVAL与EVALSHA命令，可以让Redis执行LUA脚本。这就类似于RDBMS的存储过程一样，可以把客户端与Redis之间密集的读/写交互放在服务端进行，避免过多的数据交互，提升性能。 Scripting功能是作为事务功能的替代者诞生的，事务提供的所有能力Scripting都可以做到。Redis官方推荐使用LUA Script来代替事务，前者的效率和便利性都超过了事务。 关于Scripting的具体使用，本文不做详细介绍，请参考官方文档 https://redis.io/commands/eval Redis性能调优 尽管Redis是一个非常快速的内存数据存储媒介，也并不代表Redis不会产生性能问题。 前文中提到过，Redis采用单线程模型，所有的命令都是由一个线程串行执行的，所以当某个命令执行耗时较长时，会拖慢其后的所有命令，这使得Redis对每个任务的执行效率更加敏感。 针对Redis的性能优化，主要从下面几个层面入手： 最初的也是最重要的，确保没有让Redis执行耗时长的命令 使用pipelining将连续执行的命令组合执行 操作系统的Transparent huge pages功能必须关闭： echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled 如果在虚拟机中运行Redis，可能天然就有虚拟机环境带来的固有延迟。 可以通过./redis-cli –intrinsic-latency 100命令查看固有延迟。同时如果对Redis的性能有较高要求的话，应尽可能在物理机上直接部署Redis。 检查数据持久化策略 考虑引入读写分离机制 长耗时命令 Redis绝大多数读写命令的时间复杂度都在O(1)到O(N)之间，在文本和官方文档中均对每个命令的时间复杂度有说明。 通常来说，O(1)的命令是安全的，O(N)命令在使用时需要注意，如果N的数量级不可预知，则应避免使用。 例如对一个field数未知的Hash数据执行HGETALL/HKEYS/HVALS命令，通常来说这些命令执行的很快，但如果这个Hash中的field数量极多，耗时就会成倍增长。 又如使用SUNION对两个Set执行Union操作，或使用SORT对List/Set执行排序操作等时，都应该严加注意。 避免在使用这些O(N)命令时发生问题主要有几个办法： 不要把List当做列表使用，仅当做队列来使用 通过机制严格控制Hash、Set、Sorted Set的大小 可能的话，将排序、并集、交集等操作放在客户端执行 绝对禁止使用KEYS命令 避免一次性遍历集合类型的所有成员，而应使用SCAN类的命令进行分批的，游标式的遍历 Redis提供了SCAN命令，可以对Redis中存储的所有key进行游标式的遍历，避免使用KEYS命令带来的性能问题。同时还有SSCAN/HSCAN/ZSCAN等命令，分别用于对Set/Hash/Sorted Set中的元素进行游标式遍历。SCAN类命令的使用请参考官方文档：https://redis.io/commands/scan Redis提供了Slow Log功能，可以自动记录耗时较长的命令。相关的配置参数有两个： slowlog-log-slower-than xxxms #执行时间慢于xxx毫秒的命令计入Slow Log slowlog-max-len xxx #Slow Log的长度，即最大纪录多少条Slow Log 使用SLOWLOG GET [number]命令，可以输出最近进入Slow Log的number条命令。 使用SLOWLOG RESET命令，可以重置Slow Log 网络引发的延迟 尽可能使用长连接或连接池，避免频繁创建销毁连接 客户端进行的批量数据操作，应使用Pipeline特性在一次交互中完成。具体请参照本文的Pipelining章节 数据持久化引发的延迟 Redis的数据持久化工作本身就会带来延迟，需要根据数据的安全级别和性能要求制定合理的持久化策略： AOF + fsync always的设置虽然能够绝对确保数据安全，但每个操作都会触发一次fsync，会对Redis的性能有比较明显的影响 AOF + fsync every second是比较好的折中方案，每秒fsync一次 AOF + fsync never会提供AOF持久化方案下的最优性能 使用RDB持久化通常会提供比使用AOF更高的性能，但需要注意RDB的策略配置 每一次RDB快照和AOF Rewrite都需要Redis主进程进行fork操作。fork操作本身可能会产生较高的耗时，与CPU和Redis占用的内存大小有关。 根据具体的情况合理配置RDB快照和AOF Rewrite时机，避免过于频繁的fork带来的延迟 Redis在fork子进程时需要将内存分页表拷贝至子进程，以占用了24GB内存的Redis实例为例，共需要拷贝24GB / 4kB * 8 = 48MB的数据。在使用单Xeon 2.27Ghz的物理机上，这一fork操作耗时216ms。 可以通过INFO命令返回的latest_fork_usec字段查看上一次fork操作的耗时（微秒） Swap引发的延迟 当Linux将Redis所用的内存分页移至swap空间时，将会阻塞Redis进程，导致Redis出现不正常的延迟。Swap通常在物理内存不足或一些进程在进行大量I/O操作时发生，应尽可能避免上述两种情况的出现。 /proc//smaps文件中会保存进程的swap记录，通过查看这个文件，能够判断Redis的延迟是否由Swap产生。如果这个文件中记录了较大的Swap size，则说明延迟很有可能是Swap造成的。 数据淘汰引发的延迟 当同一秒内有大量key过期时，也会引发Redis的延迟。在使用时应尽量将key的失效时间错开。 引入读写分离机制 Redis的主从复制能力可以实现一主多从的多节点架构，在这一架构下，主节点接收所有写请求，并将数据同步给多个从节点。 在这一基础上，我们可以让从节点提供对实时性要求不高的读请求服务，以减小主节点的压力。 尤其是针对一些使用了长耗时命令的统计类任务，完全可以指定在一个或多个从节点上执行，避免这些长耗时命令影响其他请求的响应。 主从复制与集群分片 主从复制 Redis支持一主多从的主从复制架构。一个Master实例负责处理所有的写请求，Master将写操作同步至所有Slave。 借助Redis的主从复制，可以实现读写分离和高可用： 实时性要求不是特别高的读请求，可以在Slave上完成，提升效率。特别是一些周期性执行的统计任务，这些任务可能需要执行一些长耗时的Redis命令，可以专门规划出1个或几个Slave用于服务这些统计任务 借助Redis Sentinel可以实现高可用，当Master crash后，Redis Sentinel能够自动将一个Slave晋升为Master，继续提供服务 启用主从复制非常简单，只需要配置多个Redis实例，在作为Slave的Redis实例中配置： slaveof 192.168.1.1 6379 #指定Master的IP和端口 当Slave启动后，会从Master进行一次冷启动数据同步，由Master触发BGSAVE生成RDB文件推送给Slave进行导入，导入完成后Master再将增量数据通过Redis Protocol同步给Slave。 之后主从之间的数据便一直以Redis Protocol进行同步 使用Sentinel做自动failover Redis的主从复制功能本身只是做数据同步，并不提供监控和自动failover能力，要通过主从复制功能来实现Redis的高可用，还需要引入一个组件：Redis Sentinel Redis Sentinel是Redis官方开发的监控组件，可以监控Redis实例的状态，通过Master节点自动发现Slave节点，并在监测到Master节点失效时选举出一个新的Master，并向所有Redis实例推送新的主从配置。 Redis Sentinel需要至少部署3个实例才能形成选举关系。 关键配置： 另外需要注意的是，Redis Sentinel实现的自动failover不是在同一个IP和端口上完成的，也就是说自动failover产生的新Master提供服务的IP和端口与之前的Master是不一样的，所以要实现HA，还要求客户端必须支持Sentinel，能够与Sentinel交互获得新Master的信息才行。 集群分片 为何要做集群分片： Redis中存储的数据量大，一台主机的物理内存已经无法容纳 Redis的写请求并发量大，一个Redis实例以无法承载 当上述两个问题出现时，就必须要对Redis进行分片了。 Redis的分片方案有很多种，例如很多Redis的客户端都自行实现了分片功能，也有向Twemproxy这样的以代理方式实现的Redis分片方案。然而首选的方案还应该是Redis官方在3.0版本中推出的Redis Cluster分片方案。 Redis Cluster的能力 能够自动将数据分散在多个节点上 当访问的key不在当前分片上时，能够自动将请求转发至正确的分片 当集群中部分节点失效时仍能提供服务 其中第三点是基于主从复制来实现的，Redis Cluster的每个数据分片都采用了主从复制的结构，原理和前文所述的主从复制完全一致，唯一的区别是省去了Redis Sentinel这一额外的组件，由Redis Cluster负责进行一个分片内部的节点监控和自动failover。 Redis Cluster分片原理 Redis Cluster中共有16384个hash slot，Redis会计算每个key的CRC16，将结果与16384取模，来决定该key存储在哪一个hash slot中，同时需要指定Redis Cluster中每个数据分片负责的Slot数。 Slot的分配在任何时间点都可以进行重新分配。 客户端在对key进行读写操作时，可以连接Cluster中的任意一个分片，如果操作的key不在此分片负责的Slot范围内，Redis Cluster会自动将请求重定向到正确的分片上。 hash tags 在基础的分片原则上，Redis还支持hash tags功能，以hash tags要求的格式明明的key，将会确保进入同一个Slot中。例如：{uiv}user:1000和{uiv}user:1001拥有同样的hash tag {uiv}，会保存在同一个Slot中。 使用Redis Cluster时，pipelining、事务和LUA Script功能涉及的key必须在同一个数据分片上，否则将会返回错误。如要在Redis Cluster中使用上述功能，就必须通过hash tags来确保一个pipeline或一个事务中操作的所有key都位于同一个Slot中。 有一些客户端（如Redisson）实现了集群化的pipelining操作，可以自动将一个pipeline里的命令按key所在的分片进行分组，分别发到不同的分片上执行。 但是Redis不支持跨分片的事务，事务和LUA Script还是必须遵循所有key在一个分片上的规则要求。 主从复制 vs 集群分片 在设计软件架构时，要如何在主从复制和集群分片两种部署方案中取舍呢？ 从各个方面看，Redis Cluster都是优于主从复制的方案 Redis Cluster能够解决单节点上数据量过大的问题 Redis Cluster能够解决单节点访问压力过大的问题 Redis Cluster包含了主从复制的能力 那是不是代表Redis Cluster永远是优于主从复制的选择呢？ 并不是。 软件架构永远不是越复杂越好，复杂的架构在带来显著好处的同时，一定也会带来相应的弊端。采用Redis Cluster的弊端包括： 维护难度增加。在使用Redis Cluster时，需要维护的Redis实例数倍增，需要监控的主机数量也相应增加，数据备份/持久化的复杂度也会增加。 同时在进行分片的增减操作时，还需要进行reshard操作，远比主从模式下增加一个Slave的复杂度要高。 客户端资源消耗增加。当客户端使用连接池时，需要为每一个数据分片维护一个连接池，客户端同时需要保持的连接数成倍增多，加大了客户端本身和操作系统资源的消耗。 性能优化难度增加。你可能需要在多个分片上查看Slow Log和Swap日志才能定位性能问题。 事务和LUA Script的使用成本增加。在Redis Cluster中使用事务和LUA Script特性有严格的限制条件，事务和Script中操作的key必须位于同一个分片上，这就使得在开发时必须对相应场景下涉及的key进行额外的规划和规范要求。 如果应用的场景中大量涉及事务和Script的使用，如何在保证这两个功能的正常运作前提下把数据平均分到多个数据分片中就会成为难点。 所以说，在主从复制和集群分片两个方案中做出选择时，应该从应用软件的功能特性、数据和访问量级、未来发展规划等方面综合考虑，只在确实有必要引入数据分片时再使用Redis Cluster。 综合上面几点考虑，如果单台主机的可用物理内存完全足以支撑对Redis的容量需求，且Redis面临的并发写压力距离Benchmark`值还尚有距离，建议采用主从复制的架构，可以省去很多不必要的麻烦。 同时，如果应用中大量使用pipelining和事务，也建议尽可能选择主从复制架构，可以减少设计和开发时的复杂度。 相关链接 https://learnku.com/articles/25070 ","link":"https://GeekGhc.github.io/post/redis-ji-ben-gao-ji-te-xing-yi-ji-xing-neng-diao-you-zhuan-zai/"},{"title":"源码分析之-Facades","content":"首先看一下Laravel官方文档对Facades的解释： Facades 为应用程序的 服务容器 中可用的类提供了一个「静态」接口。Laravel 本身附带许多的 facades，甚至你可能在不知情的状况下已经在使用他们！Laravel 「facades」作为在服务容器内基 类的「静态代理」拥有简洁、易表达的语法优点同时维持着比传统静态方法更高的可测试性和灵活 性。 Facades就是一组静态接口或者代理 他们多代表的是一组服务的访问。通过Facades可以访问绑定到服务容器里的各种服务。 之前有谈过路由这个Facades，他就是\\Illuminate\\Support\\Facades\\Route类的别名。他代理的就是注册到服务容器的router服务。通过Router我们可以访问使用router中的各种服务。 我们只需要关注使用，而其中的解析过程则是由laravel内部解析的。这样我们的代码可读性也会高了不少。 我们现在就来看看一个Facade注册之后是怎么使用在应用程序里的。当然这之前我们需要关注一个引用启动时ServiceProvider这里面的作用 //Class: \\Illuminate\\Foundation\\Http\\Kernel protected function sendRequestThroughRouter($request) { $this-&gt;app-&gt;instance('request', $request); Facade::clearResolvedInstance('request'); $this-&gt;bootstrap(); //请求最终都会通过Pipeline 进行dispatch return (new Pipeline($this-&gt;app)) -&gt;send($request) -&gt;through($this-&gt;app-&gt;shouldSkipMiddleware() ? [] : $this-&gt;middleware) -&gt;then($this-&gt;dispatchToRouter()); } //引导启动Laravel应用程序 public function bootstrap() { if (! $this-&gt;app-&gt;hasBeenBootstrapped()) { /**依次执行$bootstrappers中每一个bootstrapper的bootstrap()函数 $this-&gt;bootstrappers = [ 'Illuminate\\Foundation\\Bootstrap\\DetectEnvironment', 'Illuminate\\Foundation\\Bootstrap\\LoadConfiguration', 'Illuminate\\Foundation\\Bootstrap\\ConfigureLogging', 'Illuminate\\Foundation\\Bootstrap\\HandleExceptions', 'Illuminate\\Foundation\\Bootstrap\\RegisterFacades', 'Illuminate\\Foundation\\Bootstrap\\RegisterProviders', 'Illuminate\\Foundation\\Bootstrap\\BootProviders', ];*/ $this-&gt;app-&gt;bootstrapWith($this-&gt;bootstrappers()); } } 其中会执行 Illuminate\\Foundation\\Bootstrap\\RegisterFacades 在这个阶段会依次注册程序中需要用到的Facades // namespace Illuminate\\Foundation\\Bootstrap; class RegisterFacades { public function bootstrap(Application $app) { Facade::clearResolvedInstances(); Facade::setFacadeApplication($app); AliasLoader::getInstance(array_merge( $app-&gt;make('config')-&gt;get('app.aliases', []), $app-&gt;make(PackageManifest::class)-&gt;aliases() ))-&gt;register(); } } 这里会通过AliasLoader事例 为Facades注册别名 而这个别名对应的关系是在定义的app/config.php的配置文件中的 $aliases`数组中 'aliases' =&gt; [ 'App' =&gt; Illuminate\\Support\\Facades\\App::class, 'Artisan' =&gt; Illuminate\\Support\\Facades\\Artisan::class, 'Auth' =&gt; Illuminate\\Support\\Facades\\Auth::class, 'Blade' =&gt; Illuminate\\Support\\Facades\\Blade::class, 'Broadcast' =&gt; Illuminate\\Support\\Facades\\Broadcast::class, 'Bus' =&gt; Illuminate\\Support\\Facades\\Bus::class, 'Cache' =&gt; Illuminate\\Support\\Facades\\Cache::class, 'Config' =&gt; Illuminate\\Support\\Facades\\Config::class, ...... ] 进入AliasLoader 看一下是如何进行注册这些Facades的 // class Illuminate\\Foundation\\AliasLoader //获取应用实例 public static function getInstance(array $aliases = []) { if (is_null(static::$instance)) { return static::$instance = new static($aliases); } $aliases = array_merge(static::$instance-&gt;getAliases(), $aliases); static::$instance-&gt;setAliases($aliases); return static::$instance; } public function register() { if (! $this-&gt;registered) { $this-&gt;prependToLoaderStack(); $this-&gt;registered = true; } } protected function prependToLoaderStack() { // 把AliasLoader::load()放入自动加载函数队列中，并置于队列头部 spl_autoload_register([$this, 'load'], true, true); } spl_autoload_register 函数是实现自动加载未定义类功能的的重要方法 这个函数的参数如下 autoload_function 这是一个函数【方法】名称，可以是字符串或者数组（调用类方法使用）。这个函数（方法）的功能就是，来把需要new 的类文件包含include(requeire)进来，这样new的时候就不会找不到文件了。其实就是封装整个项目的include和require功能。 throw 此参数设置了 autoload_function 无法成功注册时， spl_autoload_register()是否抛出异常。 prepend 如果是 true，spl_autoload_register() 会添加函数到队列之首，而不是队列尾部。 上面的代码就是 AliasLoader 将load方法注册到SPL __autoload函数队列的头部。 这样的话当我们需要一个服务类的时候会调用load方法引入进来 看一下load方法的源码 public function load($alias) { if (static::$facadeNamespace &amp;&amp; strpos($alias, static::$facadeNamespace) === 0) { $this-&gt;loadFacade($alias); return true; } if (isset($this-&gt;aliases[$alias])) { return class_alias($this-&gt;aliases[$alias], $alias); } } load方法里把$aliases里的配置Facade类创建了对应的别名，比如说我们使用Auth类的时候 laravel内部会通过AliasLoader 的load方法为 Illuminate\\Support\\Facades\\Auth 这个类创建一个别名类Auth。 所以我们在应用程序中使用的Auth类其实就是使用的 Illuminate\\Support\\Facades\\Auth 这个类 解析Facade代理服务 之前所提到的 我们已经成功将Facade服务注册了 那么我们再使用一些Facades 比如Route::get('/',function(){})这样的方法时 那么此时如何通过Route这个静态代理解析到里面的服务呢。这个就是下面要说的laravel里的隐式解析。 我们说了Route其实是对应的Illuminate\\Support\\Facades\\Route这个类 可以看下里面都有什么 namespace Illuminate\\Support\\Facades; class Route extends Facade { protected static function getFacadeAccessor() { return 'router'; } } 我们主要看下getFacadeAccessor这个方法 你会发现无论是Route还是Auth等这些Facade所对应的类都会有这个方法 这里面并没有我们所预期的get、post、patch这些方法 这个我们就去他的父类 Illuminate\\Support\\Facades\\Facade看看 依然没有 我们知道PHP如果执行没有给定的方法 回去执行他的魔术方法 也就是 __callStatic静态方法 namespace Illuminate\\Support\\Facades; abstract class Facade { public static function __callStatic($method, $args) { $instance = static::getFacadeRoot(); if (! $instance) { throw new RuntimeException('A facade root has not been set.'); } return $instance-&gt;$method(...$args); } //获取Facade根对象 public static function getFacadeRoot() { return static::resolveFacadeInstance(static::getFacadeAccessor()); } /** * 从服务容器里解析出Facade对应的服务 */ protected static function resolveFacadeInstance($name) { if (is_object($name)) { return $name; } if (isset(static::$resolvedInstance[$name])) { return static::$resolvedInstance[$name]; } return static::$resolvedInstance[$name] = static::$app[$name]; } } 我们可以发现 Illuminate\\Support\\Facades\\Facade 这个父类是一个抽象类 这样的话我们可以根据自己的需要去增加新的子系统外观类 并让外观类可以正确代理其对应的子系统。 比如这里的Route那么最终的instance就是 调用的 resolveFacadeInstance('router') 也就是子类Route Facade里设置的accessor(字符串router) 然后从服务容器中解析到对应的服务。 而router服务是在应用程序初始化时的registerBaseServiceProviders阶段被\\Illuminate\\Routing\\RoutingServiceProvider注册到服务容器里的: 具体可以参考laravel 路由的源码分析。 class RoutingServiceProvider extends ServiceProvider { /** * Register the service provider. * * @return void */ public function register() { $this-&gt;registerRouter(); ...... } /** * Register the router instance. * * @return void */ protected function registerRouter() { $this-&gt;app-&gt;singleton('router', function ($app) { return new Router($app['events'], $app); }); } ...... } router服务对应的类就是\\Illuminate\\Routing\\Router, 所以Route Facade实际上代理的就是这个类 所以通过Route Facade访问的get、post方法都是访问的\\Illuminate\\Routing\\Router这个类的方法 值的注意的是 1.解析服务时用的static::$app是在最开始的RegisterFacades里设置的，它引用的是服务容器。 2.static::$app['router'] 以数组访问的形式能够从服务容器解析出router服务是因为服务容器实现了SPL的ArrayAccess接口, 对这个没有概念的可以看下官方文档ArrayAccess 对于ArrayAccess 需要实现其四个方法 offsetExists、offsetGet、offsetSet、offsetUnset 通过重写这四个方法 那么我们就可以对服务对象进行存取 和操作数组一样 只不过我们操作的事的各种子服务而已 ","link":"https://GeekGhc.github.io/post/yuan-ma-fen-xi-zhi-facades/"},{"title":"源码分析之-Laravel路由","content":"首先这里以laravel5.5版本为例 初始化新的项目 路由是外界访问laravel应用程序的通道 通过指定URI和HTTP请求方法 那么就可以访问项目应用程序的处理方法或者闭包。现在我们可以去研究下在Laravel中是如何处理这些请求 并重新解析到对应的方法体。 以一个我们通常的访问形式说起 Route::get('/user', 'UsersController@index'); 通过这个路由 客户端通过get的http请求方法 请求'/user'这样的URI时 laravel会将请求重定向到User控制器的index方法 最后由该方法返回结果给客户端 首先我们分析下Route这个类 是通过laravel的门面来实现 通过一种简单的方式来绑定访问到容器里的服务router。 其实这里就可以理解为通过Route::get就可以访问到router这个服务的方法 所以说上面的路由等价于: app()-&gt;make('router')-&gt;get('user','UserController@index'); 其中router这个服务是在实例化应用程序时在构造方法里通过注册RoutingServiceProvider绑定到服务容器的 服务的初始化在bootstrap/app/php中时 初始化了一个Application类 $app = new Illuminate\\Foundation\\Application( realpath(__DIR__.'/../') ); 在这个Application的初始化函数中我们可以看到服务的注册 public function __construct($basePath = null) { if ($basePath) { $this-&gt;setBasePath($basePath); } $this-&gt;registerBaseBindings(); $this-&gt;registerBaseServiceProviders(); $this-&gt;registerCoreContainerAliases(); } 在注册基础服务提供者时也就是registerBaseServiceProviders我们看到这个方法里注册一些相关服务 //注册基础的服务提供器 protected function registerBaseServiceProviders() { $this-&gt;register(new EventServiceProvider($this)); $this-&gt;register(new LogServiceProvider($this)); $this-&gt;register(new RoutingServiceProvider($this)); } 其中我们就可以看到注册了RoutingServiceProvider 那么在这个类中通过registerRouter绑定到服务容器的 protected function registerRouter() { $this-&gt;app-&gt;singleton('router', function ($app) { return new Router($app['events'], $app); }); } 所以说Route的最终的方法斗都是现在\\Illuminate\\Routing\\Router这个类里 在这里类里可以看到一些路由的注册、寻址、调度的方法。 因为路由整个实现的过程无非就是围绕着注册、寻址、调度这样的流程 所以现在可以根据这个过程看下其中的具体实现 路由加载 注册路由前需要先加载路由文件 而这里的文件加载是在 App\\Providers\\RouteServiceProvider 这个服务提供者的boot方法去加载的 class RouteServiceProvider extends ServiceProvider { public function boot() { parent::boot(); } public function map() { $this-&gt;mapApiRoutes(); $this-&gt;mapWebRoutes(); } protected function mapWebRoutes() { Route::middleware('web') -&gt;namespace($this-&gt;namespace) -&gt;group(base_path('routes/web.php')); } protected function mapApiRoutes() { Route::prefix('api') -&gt;middleware('api') -&gt;namespace($this-&gt;namespace) -&gt;group(base_path('routes/api.php')); } } namespace Illuminate\\Foundation\\Support\\Providers; class RouteServiceProvider extends ServiceProvider { public function boot() { $this-&gt;setRootControllerNamespace(); if ($this-&gt;app-&gt;routesAreCached()) { $this-&gt;loadCachedRoutes(); } else { $this-&gt;loadRoutes(); $this-&gt;app-&gt;booted(function () { $this-&gt;app['router']-&gt;getRoutes()-&gt;refreshNameLookups(); $this-&gt;app['router']-&gt;getRoutes()-&gt;refreshActionLookups(); }); } } protected function loadCachedRoutes() { $this-&gt;app-&gt;booted(function () { require $this-&gt;app-&gt;getCachedRoutesPath(); }); } protected function loadRoutes() { if (method_exists($this, 'map')) { $this-&gt;app-&gt;call([$this, 'map']); } } } class Application extends Container implements ApplicationContract, HttpKernelInterface { public function routesAreCached() { return $this['files']-&gt;exists($this-&gt;getCachedRoutesPath()); } public function getCachedRoutesPath() { return $this-&gt;bootstrapPath().'/cache/routes.php'; } } 和很多框架的加载方式一样 laravel先去寻找路由的缓存文件，没有缓存文件再去加载路由。其中缓存文件一般存在bootstrap/cache/routes.php 另外我们知道artisan有个commond就是php artisan route:cache和php artisan route:clear就是针对路由缓存文件的 如果路由是闭包方法是不能进行路由缓存的 可以改为控制路由和资源路由 可以看到boot方法里 通过loadRoutes会通过魔术方法调用map方法来加载文件里的路由，map方法在App\\Providers\\RouteServiceProvider类中定义的 而这个类就是继承自Illuminate\\Foundation\\Support\\Providers\\RouteServiceProvider 通过map方法laravel将路由分成两组 分别对应web服务的路由 和做后端api服务的路由 这两个路由文件的位置就是在项目的routes目录下的web.php api.php 在5.5版本之前 可以查看5.2版本 可以看到路由文件其实是存放在app/Http/routes.php 这样的改动无非更方便了我们去管理我们的路由文件 既然加载了路由文件 那儿么接下来就是开始了路由的注册 路由注册 我们通常且统一的方式是通过Route这个facade调用其中的静态方法去实现各种http请求方法的处理 当然也可以使用上面提到过的那种方法 那么这样的调用实际调用的就是 Illuminate\\Routing\\Router 里的方法 因为Route这个facade这个是绑定到这个类的 之前也讲过 我们举例几个实现方法来说就是 public function get($uri, $action = null) { return $this-&gt;addRoute(['GET', 'HEAD'], $uri, $action); } public function post($uri, $action = null) { return $this-&gt;addRoute('POST', $uri, $action); } public function put($uri, $action = null) { return $this-&gt;addRoute('PUT', $uri, $action); } 可以看到的是路由的注册都是通过addRoute这个方法去进行注册的 protected function addRoute($methods, $uri, $action) { return $this-&gt;routes-&gt;add($this-&gt;createRoute($methods, $uri, $action)); } 而这个addRoute方法则是会将路由注册到RouteCollection 当然之前会调用路由的创建方法createRoute protected function createRoute($methods, $uri, $action) { if ($this-&gt;actionReferencesController($action)) { $action = $this-&gt;convertToControllerAction($action); } $route = $this-&gt;newRoute( $methods, $this-&gt;prefix($uri), $action ); if ($this-&gt;hasGroupStack()) { $this-&gt;mergeGroupAttributesIntoRoute($route); } $this-&gt;addWhereClausesToRoute($route); return $route; } 而这个创建路由的方法接收三个参数 分别是路由的请求方法 uri以及执行体 这执行可以使未解析的资源路径 也可以是一个闭包方法 其中我们最常见的解析到对应的控制器的方法是由($this-&gt;actionReferencesController($action)进行解析转换的 这里会判断不是一个闭包(Closure) 也就是如果是string类型的话 值得注意的是 这里的action 我们在路由注册时会以数组 字符串以及闭包的形式传递 那么如果是数组也就是比如['uses' =&gt; 'HomeController@index, 'middleware' =&gt; 'auth']这样的形式的 以及字符串类型的也就是比如HomController@index这种形式的都会通过convertToControllerAction这个方法进行解析成action数组 最后的保存形式就是 [ 'uses' =&gt; 'App\\Http\\Controllers\\HomeController@index', 'controller' =&gt; 'App\\Http\\Controllers\\HomeController@index' ] 经过转换解析之后 会补充控制器的完整的命名空间 构建完action数组之后那么之后就是创建路由了 创建路由会由之前的指定的http方法uri字符串以及转换后的action数组作为参数进行创建 \\Illuminate\\Routing\\Route类的实例: $route = $this-&gt;newRoute( $methods, $this-&gt;prefix($uri), $action ); protected function newRoute($methods, $uri, $action) { return (new Route($methods, $uri, $action)) -&gt;setRouter($this) -&gt;setContainer($this-&gt;container); } 路由创建完成后再添加到之前所说的RouteCollection中去: protected function addRoute($methods, $uri, $action) { return $this-&gt;routes-&gt;add($this-&gt;createRoute($methods, $uri, $action)); } 这里的$this-&gt;routes就是类在初始化时的RouteCollection对象 public function __construct(Dispatcher $events, Container $container = null) { $this-&gt;events = $events; $this-&gt;routes = new RouteCollection; $this-&gt;container = $container ?: new Container; } 而新添加的路由 也就是一个Route对象会更新RouteCollection中的routes、allRoutes、nameList和actionList属性 class RouteCollection implements Countable, IteratorAggregate { public function add(Route $route) { $this-&gt;addToCollections($route); $this-&gt;addLookups($route); return $route; } protected function addToCollections($route) { $domainAndUri = $route-&gt;getDomain().$route-&gt;uri(); foreach ($route-&gt;methods() as $method) { $this-&gt;routes[$method][$domainAndUri] = $route; } $this-&gt;allRoutes[$method.$domainAndUri] = $route; } protected function addLookups($route) { $action = $route-&gt;getAction(); if (isset($action['as'])) { //如果时命名路由，将route对象映射到以路由名为key的数组值中方便查找 $this-&gt;nameList[$action['as']] = $route; } if (isset($action['controller'])) { $this-&gt;addToActionList($action, $route); } } protected function addToActionList($action, $route) { $this-&gt;actionList[trim($action['controller'], '\\\\')] = $route; } } 既然更新了RouteCollection这四个属性 下面就看下这个属性的作用 routes中存放了HTTP请求方法与路由对象的映射,就像这样: [ 'GET' =&gt; [ $routeUri1 =&gt; $routeObj1 ... ] ... ] allRoutes属性里存放的内容时将routes属性里的二维数组变成一维数组后的内容: [ 'GET' . $routeUri1 =&gt; $routeObj1 'GET' . $routeUri2 =&gt; $routeObj2 ... ] nameList是路由名称与路由对象的一个映射表 [ $routeName1 =&gt; $routeObj1 ... ] actionList是路由控制器方法字符串与路由对象的映射表 [ 'App\\Http\\Controllers\\ControllerOne@ActionOne' =&gt; $routeObj1 ] 这样一来就可以完成了一个路由的注册 路由寻址 首先我们可以预先了解一个概念就是 我们知道laravel在请求路由到最终的返回之间有一层中间件 HTTP请求是在经过Pipeline通道上的中间件的前置操作后到达目的地: //Illuminate\\Foundation\\Http\\Kernel class Kernel implements KernelContract { protected function sendRequestThroughRouter($request) { $this-&gt;app-&gt;instance('request', $request); Facade::clearResolvedInstance('request'); $this-&gt;bootstrap(); return (new Pipeline($this-&gt;app)) -&gt;send($request) -&gt;through($this-&gt;app-&gt;shouldSkipMiddleware() ? [] : $this-&gt;middleware) -&gt;then($this-&gt;dispatchToRouter()); } protected function dispatchToRouter() { return function ($request) { $this-&gt;app-&gt;instance('request', $request); return $this-&gt;router-&gt;dispatch($request); }; } } 上面的代码可以看出Pipeline的destination就是dispatchToRouter函数返回的闭包: 也就是最终的目的地址是这样的一个闭包: $destination = function ($request) { $this-&gt;app-&gt;instance('request', $request); return $this-&gt;router-&gt;dispatch($request); }; 在闭包里调用了router的dispatch方法，路由寻址就发生在dispatch的一开始的findRoute里： class Router implements RegistrarContract, BindingRegistrar { public function dispatch(Request $request) { $this-&gt;currentRequest = $request; return $this-&gt;dispatchToRoute($request); } public function dispatchToRoute(Request $request) { return $this-&gt;runRoute($request, $this-&gt;findRoute($request)); } protected function findRoute($request) { $this-&gt;current = $route = $this-&gt;routes-&gt;match($request); $this-&gt;container-&gt;instance(Route::class, $route); return $route; } } 寻找路由的任务由 RouteCollection 负责，这个函数负责匹配路由，并且把 request 的 url 参数绑定到路由中： class RouteCollection implements Countable, IteratorAggregate { public function match(Request $request) { $routes = $this-&gt;get($request-&gt;getMethod()); $route = $this-&gt;matchAgainstRoutes($routes, $request); if (! is_null($route)) { //找到匹配的路由后，将URI里的路径参数绑定赋值给路由(如果有的话) return $route-&gt;bind($request); } $others = $this-&gt;checkForAlternateVerbs($request); if (count($others) &gt; 0) { return $this-&gt;getRouteForMethods($request, $others); } throw new NotFoundHttpException; } protected function matchAgainstRoutes(array $routes, $request, $includingMethod = true) { return Arr::first($routes, function ($value) use ($request, $includingMethod) { return $value-&gt;matches($request, $includingMethod); }); } } class Route { public function matches(Request $request, $includingMethod = true) { $this-&gt;compileRoute(); foreach ($this-&gt;getValidators() as $validator) { if (! $includingMethod &amp;&amp; $validator instanceof MethodValidator) { continue; } if (! $validator-&gt;matches($this, $request)) { return false; } } return true; } } $routes = $this-&gt;get($request-&gt;getMethod());会先加载注册路由阶段在RouteCollection里生成的routes属性里的值，routes中存放了HTTP请求方法与路由对象的映射。 然后依次调用这堆路由里路由对象的matches方法， matches方法, matches方法里会对HTTP请求对象进行一些验证，验证对应的Validator是：UriValidator、MethodValidator、SchemeValidator、HostValidator。 在验证之前在$this-&gt;compileRoute()里会将路由的规则转换成正则表达式。 UriValidator主要是看请求对象的URI是否与路由的正则规则匹配能匹配上: class UriValidator implements ValidatorInterface { public function matches(Route $route, Request $request) { $path = $request-&gt;path() == '/' ? '/' : '/'.$request-&gt;path(); return preg_match($route-&gt;getCompiled()-&gt;getRegex(), rawurldecode($path)); } } MethodValidator验证请求方法, SchemeValidator验证协议是否正确(http|https), HostValidator验证域名, 如果路由中不设置host属性，那么这个验证不会进行 一旦某个路由通过了全部的认证就将会被返回，接下来就要将请求对象URI里的路径参数绑定复制给路由参数: 路由参数绑定 class Route { public function bind(Request $request) { $this-&gt;compileRoute(); $this-&gt;parameters = (new RouteParameterBinder($this)) -&gt;parameters($request); return $this; } } class RouteParameterBinder { public function parameters($request) { $parameters = $this-&gt;bindPathParameters($request); if (! is_null($this-&gt;route-&gt;compiled-&gt;getHostRegex())) { $parameters = $this-&gt;bindHostParameters( $request, $parameters ); } return $this-&gt;replaceDefaults($parameters); } protected function bindPathParameters($request) { preg_match($this-&gt;route-&gt;compiled-&gt;getRegex(), '/'.$request-&gt;decodedPath(), $matches); return $this-&gt;matchToKeys(array_slice($matches, 1)); } protected function matchToKeys(array $matches) { if (empty($parameterNames = $this-&gt;route-&gt;parameterNames())) { return []; } $parameters = array_intersect_key($matches, array_flip($parameterNames)); return array_filter($parameters, function ($value) { return is_string($value) &amp;&amp; strlen($value) &gt; 0; }); } } 赋值路由参数完成后路由寻址的过程就结束了，结下来就该运行通过匹配路由中对应的控制器方法返回响应对象了 namespace Illuminate\\Routing; class Router implements RegistrarContract, BindingRegistrar { public function dispatch(Request $request) { $this-&gt;currentRequest = $request; return $this-&gt;dispatchToRoute($request); } public function dispatchToRoute(Request $request) { return $this-&gt;runRoute($request, $this-&gt;findRoute($request)); } protected function runRoute(Request $request, Route $route) { $request-&gt;setRouteResolver(function () use ($route) { return $route; }); $this-&gt;events-&gt;dispatch(new Events\\RouteMatched($route, $request)); return $this-&gt;prepareResponse($request, $this-&gt;runRouteWithinStack($route, $request) ); } protected function runRouteWithinStack(Route $route, Request $request) { $shouldSkipMiddleware = $this-&gt;container-&gt;bound('middleware.disable') &amp;&amp; $this-&gt;container-&gt;make('middleware.disable') === true; //收集路由和控制器里应用的中间件 $middleware = $shouldSkipMiddleware ? [] : $this-&gt;gatherRouteMiddleware($route); return (new Pipeline($this-&gt;container)) -&gt;send($request) -&gt;through($middleware) -&gt;then(function ($request) use ($route) { return $this-&gt;prepareResponse( $request, $route-&gt;run() ); }); } } namespace Illuminate\\Routing; class Route { public function run() { $this-&gt;container = $this-&gt;container ?: new Container; try { if ($this-&gt;isControllerAction()) { return $this-&gt;runController(); } return $this-&gt;runCallable(); } catch (HttpResponseException $e) { return $e-&gt;getResponse(); } } } 最终会执行路由的run方法 当然也会判断是一个闭包还是一个控制器方法进行调用 最后将结果封装成Response 返回给客户端 当然这里只是简单的介绍了路由所经过的中间层 也就是中间件的执行逻辑 这个需要再去详细研究。 ","link":"https://GeekGhc.github.io/post/yuan-ma-fen-xi-zhi-laravel-lu-you/"},{"title":"MySQL · 最佳实践 · 分区表基本类型(转载)","content":"MySQL分区表概述 随着MySQL越来越流行，Mysql里面的保存的数据也越来越大。 在日常的工作中，我们经常遇到一张表里面保存了上亿甚至过十亿的记录。 这些表里面保存了大量的历史记录。 对于这些历史数据的清理是一个非常头疼事情，由于所有的数据都一个普通的表里。 所以只能是启用一个或多个带where条件的delete语句去删除（一般where条件是时间）。 这对数据库的造成了很大压力。即使我们把这些删除了，但底层的数据文件并没有变小。 面对这类问题，最有效的方法就是在使用分区表。最常见的分区方法就是按照时间进行分区。 分区一个最大的优点就是可以非常高效的进行历史数据的清理。 分区类型 目前MySQL支持范围分区（RANGE），列表分区（LIST），哈希分区（HASH）以及KEY分区四种。下面我们逐一介绍每种分区： RANGE分区 基于属于一个给定连续区间的列值，把多行分配给分区。最常见的是基于时间字段. 基于分区的列最好是整型，如果日期型的可以使用函数转换为整型。本例中使用to_days函数 CREATE TABLE my_range_datetime( id INT, hiredate DATETIME ) PARTITION BY RANGE (TO_DAYS(hiredate) ) ( PARTITION p1 VALUES LESS THAN ( TO_DAYS('20171202') ), PARTITION p2 VALUES LESS THAN ( TO_DAYS('20171203') ), PARTITION p3 VALUES LESS THAN ( TO_DAYS('20171204') ), PARTITION p4 VALUES LESS THAN ( TO_DAYS('20171205') ), PARTITION p5 VALUES LESS THAN ( TO_DAYS('20171206') ), PARTITION p6 VALUES LESS THAN ( TO_DAYS('20171207') ), PARTITION p7 VALUES LESS THAN ( TO_DAYS('20171208') ), PARTITION p8 VALUES LESS THAN ( TO_DAYS('20171209') ), PARTITION p9 VALUES LESS THAN ( TO_DAYS('20171210') ), PARTITION p10 VALUES LESS THAN ( TO_DAYS('20171211') )， PARTITION p11 VALUES LESS THAN (MAXVALUE) ); p11是一个默认分区，所有大于20171211的记录都会在这个分区。MAXVALUE是一个无穷大的值。p11是一个可选分区。如果在定义表的没有指定的这个分区，当我们插入大于20171211的数据的时候，会收到一个错误。 我们在执行查询的时候，必须带上分区字段。这样可以使用分区剪裁功能 mysql&gt; insert into my_range_datetime select * from test; Query OK, 1000000 rows affected (8.15 sec) Records: 1000000 Duplicates: 0 Warnings: 0 mysql&gt; explain partitions select * from my_range_datetime where hiredate &gt;= '20171207124503' and hiredate&lt;='20171210111230'; +----+-------------+-------------------+--------------+------+---------------+------+---------+------+--------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------------------+--------------+------+---------------+------+---------+------+--------+-------------+ | 1 | SIMPLE | my_range_datetime | p7,p8,p9,p10 | ALL | NULL | NULL | NULL | NULL | 400061 | Using where | +----+-------------+-------------------+--------------+------+---------------+------+---------+------+--------+-------------+ 1 row in set (0.03 sec) 注意执行计划中的partitions的内容，只查询了p7，p8，p9，p10三个分区，由此来看，使用to_days函数确实可以实现分区裁剪。 上面是基于datetime的，如果是timestamp类型，我们遇到上面问题呢？ 事实上，MySQL提供了一种基于UNIX_TIMESTAMP函数的RANGE分区方案，而且，只能使用UNIX_TIMESTAMP函数，如果使用其它函数，譬如to_days， 会报如下错误：“ERROR 1486 (HY000): Constant, random or timezone-dependent expressions in (sub)partitioning function are not allowed”。 而且官方文档中也提到“Any other expressions involving TIMESTAMP values are not permitted. (See Bug #42849.)”。 下面来测试一下基于UNIX_TIMESTAMP函数的RANGE分区方案，看其能否实现分区裁剪。 针对TIMESTAMP的分区方案 创表语句如下： CREATE TABLE my_range_timestamp ( id INT, hiredate TIMESTAMP ) PARTITION BY RANGE ( UNIX_TIMESTAMP(hiredate) ) ( PARTITION p1 VALUES LESS THAN ( UNIX_TIMESTAMP('2017-12-02 00:00:00') ), PARTITION p2 VALUES LESS THAN ( UNIX_TIMESTAMP('2017-12-03 00:00:00') ), PARTITION p3 VALUES LESS THAN ( UNIX_TIMESTAMP('2017-12-04 00:00:00') ), PARTITION p4 VALUES LESS THAN ( UNIX_TIMESTAMP('2017-12-05 00:00:00') ), PARTITION p5 VALUES LESS THAN ( UNIX_TIMESTAMP('2017-12-06 00:00:00') ), PARTITION p6 VALUES LESS THAN ( UNIX_TIMESTAMP('2017-12-07 00:00:00') ), PARTITION p7 VALUES LESS THAN ( UNIX_TIMESTAMP('2017-12-08 00:00:00') ), PARTITION p8 VALUES LESS THAN ( UNIX_TIMESTAMP('2017-12-09 00:00:00') ), PARTITION p9 VALUES LESS THAN ( UNIX_TIMESTAMP('2017-12-10 00:00:00') ), PARTITION p10 VALUES LESS THAN (UNIX_TIMESTAMP('2017-12-11 00:00:00') ) ); 插入数据并查看上述查询的执行计划 mysql&gt; insert into my_range_timestamp select * from test; Query OK, 1000000 rows affected (13.25 sec) Records: 1000000 Duplicates: 0 Warnings: 0 mysql&gt; explain partitions select * from my_range_timestamp where hiredate &gt;= '20171207124503' and hiredate&lt;='20171210111230'; +----+-------------+-------------------+--------------+------+---------------+------+---------+------+--------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------------------+--------------+------+---------------+------+---------+------+--------+-------------+ | 1 | SIMPLE | my_range_timestamp | p7,p8,p9,p10 | ALL | NULL | NULL | NULL | NULL | 400448 | Using where | +----+-------------+-------------------+--------------+------+---------------+------+---------+------+--------+-------------+ 1 row in set (0.00 sec) 同样也能实现分区裁剪。 在5.7版本之前，对于DATA和DATETIME类型的列，如果要实现分区裁剪，只能使用YEAR() 和TO_DAYS()函数，在5.7版本中，又新增了TO_SECONDS()函数。 LIST 分区 LIST分区和RANGE分区类似，区别在于LIST是枚举值列表的集合，RANGE是连续的区间值的集合。二者在语法方面非常的相似。同样建议LIST分区列是非null列， 否则插入null值如果枚举列表里面不存在null值会插入失败，这点和其它的分区不一样，RANGE分区会将其作为最小分区值存储，HASH\\KEY分为会将其转换成0存储， 主要LIST分区只支持整形，非整形字段需要通过函数转换成整形. create table t_list( a int(11), b int(11) )(partition by list (b) partition p0 values in (1,3,5,7,9), partition p1 values in (2,4,6,8,0) ); Hash 分区 我们在实际工作中经常遇到像会员表的这种表。并没有明显可以分区的特征字段。但表数据有非常庞大。 为了把这类的数据进行分区打散mysql 提供了hash分区。基于给定的分区个数，将数据分配到不同的分区， HASH分区只能针对整数进行HASH，对于非整形的字段只能通过表达式将其转换成整数。表达式可以是mysql中任意有效的函数或者表达式， 对于非整形的HASH往表插入数据的过程中会多一步表达式的计算操作，所以不建议使用复杂的表达式这样会影响性能。 Hash分区表的基本语句如下： CREATE TABLE my_member ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), created DATE NOT NULL DEFAULT '1970-01-01', separated DATE NOT NULL DEFAULT '9999-12-31', job_code INT, store_id INT ) PARTITION BY HASH(id) PARTITIONS 4; 注意： 1.HASH分区可以不用指定PARTITIONS子句，如上文中的PARTITIONS 4，则默认分区数为1。 2.不允许只写PARTITIONS，而不指定分区数。 3.同RANGE分区和LIST分区一样，PARTITION BY HASH (expr)子句中的expr返回的必须是整数值。 4.HASH分区的底层实现其实是基于MOD函数。譬如，对于下表 CREATE TABLE t1 (col1 INT, col2 CHAR(5), col3 DATE) PARTITION BY HASH( YEAR(col3) ) PARTITIONS 4; 如果你要插入一个col3为“2017-09-15”的记录，则分区的选择是根据以下值决定的： MOD(YEAR(‘2017-09-01’),4) = MOD(2017,4) = 1 LINEAR HASH分区 LINEAR HASH分区是HASH分区的一种特殊类型，与HASH分区是基于MOD函数不同的是，它基于的是另外一种算法。 格式如下： CREATE TABLE my_members ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT '1970-01-01', separated DATE NOT NULL DEFAULT '9999-12-31', job_code INT, store_id INT ) PARTITION BY LINEAR HASH( id ) PARTITIONS 4; 说明： 它的优点是在数据量大的场景，譬如TB级，增加、删除、合并和拆分分区会更快，缺点是，相对于HASH分区，它数据分布不均匀的概率更大。 KEY分区 KEY分区其实跟HASH分区差不多，不同点如下： 1.KEY分区允许多列，而HASH分区只允许一列。 2.如果在有主键或者唯一键的情况下，key中分区列可不指定，默认为主键或者唯一键，如果没有，则必须显性指定列。 3.KEY分区对象必须为列，而不能是基于列的表达式。 4.KEY分区和HASH分区的算法不一样，PARTITION BY HASH (expr)，MOD取值的对象是expr返回的值，而PARTITION BY KEY (column_list)，基于的是列的MD5值。 格式如下： CREATE TABLE k1 ( id INT NOT NULL PRIMARY KEY, name VARCHAR(20) ) PARTITION BY KEY() PARTITIONS 2; 在没有主键或者唯一键的情况下，格式如下： CREATE TABLE tm1 ( s1 CHAR(32) ) PARTITION BY KEY(s1) PARTITIONS 10; 总结： 1.MySQL分区中如果存在主键或唯一键，则分区列必须包含在其中。 2.对于原生的RANGE分区，LIST分区，HASH分区，分区对象返回的只能是整数值。 3.分区字段不能为NULL，要不然怎么确定分区范围呢，所以尽量NOT NULL 相关链接 数据库内核月报 － 2017 / 11 ","link":"https://GeekGhc.github.io/post/mysql-zui-jia-shi-jian-fen-qu-biao-ji-ben-lei-xing-zhuan-zai/"},{"title":"聊聊PHP中实现的依赖注入(Di)实现","content":"在PHP的开发中 利用IOC容器可以很方便的储存以及获取资源 这样就可以实现解耦, 使用依赖注入的好处就是有效的分离了对象和他所需的外部资源，使得他们松散耦合,这样就有利于功能的复用。这样一来程序代码的整体结构也会变的整节可读。即随取随用。 在开发的过程中,如果我们需要一个对象就需要不断的new一个新的对象,可能会在很多处地方用到这个对象的功能,这样的话如果一些的不好的编程习惯会造成对象的无法回收,这会为 整个项目的代码和维护造成不小的困扰,所以依照我们提倡的松耦合、少入侵的原则,可以采用ICO容器也就是对这些对象的集中管理。 这里我们可以看看EasySwoole里面是怎么实现这样的容器管理的 代码入手 首先为了实现容器的统一管理,定义了一个单例trait这趟就很好的对Di这个容器进行单例化。 trait Singleton { private static $instance; static function getInstance(...$args) { if(!isset(self::$instance)){ self::$instance = new static(...$args); } return self::$instance; } } 实现代码很简单 就是首先定义一个私有事例 在暴露一个static获取事例的function 如果已实例化则直接返回 如果没有则在初始化的时候new一个对象返回 接下来就是Di这个容器的编写 use这个单例trait时它成为单例类 然后可以想象一下容器的设置/获取这样的一些操作 那么很自然的想到我们可以这么写 class Di { use Singleton; private $container = array(); public function set($key, $obj,...$arg):void { $this-&gt;container[$key] = array( &quot;obj&quot;=&gt;$obj, &quot;params&quot;=&gt;$arg, ); } function delete($key):void { unset( $this-&gt;container[$key]); } function clear():void { $this-&gt;container = array(); } /** * @param $key * @return null * @throws \\Throwable */ function get($key) { if(isset($this-&gt;container[$key])){ $obj = $this-&gt;container[$key]['obj']; $params = $this-&gt;container[$key]['params']; if(is_object($obj) || is_callable($obj)){ return $obj; }else if(is_string($obj) &amp;&amp; class_exists($obj)){ try{ $this-&gt;container[$key]['obj'] = new $obj(...$params); return $this-&gt;container[$key]['obj']; }catch (\\Throwable $throwable){ throw $throwable; } }else{ return $obj; } }else{ return null; } } } 这里定义了四个function 分别是对象的设置、获取、删除和清空。首先可以看下set也就是注入函数的原型,它接收三个参数，第一个就是key名 也就是我们再项目其他地方需要用到时可以通过key来获取这个object。在set这个function中第二个参数$obj即为需要设置储存的object。第三个参数就是一系列参数，这个参数就是object需要初始化的参数。 所以在设置时同意放入私有的container变量中 因为这个是单例类 我们不需要暴露这个container变量 我们可以通过get获取这个obj 在get获取注入的资源时 首先进行判空 如果存在那么接下来就是对传之前传入对象的形式进行初始化new一个对象返回。也就是说如果是一个可调用的对象的话比如 new User() 那么直接返回这个obj。如果是一个string类型，并且存在这个class的话 很简单实例化这个类，并通过set时候传入放入初始化参数进行初始化对象并返回。另外其他的删除以及清空container的都比较简单。 在项目使用时如果需要获取这个对象那么可以直接通过get方法获取并调用 比如 $di = Di::getInstance(); $obj = $di-&gt;get('database'); 那么就可以使用key为database的这个object。因为Di是单利类 所以对象的存取就会很好实现。 这里只是一个比较简单的IOC的实现 我们还可以根据服务的一些特性去实现其他的功能 具体的可以参考Laravel里的IOC的实现 有兴趣的可以了解一下。 ","link":"https://GeekGhc.github.io/post/liao-liao-php-zhong-shi-xian-de-yi-lai-zhu-ru-dishi-xian/"},{"title":"Kafka的实践和使用","content":"kafka作为一个分布式的流处理平台 在实际的项目中有这广泛的应用 而其可以用于两大类别的应用分类 构造实时流数据管道，它可以在系统或应用之间可靠地获取数据。 (相当于message queue) 构建实时流式应用程序，对这些流数据进行转换或者影响。 (就是流处理，通过kafka stream topic和topic之间内部进行变化) 这里我们以php作为后端语言为例 引入kafka来解决一些实际问题 PHP引入kafka 本地环境为mac环境 首先通过brew安装工具 $ brew install kafka 安装的同时会自动安装依赖zookeeper 而在安装kafka的扩展之前，在安装php-rdkafka之前，需要先安装librdkafka $ git clone https://github.com/edenhill/librdkafka.git $ cd librdkafka $ ./configure $ make &amp;&amp; make install 开始安装kafka扩展 $ git clone https://github.com/arnaud-lb/php-rdkafka.git $ cd php-rdkafka $ phpize $ ./configure --with-php-config=/usr/local/php/bin/php-config ###你安装的php下的php-config路径 $ make &amp;&amp; make install 编译完成之后再php.ini中加入扩展 extension=rdkafaka.so kafka启动执行 进入mac环境下的kafka编译后的目录 可以看到kafka和zookeeper的配置文件 server.proprties和zookeeper.properties 启动zookeeper $ zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties 启动服务后 端口是绑定到了2181端口 启动kafaka 进入服务的文件目录 可以看到关于kafka的一些命令列表 其中包含了镜像管理和日志管理 $ kafka-server-start /usr/local/etc/kafka/server.properties 其实有关kafka的基本概念可以查看其官方文档的一些说明 在这里为了测试可以先创建一个topic 当然可以先看一下所有的topics $ kafka-topics --list --zookeeper localhost:2181 让我们创建一个名为“test”的topic，它有3个分区和1个副本 $ kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic test 再次查看所有topics信息 查看刚创建的话题信息 启动生产者 $ kafka-console-producer --broker-list localhost:9092 --topic test 这里的9092是kafaka的端口地址 并非zookeeper端口地址 启动消费者 $ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning 这里的from-beginning 表示信息每次都会从头开始读取 这里只是设置了单个服务 作为一个分布式处理平台 可以设置多个代理 这里是另外两个服务的配置信息 也就是kafka的配置文件的信息 server-1.properties: broker.id=1 zookeeper.connect=localhost:2181 listeners=PLAINTEXT://:9093 log.dir=/usr/local/var/lib/kafka-logs-1 server-2.properties: broker.id=2 zookeeper.connect=localhost:2181 listeners=PLAINTEXT://:9094 log.dir=/usr/local/var/lib/kafka-logs-2 这样的话去启动其他两个新的节点 $ bin kafka-server-start /usr/local/etc/kafka/server-1.properties $ bin kafka-server-start /usr/local/etc/kafka/server-2.properties 现在创建副本为3的新topic $ kafka-topics --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-topic 现在已经有了一个集群 查看代理情况 值得注意的是: “leader”是负责给定分区所有读写操作的节点。每个节点都是随机选择的部分分区的领导者。 “replicas”是复制分区日志的节点列表，不管这些节点是leader还是仅仅活着。 “isr”是一组“同步”replicas，是replicas列表的子集，它活着并被指到leader。 相关链接 Kafka 中文文档 - ApacheCN Apache Kafka ","link":"https://GeekGhc.github.io/post/kafka-de-shi-jian-he-shi-yong/"},{"title":"Mysql数据库的主从复制和读写分离","content":"数据库的应用在日常的工作和生产中充当着数据管理者的身份 对于一些企业级的数据 数据安全和数据的操作效率显得尤为重要 那么在这里将以数据库常见的主从复制和数据库的读写分离进行一个总结 读写分离 对于这样的一个应用首先要明白他的应用场景 我们知道sql的查询比一些新增 删除这些操作更为耗时 也就是主数据库负责网站NonQuery操作，从服务器负责Query操作，用户可以根据网站功能模特性块固定访问Slave服务器，或者自己写个池或队列，自由为请求分配从服务器连接 大型网站对于网站的大并发量的访问 除了实现网站的负载均衡 对于数据的处理也格外注意 如果还是传统的数据库架构 如果多的数据的操作势必会造成互数据的压力倍增从而导致请求超时和用户 体验的下降 因此我们会想到减少数据库的连接 除了代码的优化以及一些缓存技术的应用我们也是可以尝试这样的架构方式来减轻主数据库的压力 即主数据库负责写 从数据库只负责数据查询 主从复制 数据库的主从复制可以理解为就是主数据库也就是我们项目直接使用操作的数据库 而从数据库当主数据库进行数据操作变化时也会 同步更新 即主数据库怎么做从属数据库就怎么做 这样一来对于主数据库的数据就可以起到一种备份的作用 对于原理其实就是对于主数据库在进行数据操作 如增删改查时 会把这些操作记录在二进制日志中 这样从属数据库如果拿到这份日志 这样就可以重复执行这些动作 所以也就达到了复制的作用 为了演示可以准备两台服务器 这里我用的是DigitalOcean 作为测试服务器很是方便 选择创建两个Droplets 具体配置根据自己的需求来 这里由于选择本地的ssh key因此可以无需密码登录服务器 登录master和slave服务器安装mysql 这里我以5.7 $ apt-get install mysql-server-5.7 -y 配置Mysql Master 进入mysql的配置路径/etc/mysql 这里更改bind_ip为内网ip 更改server_id=1这里的1可以根据需要更改为一个数字一用来区分 最终配置为 server-id = 1 log_bin = /var/log/mysql/mysql-bin.log expire_logs_days = 10 max_binlog_size = 100M 重启mysql服务 $ service mysql restart 进入mysql创建一个数据库app $ create database app charset utf8mb4; 配置主数据 mysql&gt; create user 'slave'@'10.138.12.79' identified by 'slavepwd'; mysql&gt; GRANT REPLICATION SLAVE ON *.* TO 'slave'@'10.138.12.79'; mysql&gt; FLUSH PRIVILEGES; 我们可以查看到主数据库的状态 mysql&gt; show master status; 记录下这个日志的position和file Slave 进入主服务器 我们需要配置下mysql的基本配置 路径还是/etc/mysql 需要区分从数据库的server_id这里更改为2 最终的配置为 server-id = 2 log_bin = /var/log/mysql/mysql-bin.log expire_logs_days = 10 max_binlog_size = 100M 接下来就是从数据库的配置 进入mysql 首先关系从属配置 mysql&gt; stop slave; mysql&gt; CHANGE MASTER TO mysql&gt; MASTER_HOST='10.138.204.172', mysql&gt; MASTER_USER='slave', mysql&gt; MASTER_PASSWORD='slavepwd', mysql&gt; MASTER_LOG_FILE='mysql-bin.000001', mysql&gt; MASTER_LOG_POS=771; 这里的log就是master数据库配置的信息 重启slave mysql&gt; start slave; 为了确认可以查看最终的Slave状态 mysql&gt; show slave status/G 如果Slave_IO_Running和Slave_SQL_Running都为yes即为成功配置 这样的话在主数据库同一个数据库下的sql都将同步到salve 也就实现了我们需要的mysql主从复制 ","link":"https://GeekGhc.github.io/post/mysql-shu-ju-ku-de-zhu-cong-fu-zhi-he-du-xie-fen-chi/"},{"title":"HomeStead通过docker配置mysql容器","content":"官方homestead提供的mysql默认是5.7版本 那么如果我们需要其他版本的mysql我们可以在此基础之上新建mysql容器进行开发 简介 homestead作为PHP开发的可共享的集成虚拟环境 官方默认提供的mysql为5.7版本 PHP的版本包含了5.6 7.0 7.1 7.2还有一些诸如redis memcache 这样的缓存数据库 其实这足以应对大部分的开发环境要求 对于mysql有时我们需要更新或者更低版本的数据库时 但是又不想摒弃现有的版本 那么我们完全可以借助docker建立对应的mysql容器 安装 我们这里以安装mysql5.6为例 对于现有的homestead即Ubuntu的环境 我们需要先下载docker 先更新下源 $ sudo apt-get update 下载对应的docker $ sudo apt-get install docker.io 下载完成之后启动服务 $ sudo service docker start 由于种种原因 对于官方库拉取image我们需要镜像加速 更改/etc/docker/daemon.json来配置(没有则新建) { &quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;] } 接下来就是验证docker 以经典的hello world为例 如果出现这样的界面即为成功 因为已经拉取这个image 我们可以查看一下现有的image 对于docker的基础知识可以查看阮一峰的博客 接下来就是拉取mysql5.6的官方library 拉取完成之后再次查看image列表 接下来新建/docker/mysql5.6目录以配置一些mysql下面会用到的映射目录 拉取完成之后再次查看image列表 接下里在/docker/mysql5.6目录下启动mysql容器 $ sudo docker run -p 33061:3306 --name mysql1 -v $PWD/conf:/etc/mysql -v $PWD/logs:/logs -v $PWD/data:/mysql_data -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 这里也是在启动的时候为数据库设置了密码为123456 -p 指明mysql容器的3306端口映射到本地的33061端口 --name 指明启动后的container的别名 这样我们可以很方便的操作 -v 表明本地和容器的目录映射 启动mysql容器 这里我们可以通过别名启动 $ sudo docker start mysql1 进入mysql的bash即终端 $ docker exec -it mysql1 env LANG=C.UTF-8 bash 其中 env LANG=C.UTF-8 bash 让docker命令行支持中文 复制配置文件 $ cp /usr/my.cnf /etc/mysql/my.cnf 由于之前我们在/docker/mysql5.6新建了三个目录并进行了映射 如需要修改编码 这时退出容器bash可在本地修改即可 $ vim conf/my.cnf 主要是在my.cnf添加了字符编码 修改完保存即可 $ [client] default-character-set=utf8 [mysql] default-character-set=utf8 [mysqld] character-set-server=utf8 再次进入mysql的bash 这次先登录mysql $ mysql -u root -p 密码就是启动容易时设定的123456 进入成功之后修改下时远程客户端可以连接这个mysql $ grant all privileges on rise.* to root@'%' identified by '123456'; $ flush privileges; 即远程客户端可以使用root作为用户名 密码使用123456进行登录 至此我们的mysql容器安装完毕 我们知道本地连接homestead的mysql是通过33060端口进行连接 那么如果想连接homestead中的mysql容器那么可以再次进行端口映射 具体操作就是修改Homestead.yaml 增加端口从本地到homestead即可 ports: - send: 33062 to: 33061 这样本地 可以使用用户名为root 密码为123456 端口这时改为33062即可连接homestead的mysql容器 相关链接 Docker 入门教程 ","link":"https://GeekGhc.github.io/post/homestead-tong-guo-docker-pei-zhi-mysql-rong-qi/"},{"title":"Laravel5.5新特性之自定义验证规则","content":"在Laravel5.5中支持了自定义验证规则以此来作为Validator:extend进行验证规则的替换方法 简介 Laravel5.5正式版这些天也即将发布 他给我们带来的新特性也十分一批开发者激动不已 我们先来谈谈所支持的自定义验证规则 在Laravel5.5中新增了php artisan make:rule这个命令 这个命令可以很方便的生成我们自己定义的验证规则 在自己的实现的验证规则里我们可以自己去定义业务判断逻辑 着手实现 1.创建项目 因为Laravel5.5还没有正式发布 不过我们是可以拿到它的源码 在命令行执行 $ laravel new laravel5-5 --dev 过会儿便可以下载完成 下载完成之后我们会看见有个报错页面 嗯哼 这个报错看起来很酷有木有 我们可以准备看到报错信息和文件位置 当然还可以直接google到相应的解决方案 nice~ 当然这里的报错是因为我们还没有生成app:key so在项目根目录下执行 $ php artisan key:generate 重新启动一下我们的项目就ok啦 2.创建规则 作为事例 我们可以去创建一个简单的字符串长度判断的验证 在终端执行: $ php artisan make:rule LengthRule 这个时候我们会发现在app目录下的rule文件夹下生成了LengthRule.php的验证文件 而我们主要的验证方法就是在pass方法中去实现 public function passes($attribute, $value) { return $value&gt;6; } 当然在下面的message方法中去定义我们的验证结果的消息 public function message() { return '字符串长度不能低于6位'; } 这样的话我们就可以在控制器的方法中去使用这个验证规则 public function testRule() { $this-&gt;validate(request(),[ 'name'=&gt;[new LengthRule()] ]); } 如果想要测试简单的方法就是在路由中新建一个方法 对于参数可以使用借助postman等工具提交参数请求 值得注意的是如果这个参数字段为空 那么这个验证规则是不会执行的 要想不存在这个字段也去执行这个这个验证规则 那么我们需要去实现ImplicitRule契约 比方说 class LengthRule implements ImplicitRule { public function __construct() { // } public function passes($attribute, $value) { return $value&gt;6; } public function message() { return '字符串长度不能低于6位'; } } 对于想了解这样的新特性最好的办法就是去Github上的这个项目的PR ","link":"https://GeekGhc.github.io/post/laravel55-xin-te-xing-zhi-zi-ding-yi-yan-zheng-gui-ze/"},{"title":"Elasticsearch搜索引擎","content":"Elasticsearch是一个基于Apache Lucene(TM)的开源搜索引擎。无论在开源还是专有领域，Lucene可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。 简介 Elasticsearch是一个基于Apache Lucene(TM)的开源搜索引擎。无论在开源还是专有领域，Lucene可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。 但是，Lucene只是一个库。想要使用它，你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。 Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。 安装 Elasticsearch需要Java8的运行环境。因此无论你是windows平台还是linux平台。先确保Java环境的安装。可以查看Oracle官网查看，这里不做详述。 Java环境安装完毕之后就可以去下载Elasticsearch。这里我是以Linux的环境进行安装。在官网的下载页面https://www.elastic.co/downloads/elasticsearch 这里我下载的是6.2.3的版本。6.0版本更新的还是很多的，这在后面会提到。下载tar包: $ curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.3.tar.gz 解压下载的安装包： $ tar -xvf elasticsearch-6.2.3.tar.gz 进入解压后的文件夹后我们就可以启动Elasticsearch $ ./elasticsearch 如果启动成功你会看到这样的: 启动一个新的窗口 因为服务的默认端口是9200 当然可以自行设置 http请求会返回如下 $ curl -X GET 'localhost:9200/?pretty' 基本概念 集群(Cluster) 一个集群是由一个或多个节点(服务器)组成的，通过所有的节点一起保存你的全部数据并且提供联合索引和搜索功能的节点集合。每个集群有一个唯一的名称标识，默认是“elasticsearch”。这个名称非常重要，因为一个节点(Node)只有设置了这个名称才能加入集群，成为集群的一部分。 确保你没有在不同的环境下重用相同的名称，否则你最终可能会将节点加入错误的集群。例如你可以使用dev，stage和prod来分别给开发，展示和生产集群命名。 一个集群中只有一个节点是有效的并且是非常好的。所以这样的话，你可能需要部署多个集群并且每个集群有它们唯一的集群名称。 索引(Index) 一个索引就是含有某些相似特性的文档的集合。例如，你可以有一个用户数据的索引，还有其他的有规则数据的索引。一个索引被一个名称(必须都是小写)唯一标识，并且这个名称被用于索引通过文档去执行索引，搜索，更新和删除操作。 这里的索引其实你就可以理解为MySQL中的数据库，因此你在一个集群中可以定义任意多的Index 使用 在开始使用ES之前可以大概了解一下他与我们平时的关系型数据库的对比 MySQL ES Database（数据库） Index （索引） Table（表） Type （类型） Row（行） Document （文档） Index（索引） Everything Indexed by default （所有字段都被索引）） SQL（结构化查询语言） Query DSL（查询专用语言） 值得注意的是：Type在6.0.0版本中已经不赞成使用 所以在6.0版本以后一个doc也就是一个文档是一个可被索引的数据的基础单元。我们完全可以通过类似/index/doc/:id这样的形式表现出来 基本命令 1.首先我们可以去了解一个集群的阿康状况 使用_cat API去查看(包括查看节点索引这些都会使用这个API) $ curl -X GET 'localhost:9200/_cat/health?v&amp;pretty' pretty参数 是返回格式的美化 下面不做太多介绍 响应的结果会是这样： epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent 1475247709 17:01:49 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0% 注意这里的状态为green 这里的状态有三种 red、yellow、green green - 一切运行正常(集群功能齐全) yellow - 所有数据是可以获取的，但是一些复制品还没有被分配(集群功能齐全) red 一些数据因为一些原因获取不到(集群部分功能不可用) 2.查看节点 $ curl -X GET 'localhost:9200/_cat/nodes?v&amp;pretty' 返回的结果为: ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name 127.0.0.1 16 94 3 0.01 0.01 0.02 mdi * XJCA34j 这里可以看到返回的只有一个名为XJCA34j的节点 它也是目前集群中唯一的节点 3.列出所有索引 $ curl -X GET 'localhost:9200/_cat/indices?v&amp;pretty' 返回的结果为: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size 使用事例 1.先尝试创建一个索引 $ curl -X PUT 'localhost:9200/user?pretty&amp;pretty' 返回的结果为: { &quot;acknowledged&quot; : true, &quot;shards_acknowledged&quot; : true, &quot;index&quot; : &quot;user&quot; } 再列出所有的索引 $ curl -X GET 'localhost:9200/_cat/indices?v&amp;pretty' 2.插入一条数据(文档) curl -X PUT 'localhost:9200/user/doc/1?pretty&amp;pretty' -H 'Content-Type: application/json' -d '{&quot;name&quot;: &quot;GeekGhc&quot;}' 返回的结果为: { &quot;_index&quot; : &quot;user&quot;, &quot;_type&quot; : &quot;doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : { &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1 } 从返回的结果我们可以看到这条数据的基本信息 如是在user这个index type为doc 并且id为1 3.获取一条数据 那么查看这条数据时执行 $ curl -X GET 'localhost:9200/user/doc/1?pretty&amp;pretty' 返回的结果为: { &quot;_index&quot; : &quot;user&quot;, &quot;_type&quot; : &quot;doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : { &quot;name&quot;: &quot;GeekGhc&quot; } } 4.修改一条数据 其实修改和插入形式是一样的 如果存在那个id那么就会更新之前文档的数据 反之则会增加一条文档数据 curl -X PUT 'localhost:9200/user/doc/1?pretty&amp;pretty' -H 'Content-Type: application/json' -d '{&quot;name&quot;: &quot;JellyBean&quot;}' 5.删除一条索引(可以理解为删除一个数据库) $curl -X DELETE 'localhost:9200/customer?pretty&amp;pretty' 在此列出所有索引即可查看到最终效果 6.批处理数据 往往我们会需要批处理一些数据 比如我们需要插入几条数据 这个时候我们需要使用_bulk API 举个例子来说就是去插入两条数据 curl -X POST 'localhost:9200/user/doc/_bulk?pretty&amp;pretty' -H 'Content-Type: application/json' -d' {&quot;index&quot;:{&quot;_id&quot;:&quot;1&quot;}} {&quot;name&quot;: &quot;Jelly One&quot; } {&quot;index&quot;:{&quot;_id&quot;:&quot;2&quot;}} {&quot;name&quot;: &quot;Jelly Two&quot; } ' 当然也可以同时进行插入和删除等操作 搜索 在使用搜索之前我们需要一批数据 这里我引用别人的一段json数据 其生成工具是www.json-generator.com json-generator 里面会有字段的定义语法 对着它的帮组文档可以定义你需要的字段 你可以从这里进行下载 下载之后放入当前目录我们就可以加载到集群中 $ curl -H &quot;Content-Type: application/json&quot; -XPOST 'localhost:9200/bank/account/_bulk?pretty&amp;refresh' --data-binary &quot;@accounts.json&quot; 接着再去查看所有索引 $ curl -X GET 'localhost:9200/_cat/indices?v&amp;pretty' 这样的话我们就在bank这个Index中的account类型(type)下插入了1000条数据 搜索API 在执行搜索时我们需要的API是_search 在执行搜索时会有两种方式 一种是通过url方式 而另外一种则是通过将请求放入请求体中 这里更推荐所有的请求放入请求体 这样更为友好和解读 就以一个简单的搜索来说 如果是url方式 Curl请求 $ curl -X GET 'localhost:9200/bank/_search?q=*&amp;sort=account_number:asc&amp;pretty&amp;pretty' 如果是放入请求体中： $ curl -X GET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d' { &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;: [ { &quot;account_number&quot;: &quot;asc&quot; } ] } ' 两者返回结果为: { &quot;took&quot; : 63, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : 1000, &quot;max_score&quot; : null, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;bank&quot;, &quot;_type&quot; : &quot;account&quot;, &quot;_id&quot; : &quot;0&quot;, &quot;sort&quot;: [0], &quot;_score&quot; : null, &quot;_source&quot; : {&quot;account_number&quot;:0,&quot;balance&quot;:16623,&quot;firstname&quot;:&quot;Bradshaw&quot;,&quot;lastname&quot;:&quot;Mckenzie&quot;,&quot;age&quot;:29,&quot;gender&quot;:&quot;F&quot;,&quot;address&quot;:&quot;244 Columbus Place&quot;,&quot;employer&quot;:&quot;Euron&quot;,&quot;email&quot;:&quot;bradshawmckenzie@euron.com&quot;,&quot;city&quot;:&quot;Hobucken&quot;,&quot;state&quot;:&quot;CO&quot;} }, { &quot;_index&quot; : &quot;bank&quot;, &quot;_type&quot; : &quot;account&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;sort&quot;: [1], &quot;_score&quot; : null, &quot;_source&quot; : {&quot;account_number&quot;:1,&quot;balance&quot;:39225,&quot;firstname&quot;:&quot;Amber&quot;,&quot;lastname&quot;:&quot;Duke&quot;,&quot;age&quot;:32,&quot;gender&quot;:&quot;M&quot;,&quot;address&quot;:&quot;880 Holmes Lane&quot;,&quot;employer&quot;:&quot;Pyrami&quot;,&quot;email&quot;:&quot;amberduke@pyrami.com&quot;,&quot;city&quot;:&quot;Brogan&quot;,&quot;state&quot;:&quot;IL&quot;} }, ... ] } } 对于返回结果的字段的解释如下: took - ES执行此次搜索所用的时间(单位：毫秒) timed_out - 告诉我们此次搜索是否超时 _shards - 告诉我们搜索了多少分片，还有搜索成功和搜索失败的分片数量 hits - 搜索结果 hits.total - 符合搜索条件的文档数量 hits.hits - 实际返回的搜索结果对象数组(默认只返回前10条) hits.sort - 返回结果的排序字段值(如果是按score进行排序，则没有) hits._score 和 max_score - 目前先忽略这两个字段 查询语法 和很多关系型数据库一样 在查询方面 ES也提供了很多查询方法 举个例子来说 curl -X GET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d' { &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;: [ { &quot;account_number&quot;: &quot;asc&quot; } ], &quot;from&quot;: 10, &quot;size&quot;: 1 } ' 这里我一下子列出了几个参数 match_all部分简单指定了我们想去执行的查询类型，这里意思就是在索引中搜索所有的文档。sort来指定搜索结果的顺序。size来指定返回的结果数量。rom参数(从0开始)指定了从哪个文档索引开始。 这个和mysql中的limit和offset是一个意思。 还有就是在Laravel中有一种字段映射，在MySQL中返回指定字段 那么只需要在请求体中加入_source属性即可: curl -X GET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d' { &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;_source&quot;: [&quot;account_number&quot;, &quot;balance&quot;] } ' 这样的话我们只会返回account_number和balance两个字段 关键字搜索 这是为了应对全文搜索这样的应用场景 下面来搜索address字段里有mill这个单词的文档集 curl -X GET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d' { &quot;query&quot;: { &quot;match&quot;: { &quot;address&quot;: &quot;mill&quot; } } } ' 当然我们回遇到多个查询条件 如满足关键词既包含mail也包含com关键词那么其语法为: curl -X GET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d' { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match&quot;: { &quot;address&quot;: &quot;mill&quot; } }, { &quot;match&quot;: { &quot;address&quot;: &quot;com&quot; } } ] } } } ' 对应与的操作还有或操作 即包含mail或com curl -X GET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d' { &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;match&quot;: { &quot;address&quot;: &quot;mill&quot; } }, { &quot;match&quot;: { &quot;address&quot;: &quot;com&quot; } } ] } } } ' 当然还有非这样的操作 即没有这两个关键词 curl -X GET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d' { &quot;query&quot;: { &quot;bool&quot;: { &quot;must_not&quot;: [ { &quot;match&quot;: { &quot;address&quot;: &quot;mill&quot; } }, { &quot;match&quot;: { &quot;address&quot;: &quot;com&quot; } } ] } } } ' 从上面三个事例可以看出bool下其实包含了三种组合 must should must_not对应着与或非 你可以随意组合这些条件 判断过滤 在查询语言中经常会遇到一些大于等于小于这些判断过滤 这在ES中的表现为 curl -X GET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d' { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: { &quot;match_all&quot;: {} }, &quot;filter&quot;: { &quot;range&quot;: { &quot;balance&quot;: { &quot;gte&quot;: 20000, &quot;lte&quot;: 30000 } } } } } } ' 例使用bool查询返回所有余额在20000到30000之间的账户(包含边界)。换句话说，我们想查询账户余额大于等于20000并且小于等于30000的用户。 使用ik分词进行搜索 为了实现分词 在github上根据文档进行安装https://github.com/medcl/elasticsearch-analysis-ik 因为我这里版本是6.2.3的 在当前目录可以执行 $ ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.2.3/elasticsearch-analysis-ik-6.2.3.zip 安装完毕之后重启ES 在去执行list命令查看是否有这个plugin $ ./bin/elasticsearch-plugin list 下面可以进行一次关键词搜索 需指定高亮的字段 curl -X POST 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d' { &quot;query&quot;: { &quot;match&quot;: { &quot;address&quot; : &quot;993&quot; } }, &quot;highlight&quot; : { &quot;pre_tags&quot; : [&quot;&lt;tag1&gt;&quot;, &quot;&lt;tag2&gt;&quot;], &quot;post_tags&quot; : [&quot;&lt;/tag1&gt;&quot;, &quot;&lt;/tag2&gt;&quot;], &quot;fields&quot; : { &quot;address&quot; : {} } } }' 这里我们就是去查找address字段含有993的文档数据 最后返回的结果为: { &quot;took&quot; : 187, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : 2, &quot;max_score&quot; : 4.388994, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;bank&quot;, &quot;_type&quot; : &quot;account&quot;, &quot;_id&quot; : &quot;766&quot;, &quot;_score&quot; : 4.388994, &quot;_source&quot; : { &quot;account_number&quot; : 766, &quot;balance&quot; : 21957, &quot;firstname&quot; : &quot;Thomas&quot;, &quot;lastname&quot; : &quot;Gillespie&quot;, &quot;age&quot; : 38, &quot;gender&quot; : &quot;M&quot;, &quot;address&quot; : &quot;993 Williams Place&quot;, &quot;employer&quot; : &quot;Octocore&quot;, &quot;email&quot; : &quot;thomasgillespie@octocore.com&quot;, &quot;city&quot; : &quot;Defiance&quot;, &quot;state&quot; : &quot;MS&quot; }, &quot;highlight&quot; : { &quot;address&quot; : [ &quot;&lt;tag1&gt;993&lt;/tag1&gt; Williams Place&quot; ] } }, { &quot;_index&quot; : &quot;bank&quot;, &quot;_type&quot; : &quot;account&quot;, &quot;_id&quot; : &quot;58&quot;, &quot;_score&quot; : 4.388994, &quot;_source&quot; : { &quot;account_number&quot; : 58, &quot;balance&quot; : 31697, &quot;firstname&quot; : &quot;Marva&quot;, &quot;lastname&quot; : &quot;Cannon&quot;, &quot;age&quot; : 40, &quot;gender&quot; : &quot;M&quot;, &quot;address&quot; : &quot;993 Highland Place&quot;, &quot;employer&quot; : &quot;Comcubine&quot;, &quot;email&quot; : &quot;marvacannon@comcubine.com&quot;, &quot;city&quot; : &quot;Orviston&quot;, &quot;state&quot; : &quot;MO&quot; }, &quot;highlight&quot; : { &quot;address&quot; : [ &quot;&lt;tag1&gt;993&lt;/tag1&gt; Highland Place&quot; ] } } ] } } 相关连接 ES6.2官方文档 IK Analysis for Elasticsearch ","link":"https://GeekGhc.github.io/post/elasticsearch-sou-suo-yin-qing/"},{"title":"Swagger 构建项目中Api文档","content":"作为一个后端程序员,在编辑后台接口时,我们可以有多种选择,当然在此之前介绍过apidocJs,这里再介绍一个目前在用的接口管理工具Swagger 简介 在后端提供的前端的接口时，以前可能以一分md形式的文档提供给前端，现在开发对于前后台的交互很是常见，在编写我们后台的文档时，再遵循一定的规范即OpenAPI规范，根据这样的规范我们可以更加准确快速的描述API 而和swagger这些文档描述一样 ，你可以理解成一种独立与程序语言的注释性语言，因为他们不会随着最后程序而编译，而是单独的独立出来，根据自身的解释器而转换成一种格式文档，最后再加以渲染，比如swagger由swagger-ui渲染之后，前端程序员就可以直观的指导API的作用，并提供了交换测试的方式，这样的话很是方便 swagger具有十分庞大的生态系统，几乎支持所有的编程语言，对于这样的交互式的API文档对于开发者来说是很好的福音 使用 这里以laravel项目为例，对于swagger已有先人帮我们集成好了，这里我们使用的是zircote/swagger-php,和laravel的扩展一样通过composer安装 $ composer require zircote/swagger-php 安装完毕之后 我们可以在应用的目录App/Http下新建Api目录用来存放我们的Api接口 新建一个BaseController来写基本的返回信息格式 当然在这个控制器里也会标注我们接口的版本信息 在BaseController里去定义接口的版本等信息 /** * @SWG\\Swagger( * host=&quot;www.geekghc.com&quot;, * schemes={&quot;http&quot;, &quot;https&quot;}, * basePath=&quot;/api/&quot;, * @SWG\\Info( * version=&quot;1.0.0&quot;, * title=&quot;codespace API文档&quot;, * description=&quot;codespace community description...&quot;, * ) * ) */ 这里指明了host version title description这些描述信息 在接下来的接口类中都会继承这个父类 在解释器最终形成文档时也只会去解析这样的非代码的解释语言 最终就是得我们的API文档 在这里我们最终生成的是json组成的文件信息,那么如果只是这样留给我们的前端相比会一脸懵逼,作为接口文档我们 希望其他人可以在线执行模拟接口信息并提供一定的UI这样看起来才是友好的 接下来新建一个路由用来展示最终的Api文档 //api文档 Route::get('/api/docs',function(){ return view('apidoc'); }); //api文档json Route::get('get/swagger', ['middleware' =&gt; 'auth', function () { echo file_get_contents('./swagger.json'); }]); 第一个api/docs用来返回我我们的view视图 最后需要呈现的是一个UI的接口文档 因此我们需要引入swagger-ui来丰富我们的界面 这里返回的视图为apidoc.blade.php 首先定义好文档的用户名和密码 这样方便为内部使用 &lt;?php define('ADMIN_USERNAME','admin'); // Admin Username define('ADMIN_PASSWORD','ghc1996'); // Admin Password if (!isset($_SERVER['PHP_AUTH_USER']) || !isset($_SERVER['PHP_AUTH_PW']) || $_SERVER['PHP_AUTH_USER'] != ADMIN_USERNAME ||$_SERVER['PHP_AUTH_PW'] != ADMIN_PASSWORD) { Header(&quot;WWW-Authenticate: Basic realm=\\&quot;Memcache Login\\&quot;&quot;); Header(&quot;HTTP/1.0 401 Unauthorized&quot;); echo &lt;&lt;&lt;EOB &lt;html&gt;&lt;body&gt; &lt;h1&gt;Rejected!&lt;/h1&gt; &lt;big&gt;Wrong Username or Password!&lt;/big&gt; &lt;/body&gt;&lt;/html&gt; EOB; exit; } ?&gt; 至于swagger-ui可以直接去官网https://swagger.io/swagger-ui/进行下载 在文件中继续引入必要的css和js文件即可 这里的详情我放在gist上 请自行查看 ##Swagger结合项目 之前都是安装的准备工作，接下来结合到具体的项目里，这里我以我其中的一个项目为例： 首先介绍一下OpenAPI规范。OpenAPI是Linux基金会的一个项目。试图通过定义一种用来描述API格式或API定义的语言，来规范RESTful服务开发过程。OpenAPI规范帮助我们描述一个API的基本信息，比如： 有关该API的一般性描述 可用路径（/资源） 在每个路径上的可用操作（获取/提交...） 每个操作的输入/输出格式 在实际项目中 对于接口的基类 这里放在BaseController我们定义swagger的基本组成部分 /** * @SWG\\Swagger( * swagger=&quot;2.0&quot;, * host=&quot;codespace.example&quot;, * schemes={&quot;http&quot;, &quot;https&quot;}, * basePath=&quot;/api/&quot;, * @SWG\\Info( * version=&quot;1.0.1&quot;, * title=&quot;codespace API文档&quot;, * description=&quot;codespace system description...&quot;, * ), * @SWG\\Tag(name=&quot;User&quot;, description=&quot;用户&quot;), * @SWG\\Tag(name=&quot;Post&quot;, description=&quot;动态&quot;), * @SWG\\Tag(name=&quot;Article&quot;, description=&quot;文章&quot;), * @SWG\\Tag(name=&quot;Message&quot;, description=&quot;消息&quot;), * @SWG\\Tag(name=&quot;Article&quot;, description=&quot;标签&quot;), * @SWG\\Tag(name=&quot;Comment&quot;, description=&quot;评论&quot;), * @SWG\\Tag(name=&quot;Action&quot;, description=&quot;用户互动操作&quot;), * @SWG\\Tag(name=&quot;Comment&quot;, description=&quot;侧边栏&quot;), * @SWG\\Tag(name=&quot;Setting&quot;, description=&quot;用户设置&quot;), * * ) */ 这里首先通过swagger属性来申明OpenAPI 规范的版本。 &lt;?php /** * @SWG\\Swagger( * swagger=&quot;2.0&quot; * ) */ 接下来就是接口文档的地址和给开发者的URL地址 这里还定义了接口地址的前缀 * host=&quot;codespace.example&quot;, * schemes={&quot;http&quot;, &quot;https&quot;}, * basePath=&quot;/api/&quot;, 接下来就是接口文档的描述信息 这其中包含了标题、版本和描述信息 * @SWG\\Info( * version=&quot;1.0.1&quot;, * title=&quot;codespace API文档&quot;, * description=&quot;codespace system description...&quot;, * ), 为了实际的接口分类 这里引入了Tag的概念 这在每个接口申明时可以对其进行归并分类 ```?php?start_inline=` @SWG\\Tag(name=&quot;User&quot;, description=&quot;用户&quot;), @SWG\\Tag(name=&quot;Post&quot;, description=&quot;动态&quot;), @SWG\\Tag(name=&quot;Article&quot;, description=&quot;文章&quot;), 定义完毕基类中的接口基本信息后我们可以尝试着去书写详细的接口信息 以用户为例 ```php?start_inline=1 /** * 当前用户 * @SWG\\Get( * tags={&quot;User&quot;}, * path=&quot;/user&quot;, * operationId=&quot;UserController&quot;, * summary=&quot;当前用户&quot;, * description=&quot;当前用户&quot;, * produces={&quot;application/json&quot;}, * @SWG\\Parameter( * name=&quot;id&quot;, * in=&quot;query&quot;, * description=&quot;用户id&quot;, * required=true, * type=&quot;string&quot;, * ), * @SWG\\Response( * response=200, * description=&quot;当前用户&quot;, * @SWG\\Schema( * type=&quot;array&quot;, * example={&quot;errcode&quot;:&quot;响应代码&quot;,&quot;data&quot;:{{&quot;name&quot;:&quot;项目名称&quot;,&quot;logo&quot;:&quot;项目封面&quot;,&quot;creatorId&quot;:&quot;项目拥有者id&quot;,&quot;status&quot;:&quot;0 删除 | 1 活动&quot;,&quot;updated_at&quot;:&quot;项目修改时间&quot;,&quot;isArchived&quot;:&quot;是否归档 true 归档 false 未归档&quot;,&quot;created_at&quot;:&quot;项目添加时间&quot;,&quot;creator&quot;: {&quot;uid&quot;: &quot;项目拥有者id&quot;,&quot;full_name&quot;: &quot;项目拥有者姓名&quot;,&quot;avatar&quot;: &quot;项目拥有者头像&quot;}}},&quot;errmsg&quot;:&quot;响应信息&quot;} * ) * ) * ) */ 这里将当前用户的这个接口归并为User这个Tag 同时也包含了url地址、请求方式、操作控制器、说明这些属性信息 在Parameter里申明请求参数 这里需要的是用户的id,in申明请求体类型 当然如果是post请求 这里的方式应该为formData 有了请求的参数定义当然也需要有返回值得定义 在Response体里返回对应的数据体 这里只是以一个小的用户信息接口为例 在每个接口方法中可以对应着不同的接口请求和返回 而这些也基本都是大同小异 最后完成了接口文档的编写 在此之前说过最后我们需要将所有的接口描述生成到一个json文件中再通过semantic-ui渲染的界面呈现给前端开发者 这里的生成命令为 $ php ./vendor/zircote/swagger-php/bin/swagger ./app/Http/Controllers/Api -o ./public 也就是说将./app/Http/Controllers/Api路径下的所有API的控制文件的接口信息生成到public目录 最后的结果访问api/doc就是这样的 相关链接 swagger官网 zircote/swagger-php插件地址 gist地址 如何编写 Swagger-PHP 的 API 文档 ","link":"https://GeekGhc.github.io/post/swagger-gou-jian-xiang-mu-zhong-api-wen-dang/"},{"title":"QueryList-PHP的采集利器","content":"在php中做数据采集比起采用正则进行匹配获取 我们也会有其他的解决方案 比如说今天要说的基于PHPQuery的PHP采集利器QueryList 简介 在面对数据采集的需求时 我们自然会想到采用爬虫这类的工具 在php中我们同样是以爬虫采集数据的形式 在PHP中我们需要知道和了解的 就是要说的QueryList 这是一套基于PHPQuery的数据采集方案 其使用习惯和我们使用jquery操作dom元素的选择器一样 十分直观 数据采集 一开始我们就说过queryList获取元素和jquery的选择器差不多 所以说如果我们想要获取一些基本元素 诸如一个页面的图片 指定区域的文本文件等等 当然在采集数据数据之前我们要明白所给我们的采集规则是这样的: //采集规则 $rules = array( '规则名' =&gt; array('jQuery选择器','要采集的属性'[,&quot;标签过滤列表&quot;][,&quot;回调函数&quot;]), '规则名2' =&gt; array('jQuery选择器','要采集的属性'[,&quot;标签过滤列表&quot;][,&quot;回调函数&quot;]), .......... ); //注:方括号括起来的参数可选 所对应的具体规则参数直接参照文档就好了 https://doc.querylist.cc/site/index/doc/13 既然有了这些规则参数 那么我们就可以通过规则去采集一个页面的数据了 这时我们假设去采集下laravel-china的页面部分数据 $ql = QueryList::get('https://laravel-china.org/topics')-&gt;rules([ 'img'=&gt;array('.media-object.avatar','src'), 'title'=&gt;array('.media-heading&gt;a','text') ]); $data = $ql-&gt;query(function($item){ return $item; })-&gt;getData(); print_r($data-&gt;all()); 这样就可以分别获取对应的href和text属性 在定义规则执行完query自后才可以通过getData获取数据 并在这里的回调函数自定义自己的返回形式 最后这样是以一个二维数据的形式返回的 当然这个会有个证书的问题 因此我们可以直接获取页面的内容 $html = file_get_contents('https://laravel-china.org/topics'); $ql = QueryList::html($html)-&gt;rules([ 'img'=&gt;array('.mddia-object.avatar','src'), 'title'=&gt;array('.media-heading&gt;a','text') ]); 得到数据数组后我们最好相应的释放资源 $ql-&gt;destruct(); Api用法 就如之前多说 我们可以像jquery操作dom一样 我们可以直接使用find查找我们需要的元素 //获取页面中所有图片地址 $data = $ql-&gt;find('img')-&gt;attrs('src'); print_r($data-&gt;all()); 这样我们其实和再去定义规则筛选的结果是一样的 只不过这样的方式更为简单方便 除了find还有一些其他的常用的Api用法其实和我们平时写js时差不多 如获取文本 获取html文本 获取孩子节点等 具体的用法和举例文档上也写的很清楚了 https://doc.querylist.cc/site/index/doc/20 还有一些高级的用法 如绑定自定义参数 这样我们可以使用自定义的方法 如文档介绍的一种就是 $ql = QueryList::getInstance(); //注册一个myHttp方法到QueryList对象 $ql-&gt;bind('myHttp',function ($url){ $html = file_get_contents($url); $this-&gt;setHtml($html); return $this; }); //然后就可以通过注册的名字来调用 $data = $ql-&gt;myHttp('https://toutiao.io')-&gt;find('h3 a')-&gt;texts(); print_r($data-&gt;all()); 扩展的作用就是定义自己的需求 这样就达到了功能扩展的目的 如我们需要对一个页面上的图片进行二次处理 这样我们就可以通过bind自定义的方法实现 还有一个需要注意的就是在使用插件时 我们需要使用use这个关键字 具体的用法下面在说到插件就知道了 http请求 在queryList也是可以进行http请求的 那么这样一来我们就可以进行模拟登陆 请求获取等操作 在get请求中我们可以进行携带cookie进行get请求 这样我们就可以请求到一个登陆后的页面 拿文档所说的一个微博实例来说就是为了获取登陆后的页面内容 我们就可以在拿到登录后的cookie尽情get请求 $ql = QueryList::get('http://weibo.com',[],[ 'headers' =&gt; [ //填写从浏览器获取到的cookie 'Cookie' =&gt; 'SINAGLOBAL=546064; wb_cmtLike_2112031=1; wvr=6;....' ] ]); //echo $ql-&gt;getHtml(); echo $ql-&gt;find('title')-&gt;text(); //输出: 我的首页 微博-随时随地发现新鲜事 同样的post的用法其实差不多 就如我们在模拟一些网站的登录请求 插件使用 最后他为我们提供了几个常见的功能插件 如url的转换插件 $ql = QueryList::getInstance(); $ql-&gt;use(AbsoluteUrl::class); //或者自定义函数名 $ql-&gt;use(AbsoluteUrl::class,'absoluteUrl','absoluteUrlHelper'); 这里的use就是引入我们的url插件 这样的话我们就可以转换一个也页面的所有url $data = $ql-&gt;get('https://toutiao.io/') -&gt;absoluteUrl('https://toutiao.io/') -&gt;find('a')-&gt;attrs('href'); print_r($data); 还有一些基于百度和谷歌的搜索插件 也是非常实用的 当然所有的插件都需要compoer进来 这样我们才可以使用use进行声明使用 实际应用场景 最后举个我们平时项目中需要用到的一个应用场景吧 我们在发布文章时 可能我们会提供给作者一个抓取链接发布文章 当用户发现一个是非精彩的文章 但又省去粘贴复制这样的操作时 我们就可以利用queryList进行数据采集 通过用户提供的url我们可以采集页面上的文章标题 作者 文章的主体内容这些有用信息 具体的操作就是拿到这些元素的规则这样我们就会很方便了 这里我以微信公众号的文章为例 我们在通过url抓取到页面的文章内容 具体的代码如下 $url = &quot;http://mp.weixin.qq.com/s/r5MGWqaTEiYHi9c3fEIDhQ&quot;; $html = file_get_contents($url); //获取文章源码并保存到参数中 $html = str_replace(&quot;&lt;!--headTrap&lt;body&gt;&lt;/body&gt;&lt;head&gt;&lt;/head&gt;&lt;html&gt;&lt;/html&gt;--&gt;&quot;, &quot;&quot;, $html); $data = QueryList::html($html)-&gt;rules([ 'title' =&gt; ['#activity-name', 'text'], 'author' =&gt; ['#post-user', 'text'], 'content' =&gt; ['#js_content', 'html']])-&gt;query(function($item){ $item['content'] = &quot;文章内容 :&quot;.$item['content']; return $item; })-&gt;getData(); dd($data); 如果是获取第一篇文章的标题其实很简单 这样的在laravel其实返回的是一个collection 所以我们可以这样 dd($data[0]['title']); 至于其他的以此类推就是了 相关链接 QueryList 中文文档 ","link":"https://GeekGhc.github.io/post/querylist-php-de-cai-ji-li-qi/"},{"title":"laravel5.5 之Api Resource","content":"在laravel5.5中 引入了新的概念就是Api Resource这样我们就可以不用之前的Dingo而自定义返回字段和数据 在Laravel5.5之前 我们在处理我们的api接口时 会用到dingo来管理我们的后台数据 那么在5.5中 引入了新的概念就是 api resource 正如这个名字一样 我们可以建立api资源管理 在之前我们也有说过编写后台的api服务 那么在处理数据返回字段时 我们会选择对字段进行映射 这样来选取我们客户端需要的 数据 这在之前我们是通过一个Transformer这个中间层来进行映射 当然再此之前我们先创建好User 和 Post这两个Model 并生成对应的数据字段 在5.5中我们可以有更好的资源管理形式 话不都说 新建好必要的项目后 我们去创建用户的api管理 $ php artisan make:resource User 其中make:resource命令也是5.5新增的 这样我们会在app/Http/Resources 增加了一个继承Resource的User Class 在路由中 为了演示 我们可以在路由返回其中的一个用户 Route::get('user',function(){ $user = \\App\\User::find(2); return new \\App\\Http\\Resources\\User($user); }); 这样的话我们会返回id为2的这个用户的全部信息 但这并不是我们所需要的 那么我们可以借用api resource进行字段的映射 在\\App\\Http\\Resources\\User里我们去返回一些基本字段 public function toArray($request) { return [ 'name'=&gt;$this-&gt;name, 'email'=&gt;$this-&gt;email ]; } 这样一来我们再去访问之前的路由返回给我们的也就是筛选之后的字段 当然现在有个问题就是 实际项目中我们可能需要返回其他字段 比如我们的客户端需要知道是否返回成功的信息 那么我们可以再去重写这个Resource中的with方法 public function with($request) { return [ 'code'=&gt;1, 'status'=&gt;'success' ]; } 这样的话我们就是增加了返回字段code和status 当然我们在返回数据body部分我们的是放在data中 我们也可以更改这个key 在app/Providers/AppServiceProvider中 public function boot() { Resource::wrap('lists'); } 这样我们的返回数据字段就是 { &quot;lists&quot;: { &quot;name&quot;: &quot;Mrs. Orpha Waters MD&quot;, &quot;email&quot;: &quot;jose82@example.net&quot; }, &quot;code&quot;: 1, &quot;status&quot;: &quot;success&quot; } 如果我们需要返回该用户的帖子 这也很简单 再增加一个posts字段就是了 不够再此之前先关联一下用户和帖子的关系 当然有一种应用场景就是我们对字段数据的判断 就如文档中所说如果是管理员的话我们就返回特定的字段 反之不然 laravel为我们提供了when 这个 function 具体判断则是 public function toArray($request) { return [ 'name'=&gt;$this-&gt;name, 'email'=&gt;$this-&gt;email, 'secret' =&gt; $this-&gt;when($this-&gt;isAdmin(), 'secret-value'), ]; } 这个时候只有$this-&gt;isAdmin返回为true时 返回字段里才会出现secret这个key 不然的话在返回之前就已经过滤掉了 ok 在很多的场景中 我们需要返回的是一个collection比如一个用户的所有帖子 那么api resource同样为我们提供了对collection 的处理方式 在终端执行 $ php artisan make:resource UserCollection 当然这个命名是有讲究的 在生成的类的继承父类就可以看到出来 那么如果我们需要制定一个collection的处理方式在命令中可以加一个 --collection这个flag 具体可以在终端查看一下 $ php artisan make:resource -h 生成完毕之后我们会发现这个类是继承ResourceCollection这个父类的 所以同样的我们可以去返回所有的用户信息 Route::get('user',function(){ $users = \\App\\User::get(); return new \\App\\Http\\Resources\\UserCollection($users); }); 如果需要提供其他字段的也可以在with function增加 当然对于处理分页 其实在collection的分页也提供了非常详细的字段数据 Route::get('user',function(){ $users = \\App\\User::with('posts')-&gt;paginate(3); return new \\App\\Http\\Resources\\UserCollection($users); }); 从返回的数据我们可以看到新增了links和meta字段里面包含了分页我们可能需要的数据 如上一页下一页 如果需要对用户的帖子字段进行映射 创建一个Post Resource再进行一次transformer就行 对于关联数据的返回可以如以前所说采用eager loader来避免N+1的查询 那么在路由方法中我们可以采用with('posts')这样的方式 当然在Resource里我们还可以采用提供给我们的whenLoaded 这个function 具体表现为 public function toArray($request) { return [ 'name'=&gt;$this-&gt;name, 'email'=&gt;$this-&gt;email, 'posts'=&gt;Post::collection($this-&gt;whenLoaded('posts')) ]; } 相关链接 Eloquent: API 资源 ","link":"https://GeekGhc.github.io/post/laravel55-zhi-api-resource/"},{"title":"LaraDocker 在Docker中轻松运行你的laravel项目","content":"除了Homestead 我们是不是可以考虑将我们的laravel应用运行在Docker. LaraDocker无疑是一个不错的解决方案 什么是Docker Docker是一个开源的引擎，可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。 开发者在笔记本上编译测试通过的容器可以批量地在生产环境中部署，包括VMs（虚拟机）、bare metal、OpenStack 集群和其他的基础应用平台。 Docker vs Vagrant 从时间上来看，Vagrant启动虚拟机需要数分钟，而Docker只需数秒；从体量上来看，Vagrant提供的是完整的虚拟机，而Docker提供的是轻量级的虚拟容器， 这些虚拟容器共享同一个内核并且允许在独立进程中运行；此外，从应用范围来说，Vagrant只能用于开发环境，而Docker即可用于开发环境也可用于生产环境。 如果是在Linux可以查看这个文章使用 docker 部署你的 Laravel 项目 Docker 优势 比起Homestead、Vagrant，Docker的优势是轻量级，可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器 安装准备 在进行开发环境的搭建之前(laravel环境已经安装完毕) 我们需要准备下面几个 composer git docker toolbox virtual box 我们可以使用以下命令创建一个Docker虚拟机 $ docker-machine create --driver=virtualbox default 这样的话我们在virtual box就会新建一个虚拟机 我们可以去查看Docker IP的地址 在hosts文件里映射我们的ip地址 192.168.99.100 laravel.dev 接下来就是安装LaraDock 如果系统尚未安装Laravel应用，需要从头开始搭建全新的环境，可以在系统任意位置克隆Github仓库到本地： $ git clone https://github.com/LaraDock/laradock.git 这里我已经安装好了一个laravel项目 那么我在这个项目的根目录下下载就好 在项目的根目录 之前已经安装好了Laradock 进入该目录执行(和laravel项目一样) $ cp env-example .env 因为之前我们已经将ip做了映射 所以别忘了在laravel的.env文件中修改相应的配置 DB_HOST=laravel.dev REDIS_HOST=laravel.dev 最后运行容器 $ docker-compose up -d nginx mysql 当然该容器里也包含了其他的应用服务 如(你可以选择性的启动他们) nginx, hhvm, php-fpm, mysql, redis, postgres, mariadb, neo4j, mongo, apache2, caddy, memcached, beanstalkd, beanstalkd-console, workspace workspace 和 php-fpm 将运行在大部分实例中, 所以不需要在 up 命令中加上它们 启动之后，进入workspace容器，执行Laravel安装及Artisan命令等操作： $ docker-compose exec —user=laradock workspace bash 这时候可能出现问题： UnicodeDecodeError: 'ascii' codec can't decode byte 0xe9 in position 0: ordinal not in range(128) 解决办法就是 修改mimetypes.py文件，路径位于python的安装路径下的Lib\\mimetypes.py文件。在import下添加如下几行: if sys.getdefaultencoding() != 'gbk': reload(sys) sys.setdefaultencoding('gbk') 去访问http://laravel.dev应该就可以访问得到首页了 相关链接 docker-toolbox 使用 docker 部署你的 Laravel 项目 Mac laraDocker环境开发部署 docker-compose.yml详解 ","link":"https://GeekGhc.github.io/post/laradocker-zai-docker-zhong-qing-song-yun-xing-ni-de-laravel-xiang-mu/"},{"title":"GitLab [Webhooks] 实现自动化服务器部署","content":"利用的GitLab创建私人仓库 通过钩子实现代码推送时能同步到服务器 这样也就实现了自动化的部署 简介 我们在部署我们的web应用时 我们一般会寻求一些云平台服务器部署 当然也可以直接在服务器里拉取远程仓库的代码 当然我们也可以实现通过Webhooks(钩子)来实现服务器与远程仓库代码的同步 这样一来我们在本地提交功能分支到仓库中 仓库也会同步到服务器 这样我们就无需自己手动去同步项目代码 选择 我们的项目大都托管在Github 当然Github在建立私有仓库时是需要付费的 那么在自己的网站部署时 我们完全可以选择其他的平台 目前来说我们需要试下Webhooks自动化部署 那么我们可以选择自己合适的平台 这里我选择GitLab 当然国内的话还可以选择 Coding 和 码云 这些在国内都是不错的代码托管平台 你都可以创建自己的私有项目仓库 之前我的网站项目是托管在Coding 但是近期我重写我的网站项目 因为GitLab本身可以建立自己的私有仓库并且没有限制 同样的也可以设置成员的权限 分支的工作流也十分清晰强大 现在很多的公司也都选择了GitLab so 我还是会去尝试一下新的服务 前期准备 和Github一样 我们都要去生成一个ssh key这样我们以后在提交项目和项目分支时就可以免去身份验证 在本地终端执行 $ ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 这里填写你GitLLab注册的邮箱即可(最好保持一致) 你可以一直默认下去 当然为了和之前的发生冲突 你可以自己在生成的时候重新命名 我这里生成的是gitlab_rsa和gitlab_rsa.pub 接着在命令行执行(添加刚生成的公钥和私钥) eval $(ssh-agent -s) ssh-add ~/.ssh/gitlab_rsa 查看我们的公钥内容 cat ~/.ssh/gitlab_rsa.pub 拿到我们的公钥内容 我们就可以去GitLab添加我们的sh key 添加完毕之后当然是测试本地连接 $ ssh -T git@gitlab.com 如果没有问题的话 会回馈给我们正确的欢迎信息 具体的生成信息可以看官方的 ssh文档 我们在此之前在GitLab已经上传了我们的项目 在项目中的Setting-&gt;Integrations里添加脚本钩子 比如这边我添加的url是http://kobeman.com/hook/index.php 然后填入的token是ispace 现在可以去服务器的站点目录下克隆远程仓库的项目 这里我的站点目录是/data/www 那么在这个目录下克隆我们远程的项目 克隆完毕后 当然这里以Laravel项目为例 完成一些权限 这些可自行查阅 能够成功跑起来我们的项目就ok 这里我访问的网址是www.kobeman.com 下面就需要添加钩子文件 这里我们项目的根目录 这里可以是public目录下新建hook目录 添加一个index.php 具体内容如下 具体代码我已经放在我的gist上 如果有什么问题欢迎提出issue &lt;?php error_reporting(1); $target = '/data/www/ISpace'; // 生产环境web目录 $token = 'ispace'; //GitLab 添加的token $wwwUser = 'root'; $wwwGroup = 'root'; if (empty($_SERVER['HTTP_X_GITLAB_TOKEN']) || $_SERVER['HTTP_X_GITLAB_TOKEN'] !== $token) { exit('error request'); } /*if($_SERVER['HTTP_X_GITLAB_TOKEN'] == $token){ echo &quot;校验成功&quot;; }*/ //$repo = $json['repository']['name']; // $cmds = array( // &quot;cd $target &amp;&amp; git pull&quot;, // &quot;chown -R {$wwwUser}:{$wwwGroup} $target/&quot;, // ); // foreach ($cmds as $cmd) { // shell_exec($cmd); // } chdir($target); $cmd = &quot;sudo -Hu root git pull&quot;; shell_exec($cmd); 这里的钩子文件需要注意以下几点 php配置里需要注释掉shell_exec这些被禁用的函数 详见shell_exec 执行命令时切换到管理员用户最好 查看是否进入你的项目目录 这里我是通过chdir进入项目目录 因为cd命令是没有用的 详见Can't 'cd' with PHP shell_exec() 这里为什么我们需要获取这个$_SERVER的参数呢 因为GitLab是通过post请求这个地址 所以为了验证之前填入的token 我们这里是去验证他的请求头部 因为他是以请求头部传递给我们的 如图所示 为了了解他的头部到底包含了什么信息 我们可以都打印出来看下 所以说如果我们验证这个token成功的话再去进入到项目目录 执行git pull拉取我们最新的代码 这样也就实现了 自动化的代码部署 在验证过程中我打印了下这个 $_SERVER['HTTP_X_GITLAB_TOKEN'] 这样一来下次再去提交我们的最新的功能代码时就可以哦同步到我们的服务器 相关链接 Gist 地址 GitLab 官网 Coding 官网 码云 官网 GitLab key生成 ","link":"https://GeekGhc.github.io/post/gitlab-webhooks-shi-xian-zi-dong-hua-fu-wu-qi-bu-shu/"},{"title":"API文档编写-apidocJs","content":"在后台编写api文档时 我们如果希望自己的文档注释就可以自动解析成接口文档 那么改完注释代码 接口文档也会更新 这样就不十分方便 简介 在开发后台 需要api的编写 那么在提供给web端和移动客户端的开发者时需要给他们提供必要的api文档 那么今天就来介绍 一个文档编写工具 apidocjs 整个语法还是遵循markdown的语法 当然apidoc支持Grunt，主页 https://github.com/apidoc/grunt-apidoc 其实这样类似的工具还有很多 但目的只有一个就是提供给其他开发者更好的说明 所以说文档的编写和规范都是十分重要的 安装 在命令行全局安装 $ npm install apidoc -g apidoc支持Grunt，主页https://github.com/apidoc/grunt-apidoc 在项目中可以使用npm install grunt-apidoc --save-dev安装 添加grunt.loadNpmTasks('grunt-apidoc')到Gruntfile.js 添加grunt task 这里面包含了输出目录等信息 apidoc: { myapp: { src: &quot;app/&quot;, dest: &quot;apidoc/&quot; } } module.exports = function(grunt) { grunt.config.set('clean', { apidoc: { myapp: { src: &quot;app/&quot;, dest: &quot;apidoc/&quot; } } }); grunt.loadNpmTasks('grunt-apidoc'); }; 安装完毕之后可以查看一下命令 $ apidoc -h 下面会看到一些参数 这里简单介绍几个 标题 地址 -o 指定文档的输出目录 -i 输入文件夹 这里包含了 -t 指定输出文件的模板 -c 指定配置文件的文件目录 接下来就是配置文件appidoc.json的定义 实例如下 { &quot;name&quot; : &quot;codespace&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;title&quot;: &quot;codespace&quot;, &quot;description&quot;: test project&quot; } 当然配置文件的内容放入package.json也是可以的(相同的字段就和package.json一样 而不一样的就放在apidoc下) 比如正在终端执行 $ npm init 填写你的项目信息即可 最后别忘了加上apidoc这个配置项 { &quot;name&quot;: &quot;codespace&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;description&quot;: &quot;test for codespace&quot;, &quot;main&quot;: &quot;index.js&quot;, &quot;scripts&quot;: { &quot;test&quot;: &quot;echo \\&quot;Error: no test specified\\&quot; &amp;&amp; exit 1&quot; }, &quot;keywords&quot;: [ &quot;api&quot; ], &quot;author&quot;: &quot;jellybean&quot;, &quot;license&quot;: &quot;MIT&quot;, &quot;apidoc&quot;:{ &quot;title&quot;:&quot;codespace&quot; } } 当然为了生成最后的文档文件 我们还需要生成我们的代码文件 当然在实际项目中可以新建一个文档的文件 我们在myapp文件目录下新建生成doc.php 具体内容如下(文档的语法格式在接下来会介绍) /** * @api {get} /user/:id Request User information * @apiName GetUser * @apiGroup User * * @apiParam {Number} id Users unique ID. * * @apiSuccess {String} firstname Firstname of the User. * @apiSuccess {String} lastname Lastname of the User. * * @apiSuccessExample Success-Response: * HTTP/1.1 200 OK * { * &quot;firstname&quot;: &quot;John&quot;, * &quot;lastname&quot;: &quot;Doe&quot; * } * * @apiError UserNotFound The id of the User was not found. * * @apiErrorExample Error-Response: * HTTP/1.1 404 Not Found * { * &quot;error&quot;: &quot;UserNotFound&quot; * } */ 来到myapp的上级目录(当然也可以在当前目录 看具体执行的命令) $ apidoc -i myapp/ -o apidoc/ 这样的话会在当前目录生成apidoc的文件目录 里面就包含了最后文档的样式和内容 这样的话我们可以将文档直接部署到服务器 或者一些托管平台了 对于文档的语法当然具体的还是看 官方文档 这里就先介绍几个 首先文档的代码块是以/**和*/开始和结束 这个从上面的事例就可以看出来 @api 定义接口的形式和地址 包括具体的请求类型和参数等 @api {method} path [title] @apiName 定义接口名 @apiGroup 定义接口所属组 因为接口可能会分类 比如书籍类接口和评论类接口 @apiDefine 定义一组接口返回实例 这和@apiUse对应起来用(你可以理解为在这声明了一个function) 然后再需要的地方再使用 如我们现在定义一组错误提示 /** * @apiDefine UserNotFoundError * * @apiError UserNotFound The id of the User was not found. * * @apiErrorExample Error-Response: * HTTP/1.1 404 Not Found * { * &quot;error&quot;: &quot;UserNotFound&quot; * } */ 这样的话我们可能在某个接口部分会使用到差找不到用户这个错误返回时可以加上 @apiUse UserNotFoundError @apiHeader 定义了接口请求的头部信息 如 /** * @api {get} /user/:id * @apiHeader {String} access-key Users unique access-key. */ @apiParam 接口的请求参数 列举出接口的请求参数 类型 是否可选等(可选的话[]包含参数名) 如 @apiParam {Number} id Users unique ID. @apiParam {String} [firstname] Firstname of the User. @apiSuccess 接口成功返回的字段信息 包括接口类型 描述 @apiSuccess {String} firstname Firstname of the User. @apiSuccess {String} lastname Lastname of the User. @apiSuccessExample Success-Response: 接口请求成功返回的形式事例 如 HTTP/1.1 200 OK { &quot;firstname&quot;: &quot;John&quot;, &quot;lastname&quot;: &quot;Doe&quot; } @apiError 定义接口的错误信息 包含错误类型和描述 @apiErrorExample Error-Response: 定义接口错误返回实例 如 HTTP/1.1 404 Not Found { &quot;error&quot;: &quot;UserNotFound&quot; } 事例效果图 ","link":"https://GeekGhc.github.io/post/api-wen-dang-bian-xie-apidocjs/"},{"title":"Laravel Passport 之API授权","content":"我们常常会遇到多账号系统的认证这样的一个应用场景 那么在Laravel中 我们常常会用Passport获取Token来进行密码验证 在Laravel5.3时Taylor就发布了passport的package 用于开发 OAuth 服务端 可以作为Auth验证 其实为什么最近又在整理这个呢 因为我做安卓也有近两个月的时间 期间也接触了不少的客户端的开发 此时我发现再去 研究下laravel的passport的API认证真的会深有体会 之前在写服务端的API只是集成了Dingo和jwt认证 而对于与多账号系统的认证体会不深 所以这几天想写点下来 也会自己以写安卓客户端做更好的服务和认证 这里新建一个项目 以5.3为例 新建完项目之后 开始引入我们的 passport 现在的话应该已经是3.1的版本 可以在composer.json里添加: &quot;laravel/passport&quot;: &quot;^3.0&quot; 在终端执行: $ composer update 在 app/config/app.php 添加服务: Laravel\\Passport\\PassportServiceProvider::class, 当然还需要安装项目依赖(也可以适应yarn): $ npm install Passport 使用服务提供者注册内部的数据库迁移脚本目录，所以上一步完成后，你需要更新你的数据库结构。 Passport 的迁移脚本会自动创建应用程序需要的客户端数据表和令牌数据表: $ php artisam migrate 接下来我们需要运行passport:install 命令来创建生成安全访问令牌时用到的加密密钥 当然这条命令也会创建「私人访问」客户端和「密码授权」客户端(这些解释文档上解释的也很清楚) $ php artisan passport:install 具体表现在oauth_clients 这张表会新增两条数据 执行完毕后在User Model了添加HasApiTokens 这个 Trait 这会为我们提供一些辅助函数 class User extends Authenticatable { use HasApiTokens,Notifiable; } 接着我们在 AuthServiceProvider 服务中添加路由方法(函数会注册一些发放令牌等一些必要的路由) public function boot() { $this-&gt;registerPolicies(); Passport::routes(); } 需要的话可以执行php artisan route:list这些路由在之后也会用到 最后还得需要在config/auth.php中将api的驱动配置为passport 这样的话我们的api请求时会使用 Passport的TokenGuard 'guards' =&gt; [ 'web' =&gt; [ 'driver' =&gt; 'session', 'provider' =&gt; 'users', ], 'api' =&gt; [ 'driver' =&gt; 'passport', 'provider' =&gt; 'users', ], ] 我们可以注册一个用户接下来 可以使用Laravel自带的 auth登录注册 $ php artisan make:auth 这样访问首页 去注册一个账号 为了使用这些api 我们可能需要一些前端代码 当然在passport中也预定了一些vue组件 我们如果嫌麻烦直接使用他所提供的 $ php artisan vendor:publish --tag=passport-components 使用这些组件时 我们需要去注册这些组件 在app.js中添加给我们的组件 Vue.component( 'passport-clients', require('./components/passport/Clients.vue') ); Vue.component( 'passport-authorized-clients', require('./components/passport/AuthorizedClients.vue') ); Vue.component( 'passport-personal-access-tokens', require('./components/passport/PersonalAccessTokens.vue') ); 接下来就是去模板中使用客户端的访问和私人令牌 &lt;passport-clients&gt;&lt;/passport-clients&gt; &lt;passport-authorized-clients&gt;&lt;/passport-authorized-clients&gt; &lt;passport-personal-access-tokens&gt;&lt;/passport-personal-access-tokens&gt; 我们会看到创建客户端和 assess token 我们可以去创建一个客户端 因为我们的数据要为很多的客户端服务 如果一个客户端需要访问我们的数据我们就可以为他创建一个客户端 其实这样的应用场景可以想象的就是 当我们比如安卓的app需要使用服务端的数据 可能多个app会使用到同一组数据 那么为了辨识 我们就需要为每个客户端创建一个client 并提供一个回调地址 如果做过第三方授权的就知道了 无论我们是做分享还是授权登陆 我们需要为我们这个客户端提供appID,回调地址 所以这里我们可以先去创建一个客户端 应户名就为codespace 回调url为:http://laravel-passport.dev/callback 创建完成之后就会给出我们的clientId和clientSecret这些是不是很熟悉 其实我们的服务端为我们生成的也就是客户端所需要的这些id和secret 而这个在数据库中的表现就是oauth_clients表中 生成了对应的客户端数据(也就是我们刚刚所创建的) 接下来就是模拟客户端去发起请求 这里我新建了另一个项目passport-client 进入刚创建的项目 因为之前我们已经创建了一个客户端 那么开发者会使用此客户端的 ID 和密钥向你的应用程序请求一个授权码和访问令牌。 首先,接入应用会将用户重定向到你应用程序的 /oauth/authorize 路由上 在刚创建项目的路由添加 Route::get('/redirect', function () { $query = http_build_query([ 'client_id' =&gt; 'client-id', 'redirect_uri' =&gt; 'http://example.com/callback', 'response_type' =&gt; 'code', 'scope' =&gt; '', ]); return redirect('http://your-app.com/oauth/authorize?'.$query); }); 正如我们所看到的当客户端接受到授权请求你时 会显示默认页面 用户可以取消或者允许 用户确认之后才能重定向到指定的 resirect_uri这和我们平时手机app授权登录是一样的流程 只有用户允许授权了 才会跳转到相应页面 当用户允许操作之后会跳转到对应的redirect_uri那么我们定义的是一个callback 所以我们会增加一个路由 Route::get('callback','OAuthController@oauth'); 当然我们还是要去实现这样的回调方法 public function oauth(Request $request) { $http = new GuzzleHttp\\Client; $response = $http-&gt;post('http://your-app.com/oauth/token', [ 'form_params' =&gt; [ 'grant_type' =&gt; 'authorization_code', 'client_id' =&gt; 'client-id', 'client_secret' =&gt; 'client-secret', 'redirect_uri' =&gt; 'http://example.com/callback', 'code' =&gt; $request-&gt;code, ], ]); return json_decode((string) $response-&gt;getBody(), true); } 相关代码对照着文档看应该没什么问题 值得注意的是上面的客户端和服务端的访问地址根据每个人所定义的地址 在这里我们需要修改的是我们客户端 拿到的client_id和client_secret这是我们开发者在开发客户端都可以拿得到的 填写上我们之前创建客户端所生成的值就行 既然有了这些信息 客户端就可以发起请求 我们访问oauth这个路由 这样应该会跳转到服务端的授权页面 这个过程和我们平时的无论是web端还是移动端的授权是一样的 也就是不同的客户端拿到指定的客户端key之后访问到的服务端的内容 这个时候我们点击授权的话就会跳转到callback这个url这里 也就相当于一个回调地址 我们已经在callback写了我们客户端的处理内容 如果执行到的话我们应该会得到一个access_token 当然授权之后我们也可以拿到一个refresh_token 我们有了这个access_token 就可以去请求我们的api数据了 还记得一开始我们就修改了我们api的driver为passport 所以我们理所当然的得用这个access_token去请求我们需要的数据 其中在api.php这个路由文件中已经默认为我们创建了一个关于用户的api 简单来说就去拿到我们之前注册的用户数据好了 所以在用postman去访问这个http:///your-server.com/api/user时 一开始是访问不到的 只有经过之前的授权认证 拿到我们的额access_token 这个时候加上这个参数再去访问这个url时我们就可以成功访问到了用户数据 总结 其实整个的过程就是我们所熟悉的oauth认证过程 服务端创建一个客户端数据 这也就是对应着我们去一些开发平台盛情得到我们的 ID和key等相关信息 这样服务端会存储这个客户端的信息 我们在不同的客户端当然对应着不同的客户端id 我们在客户端如app或者网站去发起客户端的请求 当然这些请求会附带上我们申请得到的ID和key这样服务端平台校验正确后会返回一个授权页面 这时候用户可以选择授权或者取消 试想一下我们平时在网站或者app利用第三方登录时的场景就知道了 当用户给予授权之后 会执行到客户端的回调地址 在这个回调函数中我们会处理自己的逻辑 比如存储用户的基本信息等 当然也会拿到服务端给我们的access_token和refresh_token这也就是我们平常所说的令牌 拿到令牌之后我们就可以对服务端的一些数据进行操作 比如获取一个用户的信息 用户的相关资料等 如果没有这个token的话是无法访问的 因为我们之前配置的就是api的driver为我们的passport 所以相应的访问数据我们是需要拿到这个token的 相关链接 Laravel Passport GitHub地址 Laravel Passport 官方文档 Laravel Passport 中文文档 ","link":"https://GeekGhc.github.io/post/laravel-passport-zhi-api-shou-quan/"},{"title":"Laravel5.5新特性之Blade::if 自定义指令","content":"在Laravel5.5中支持了简化视图if的自定义的指令 Laravel在下个月才能发布 不过不影响我们对新特性的期待 再此之前说了两个新的特性 现在再来啊介绍一个新的特性 就是在5.5中支持了自定义标签 这样就简化了之前在View中的if标签 那么问题来了 首先我们应该还是想到的是这个新特性的应用场景是什么 举个栗子来说 当我们需要判断是否是管理员还是普通用户 那么视图的展现会有所区分的 通常来说我们可能在视图中这样去写 @if(auth()-&gt;check() &amp;&amp; auth()-&gt;user()-&gt;isAdmin()) 管理员 @else 普通用户 @endif 其实这样的判断逻辑就是用户是否登录并且该用户是不是管理员身份 如果两者都符合 那么才能判断该用户是可以有管理员权限的 isAdmin()这个function 可以在model定义 用户判断用户到底属于什么身份 其实在5.5中我们有更好的解决方案 在 AppServiceProvider::boot() 方法中 use Illuminate\\Support\\Facades\\Blade; Blade::if('isAdmin', function () { return auth()-&gt;check() &amp;&amp; auth()-&gt;user()-&gt;isAdmin(); }); 这样我们就把之前在视图if中的判断逻辑搬到这边来了 那么我们现在在视图中完全可以这样写: @isAdmin 管理员 @else 普通用户 @endIsAdmin 这样我们就完成了自定义的标签 当然其实还有这样的一个应用场景就是 线上和线下的生产环境 也就是我们通常所说的开发环境和应用环境 这在Laravel5.5中更为简化了 Blade::if('prod', function () { return app()-&gt;environment('production'); }); 当然还可以传递参数这样检查更为可控 Blade::if('env', function ($env) { return app()-&gt;environment($env); }); 相关具体内容请查看 https://laravel-news.com/bladeif ","link":"https://GeekGhc.github.io/post/laravel55-xin-te-xing-zhi-bladeif-zi-ding-yi-zhi-ling/"},{"title":"Laravel5.5新特性之邮件模板浏览器查看","content":"在Laravel5.5中支持查看我们的HTML邮件模板 Laravel之父Taylor今天再推特上发布了Laravel5.5发布时间改为8月份 其实我想说 不发布Laravel5.5我就不写代码啦😄 不过话说回来 我们还是谈谈带给我们的新特性 其中就是我们可以在浏览器中呈现我们的邮件模板 例如我们去创建一个用户欢迎的邮件模板 在终端执行: $ php artisan make:mail UserWelcome --markdown=emails.user.welcome 这样我们就在resources.views.emails.user目录下就可以生成邮件模板类UserWelcome 为了查看浏览器下的浏览效果我们可以在路由中这样去定义 Route::get('/demo', function () { return new App\\Mail\\UserWelcome(); }); 下面就是浏览器显示的邮件模板视图 这个新特性将会在Laravel5.5中推出 值得期待 想要了解更多的Laravel资讯 可以关注Laravel News ","link":"https://GeekGhc.github.io/post/laravel55-xin-te-xing-zhi-you-jian-mo-ban-liu-lan-qi-cha-kan/"},{"title":"Linux MySQL定时备份并上传到git仓库","content":"在Linux执行定时任务 很多都已经很熟悉 那么在我们可能部署一个小的项目时 对于mysql的数据 我们会需要定时备份 这就需要我们的定时任务 简介 我们在部署我们的中小型项目时 在数据存储 我们通常选择mysql作为我们的存储工具 那么对于一个大的项目来说 每天的数据量是十分大的 对于每天产生的数据 如果哪一天我们的网站或者服务器受到攻击 我们的数据丢失是个很爆炸的事情 所以说自然这设计到数据库的备份 那么怎样的备份是我们想要的呢 对于备份的数据文件我们可能会存放在服务器目录 备份周期的话当然是按照数据量来说的 这里我们一般都是每天的凌晨备份一次 备份后的文件存放在我们的服务器的目录下面 但是万一有一天服务器也崩溃了 那么备份的文件也就没了 所以我们设想一个好的方案就是数据库每天备份 每次备份自动提交到远程仓库 这里我以码云为例 码云 首先建立好远程仓库 在这里我选择了码云 新建一个私有仓库 当然为了每次可以免密码提交文件 在服务器里可以生成ssh key 具体的可以看我的以前的一篇博客 服务器新建备份 在服务器为了存储备份后的文件 新建一个备份目录 $ mkdir /bak 进入该目录后 继续新建两个文件夹 mysqlBak和shDir 一个是放脚本文件 一个是放具体备份后的文件 下面我们可以去新建脚本了 进入shDir目录后 执行 $ vim mysqlBak.sh 具体的代码如下: #!bin/sh ###################数据库配置信息####################### createAt=`date +%Y-%m-%d-%H:%M:%S` user=root passwd=ghc1996 dbname=ispace mysql_back_path=/bak/mysqlBak ###################执行命令####################### mysqldump -u $user -p$passwd $dbname &gt; $mysql_back_path/$createAt.sql cd /bak/mysqlBak /usr/local/git/bin/git add . /usr/local/git/bin/git commit -m $createAt /usr/local/git/bin/git push 这里只是一个简单的脚本 我想了解linux的很容易看的懂 执行的就是备份数据库并push到远程仓库 那么既然是脚本 我们需要指明什么时候执行这个脚本 指定脚本执行 $ crontab -e 我们希望是每天的凌晨执行一次备份 并添加到远程仓库 那么添加 $ 0 0 * * * /bin/sh /bak/shDir/mysqlbak.sh 对linux的crontab指定的时间只有五个部分 段位 时间 第一段 代表分钟 0—59 第二段 代表小时 0—23 第三段 代表日期 1—31 第四段 代表月份 1—12 第五段 代表星期几，0代表星期日 0—6 使用命令 crontab -e 然后直接编辑定时脚本。 时间 +具体的名字 举个列子来说就是 0 0,3,7,9,12,15,18,21,23 * * * /bin/sh /bak/shell/mysqlBak.sh 这样的话就是我每天0,,3,7,9,12,15,18,21,23点时会去执行这个脚本文件 那么这就实现了基本的数据库的备份 执行定时任务 $ crontab -l 如果服务没有启动 那么重新启动定时任务 $ systemctl restart crond 那么现在这个定时任务就已经启动了 对于提交远程仓库前提是在服务器生成ssh key并添加到码云 这在上面也提到过 对于需要提交文件的目录初始化git目录就可以了 这样局可以构成了我们需要的本分任务 当然过程中可能会遇到一些问题 我在下面的相关链接都已经罗列出来了 这样一来我们就可以实现了每天的凌晨备份我们的数据库 并同时提交到我们的码云这个远程仓库 这也是我们想要的效果 我也说过备份的周期视我们的项目的数据量的大小而定 对于每个框架都有自己的备份机制 我这里所写的是我们自己实现的一个通用的备份机制 相关链接 crontab验证 oschina码云GIT免登陆用 码云平台帮助文档 CentOS下使用crontab命令来定时执行任务 CentOS Linux下每天自动备份MySQL数据库 ","link":"https://GeekGhc.github.io/post/linux-mysql-ding-shi-bei-fen-bing-shang-chuan-dao-git-cang-ku/"},{"title":"HTMLPurifier解决Laravel5的XSS跨站脚本攻击安全问题","content":"Web安全问题越来越受到人们的注意 对于XXS是对于站点的用户隐私的攻击 这对于用户安全造成很大的隐患 就目前来说HTMLPurifier是目前最好的PHP富文本HTML过滤器 XXS 也成为跨站脚本攻击 这也是常见的Web攻击 同时XXS涉及三个群体:黑客 客户端 Web站点。就目前来说解决 php中XXS攻击的方法就是使用HTML Purifier 基于他支持自定义过滤规则 可以把不标准的HTML转换为标准的HTML 同时我们也要相信一点的就是没有绝对的安全 所以我们也只能尽量的去过滤一些不必要的安全隐患 对于他的自定义规则就是对HTML的标签和属性的的过滤 利用白名单机制 在执行clean()方法后 对于不在白名单的 元素则会被过滤 下面就来介绍我们需要用到的 HTMLPurifier for Laravel 在已经创建好的Laravel项目中 执行: $composer require mews/purifier 在 config/app.php 文件的 providers 数组里添加 Mews\\Purifier\\PurifierServiceProvider::class, 生成 HTMLPurifier for Laravel 5 的配置文件 在命令行下执行: $ php artisan vendor:publish --provider=&quot;Mews\\Purifier\\PurifierServiceProvider&quot; 这个时候打开config/purifier.php可以看到一系列的配置 return [ 'encoding' =&gt; 'UTF-8', 'finalize' =&gt; true, 'cachePath' =&gt; storage_path('app/purifier'), 'cacheFileMode' =&gt; 0755, 'settings' =&gt; [ 'default' =&gt; [ 'HTML.Doctype' =&gt; 'XHTML 1.0 Strict', 'HTML.Allowed' =&gt; 'div,b,strong,i,em,a[href|title],ul,ol,li,p[style],br,span[style],img[width|height|alt|src]', 'CSS.AllowedProperties' =&gt; 'font,font-size,font-weight,font-style,font-family,text-decoration,padding-left,color,background-color,text-align', 'AutoFormat.AutoParagraph' =&gt; true, 'AutoFormat.RemoveEmpty' =&gt; true, ], 'test' =&gt; [ 'Attr.EnableID' =&gt; true ], &quot;youtube&quot; =&gt; [ &quot;HTML.SafeIframe&quot; =&gt; 'true', &quot;URI.SafeIframeRegexp&quot; =&gt; &quot;%^(http://|https://|//)(www.youtube.com/embed/|player.vimeo.com/video/)%&quot;, ], ], ]; 接下来对于接受到的参数 我们就可以使用clean(Input::get('name'))进行过滤了 当然这里执行的过滤规则是配置文件里setting的default的配置规则 代码如下: 'default' =&gt; [ 'HTML.Doctype' =&gt; 'XHTML 1.0 Strict', 'HTML.Allowed' =&gt; 'div,b,strong,i,em,a[href|title],ul,ol,li,p[style],br,span[style],img[width|height|alt|src]', 'CSS.AllowedProperties' =&gt; 'font,font-size,font-weight,font-style,font-family,text-decoration,padding-left,color,background-color,text-align', 'AutoFormat.AutoParagraph' =&gt; true, 'AutoFormat.RemoveEmpty' =&gt; true, ], 当然我们完全可以自定义过滤规则 如我们定义规则: 'post_topic' =&gt; array( 'HTML.Doctype' =&gt; 'XHTML 1.0 Strict', 'HTML.Allowed' =&gt; 'div,b,strong,i,em,a[href|title],ul,ol,li,p[style],br,span[style],img[width|height|alt|src],pre,code', 'CSS.AllowedProperties' =&gt; 'font,font-size,font-weight,font-style,font-family,text-decoration,padding-left,color,background-color,text-align', 'AutoFormat.AutoParagraph' =&gt; true, 'AutoFormat.RemoveEmpty' =&gt; true, ), 那么我们就可以在执行过滤是介入第二个参数 clean(Input('name'),'post_topic') 最后还是那句话 web安全的工作终究不是最完美的 我们需要不断的改进我们的防御机制已达到预期的效果 相关链接 HtmlPurifier官网 Purifier-for-laravel 跨站点脚本攻击深入解析 Laravel-China 使用 HTMLPurifier来解决Laravel5中的 XSS 跨站脚本攻击安全问题 ","link":"https://GeekGhc.github.io/post/htmlpurifier-jie-jue-laravel5-de-xss-kua-zhan-jiao-ben-gong-ji-an-quan-wen-ti/"},{"title":"Mint-UI结合laravel实现内容加载更多","content":"很多时候我们会遇到就是点击或者当页面滑到底部加载数据 因为一次性加载数据不仅对内存响应造成一定的影响 当然这样的加载更多的方式给用户的体验也会比较好 简介 我们经常会遇到这样的一个场景就是对于一些消息和列表的显示 因为数据太多如果一次性显示出来的话无论是对内存还网站的响应时间 都会造成一定的影响 所以很多时候我们的解决方案是一次性加载固定条数的数据显示 可以给用户一个加载更多的链接或者当用户滑动到页面 底部时继续加载 这样的话一个页面不会充斥太多内容 用户的体验也会不错 那么对于实现这样的按需加载 我在最近处理的一个消息显示时就以Mint-UI结合laravel来实现了数据的按需加载 最后的实现效果就是 当页面滑动到底部指定距离时再去加载数据 下载第三方包 1.下载第三方依赖包(Vue 2.0) $ npm install mint-ui -S 2.引用组件 在resource/assets/js/app.js里引用 //引入mint-ui import MintUI from 'mint-ui' import 'mint-ui/lib/style.css' Vue.use(MintUI); 3.引入axios以实现api请求 $ npm install --save axios vue-axios 然后注入axios依赖包 import axios from 'axios' import VueAxios from 'vue-axios' Vue.use(VueAxios, axios) 在Vue组件里使用 对于现实消息我这里以一个评论展示为例 所以在resource/assets/js/components目录下新建Comment.Vue文件 这里当然根据每个人需要的而现实消息来定 当然在实现下拉加载时 我们需要用到的是Mint-UI的InfiniteScroll 按着官方给出的demo实例我们在组件的Vue文件中引入进来 import { InfiniteScroll } from 'mint-ui'; 其中在列表显示时我们要去定义他们的父标签 在这个父标签中我们有几个方法需要声明 v-infinite-scroll 记载时会触发的方法 infinite-scroll-disabled 记录是否要触发加载函数 infinite-scroll-distance 指定多少距离触发加载方法 这里我先给出我项目里的评论的展示组件Commetn.Vue &lt;div class=&quot;comments-list&quot;&gt; &lt;ul class=&quot;lists&quot; v-infinite-scroll=&quot;loadMore&quot; infinite-scroll-disabled=&quot;loading&quot; infinite-scroll-distance=&quot;10&quot; v-if=&quot;comments&quot;&gt; &lt;li class=&quot;item&quot; v-for=&quot;comment in comments&quot;&gt; &lt;div class=&quot;user-avatar&quot;&gt; &lt;img :src=&quot;comment.head&quot;&gt; &lt;/div&gt; &lt;div class=&quot;comment-info&quot;&gt; &lt;div class=&quot;head-info&quot;&gt; &lt;span class=&quot;name&quot;&gt;{{comment.nickname}}&lt;/span&gt; &lt;span class=&quot;post-time&quot;&gt;{{comment.release_time}}&lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;content-title&quot;&gt; &lt;p&gt;{{comment.title}}&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;content-text&quot;&gt; &lt;p v-html=&quot;comment.content&quot;&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;loading-div&quot; v-if=&quot;is_loading&quot;&gt; &lt;div class=&quot;loading&quot;&gt; &lt;div class=&quot;loader&quot; title=&quot;5&quot;&gt; &lt;svg version=&quot;1.1&quot; id=&quot;Layer_1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot; x=&quot;0px&quot; y=&quot;0px&quot; width=&quot;24px&quot; height=&quot;30px&quot; viewBox=&quot;0 0 24 30&quot; style=&quot;enable-background:new 0 0 50 50;&quot; xml:space=&quot;preserve&quot;&gt; &lt;rect x=&quot;0&quot; y=&quot;13&quot; width=&quot;4&quot; height=&quot;5&quot; fill=&quot;#333&quot;&gt; &lt;animate attributeName=&quot;height&quot; attributeType=&quot;XML&quot; values=&quot;5;21;5&quot; begin=&quot;0s&quot; dur=&quot;0.6s&quot; repeatCount=&quot;indefinite&quot; /&gt; &lt;animate attributeName=&quot;y&quot; attributeType=&quot;XML&quot; values=&quot;13; 5; 13&quot; begin=&quot;0s&quot; dur=&quot;0.6s&quot; repeatCount=&quot;indefinite&quot; /&gt; &lt;/rect&gt; &lt;rect x=&quot;10&quot; y=&quot;13&quot; width=&quot;4&quot; height=&quot;5&quot; fill=&quot;#333&quot;&gt; &lt;animate attributeName=&quot;height&quot; attributeType=&quot;XML&quot; values=&quot;5;21;5&quot; begin=&quot;0.15s&quot; dur=&quot;0.6s&quot; repeatCount=&quot;indefinite&quot; /&gt; &lt;animate attributeName=&quot;y&quot; attributeType=&quot;XML&quot; values=&quot;13; 5; 13&quot; begin=&quot;0.15s&quot; dur=&quot;0.6s&quot; repeatCount=&quot;indefinite&quot; /&gt; &lt;/rect&gt; &lt;rect x=&quot;20&quot; y=&quot;13&quot; width=&quot;4&quot; height=&quot;5&quot; fill=&quot;#333&quot;&gt; &lt;animate attributeName=&quot;height&quot; attributeType=&quot;XML&quot; values=&quot;5;21;5&quot; begin=&quot;0.3s&quot; dur=&quot;0.6s&quot; repeatCount=&quot;indefinite&quot; /&gt; &lt;animate attributeName=&quot;y&quot; attributeType=&quot;XML&quot; values=&quot;13; 5; 13&quot; begin=&quot;0.3s&quot; dur=&quot;0.6s&quot; repeatCount=&quot;indefinite&quot; /&gt; &lt;/rect&gt; &lt;/svg&gt; &lt;/div&gt; &lt;div class=&quot;loading-text&quot;&gt;加载中。。。&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 还有其相应的加载方法 &lt;script&gt; import { InfiniteScroll } from 'mint-ui'; export default{ props:['post_num'], data(){ return{ comments:[], is_loading:true, count:1, is_finished:false } }, components:{ }, created(){ this.initData() }, methods:{ loadMore() { this.is_loading = true if(!this.is_finished){//如果还有数据可以加载 setTimeout(() =&gt; { Vue.axios.get('/api/postl?bookNum='+this.post_num+'&amp;&amp;count='+this.count).then(response =&gt; { if(response.data.code){ if(response.data.comments.length){ let last = response.data.comments.length for(let i = 0;i&lt;last;i++){ this.comments.push(response.data.comments[i]) } console.log(this.comments.length+&quot; newData = &quot;+this.count) this.count++ }else{ this.is_finished = true } }else{ console.log(&quot;error....&quot;) } }) }, 1000); }else{ this.is_loading = false } }, initData(){ Vue.axios.get('/api/post?postNum='+this.post_num).then(response =&gt; { if(response.data.code){ this.comments = response.data.comments console.log(this.comments.length+&quot; data = &quot;+response.data.code) }else{ console.log(&quot;error....&quot;) } }) } } } &lt;/script&gt; 这里的loadMore方法只是在1秒后触发加载数据 写完之后去app.js注册一下这个component Vue.component('post-comment', require('./components/Comment.vue')); 所以这时候在需要的视图文件里给出 &lt;post-comment post_num=&quot;{{$post-&gt;post_num}}&quot;&gt;&lt;/post-comment&gt; 完善api请求 在此之前我们已经写好了Vue组件 当然我们在触发的加载函数里去请求的后端请求我们可以在api.php里去完成 具体的api请求因业务需求不同而不同 这里就不再累赘 写完这些我们需要将之前的组件打包 这里因为我用的是5.4版本 Laravel以webpack替代了原先的gulp 所以在webpack.mix.js文件里声明 mix.browserSync('my-domain.dev'); mix.js('resources/assets/js/app.js', 'public/js') .sass('resources/assets/sass/app.scss', 'public/css') .version(); 还是执行npm run dev 打包之后在视图的文件里引如打包后的文件 &lt;script src=&quot;{{mix('js/app.js')}}&quot;&gt;&lt;/script&gt; 相关网址 Mint-UI Vuw-axios ","link":"https://GeekGhc.github.io/post/mint-ui-jie-he-laravel-shi-xian-nei-rong-jia-zai-geng-duo/"},{"title":"PHP中的正则表达式","content":"最近遇到公司的一个emoji表情替换的应用场景 于是我立马想到了这样的一个需求就可以用正则表达式来解决 只需要把相应的表情文本替换成表情的地址即可 简介 最近公司需要App端的网页端的页面分享 在书籍的评论区处 会有遇到Emoji表情的解析 当然这在移动端都是以图片的形式存储的 所以在网页端 自然我也需要对评论的内容找那个涉及到评论图片的地方进行解析 当然我的第一反应就是利用正则表达式去解析到 对应的表情代码 然后再去解析成图片的地址 相应的解析代码我会放到我的gist里 Emotion.php 开始 在数据库中用户的评论表情是以一定的文本格式存储的 比如[dog-00],[拜拜]这样的形式 那么我们需要对这些特定的格式进行解析成 相应的图片的地址 这样就可以显示出具体的评论内容 1.当然首先对于这些评论图片我们可以放在本地的目录 这里我以安卓端的文件夹目录放在/img/mipmap 2.对于评论的获取 这些我们可以在对应的控制器方法中去获取评论 这里给出一个事例: $comments = DB::table('posts') -&gt;where('book_num', $book-&gt;book_num) -&gt;join('user', 'user.id', '=', 'posts.uid') -&gt;orderBy('posts.release_time', 'desc') -&gt;get(); 这里只是一个对后台评论的获取 3.对于模板的评论内容的展示当然以我们的html形式展现 即{!! $comment-&gt;content !!} 关于解析的核心代码 为了功能的解耦 这里我将表情的解析放在一个Repositories目录下的Emotion.php里 在这个类文件里我写了一个替换方法 function replace_emotion($content) { preg_match_all('/\\[[\\s\\S]+?\\]/', $content, $arr); $emotions = array( &quot;[ej-01]&quot; =&gt; 'tieba_emotion_01', &quot;[ej-02]&quot; =&gt; 'tieba_emotion_02', &quot;[ej-03]&quot; =&gt; 'tieba_emotion_03', &quot;[ej-04]&quot; =&gt; 'tieba_emotion_04', &quot;[ej-05]&quot; =&gt; 'tieba_emotion_05', &quot;[ej-06]&quot; =&gt; 'tieba_emotion_06', &quot;[ej-07]&quot; =&gt; 'tieba_emotion_07', &quot;[ej-08]&quot; =&gt; 'tieba_emotion_08', &quot;[ej-09]&quot; =&gt; 'tieba_emotion_09', &quot;[ej-10]&quot; =&gt; 'tieba_emotion_10'); foreach ($arr[0] as $v) { foreach ($emotions as $key =&gt; $value) { if($v==$key){ $content = str_replace($v, '&lt;img src=&quot;/img/mipmap/' . $value . '.png&quot; width=&quot;24&quot; height=&quot;24&quot;/&gt;', $content); continue; } } } return $content; } 这里给出其中的一小段实例 这里就是利用正则表达式对类似[xxxx]这样的形式 解析成对应的图片路径 控制器方法依赖注入 对于写好的方法类 在构造函数里进行依赖注入就是了 protected $emotion; public function __construct(Emotion $emotion) { $this-&gt;emotion = $emotion; } 所以这样依赖对于获取的评论列表数据的内容都需要进行解析替换 因为返回的是一个collection so 我们通过map处理 $collections = $comments-&gt;map(function($item,$key){ $item-&gt;content = $this-&gt;emotion-&gt;replace_emotion($item-&gt;content); }); 这样一来在评论文本中如果有类似[ej-02]这样的形式就会解析成对应的表情图片 ","link":"https://GeekGhc.github.io/post/php-zhong-de-zheng-ze-biao-da-shi/"},{"title":"APP微信支付宝支付服务端接口","content":"作为App涉及到的支付业务 必然的也会需要服务端去操作相应的接口逻辑 当然服务端可以使用我们的laravel去完成这些接口的功能 简介 对于移动端开发来说 app集成支付服务对很多软件来说是必不可少的 其实整个过程描述起来就是在服务端用户在app上发起支付请求 在这个过程中看起来只是一个支付密码的确认的过程 但是这其中包括微信服务商 客户端 服务端的后台服务这些都经历了几个很重要的过程 而在服务端我们需要对请求进行操作 返回和处理相应的数据 当然这些数据可能是用户发起来的也有可能是微信或者支付宝回馈给我们的 选择 对于服务端api的请求 我们完全可以使用laravel是处理这些接口 其实很多公司的接口大都php编写 不仅是因为php操作数据的简单方便 还有就是如今php对于处理api有完整的开源库 就比如对于restful api我们可以在laravel项目里集成dingo Api 这里的内容在我的另一篇博客 Laravel API结合Dingo API和JWT 微信支付 准备工作 一开始当然是去微信的开发文档 了解下一些接口的具体参数信息 App开发者文档 对于整个流程交互 先来一张微信提供的交互时序图 交互流程 理解这个交互流程还是很重要的 之前也提到过 无论是微信支付还是支付宝支付 有几个流程是不可少的 1.用户在客户端APP选择商品 生成订单信息 选择微信支付方式 2.商户后台收到用户支付单，调用微信支付统一下单接口 参见统一下单API 3.统一下单接口返回正常的prepay_id，再按签名规范重新生成签名后，将数据传输给客户端APP。 参与签名的字段名为appId，partnerId，prepayId，nonceStr，timeStamp，package。 注意：package的值格式为Sign=WXPay 4.商户APP调起微信支付。api 参见app端开发步骤说明 5.商户后台接收支付通知。api参见 支付结果通知API 6.商户后台查询支付结果。参见查询订单API 7.商户后台回调支付结果等数据 参见支付结果通知 统一下单 这里包括应用ID、商户号、设备号等等，我们只需向其传输必填项即可，选填数据可以根据自己的实际需求来决定 其中appid 和 mch_id 分别去到微信开放平台和微信商户平台中获取，nonce_str (随机字符串) 很随意了，不长于32位就好 这里给出一个随机字符串的返回方法 private function getNonceStr() { $code = &quot;&quot;; for ($i=0; $i &gt; 10; $i++) { $code .= mt_rand(1000); //获取随机数 } $nonceStrTemp = md5($code); $nonce_str = mb_substr($nonceStrTemp, 5,37); //MD5加密后截取32位字符 return $nonce_str; } 当然这样的方法在无论在客户端和服务端都是可以的 但是就是一个随机字符串的方法在客户端生成更为方便 这里还有一个重要的参数 notify_url（通知地址）是接收微信支付异步通知回调地址， 通知url必须为直接可访问的url，不能携带参数。例如：'https://pay.weixin.qq.com/wxpay/pay.action' 这里我们在后面处理微信的回调接口就是访问的这个接口 接下来就是最为核心的步骤 签名 看完文档你会发现整个过程涉及到了3次签名 这里给出想应的文档和工具 签名算法 微信支付接口签名校验工具 一定要仔细根据签名算法的步骤 注意签名的小写 和一些排序要求 根据这个算法 这里给出具体的签名算法 /** * 获取参数签名； * @param Array 要传递的参数数组 * @return String 通过计算得到的签名 */ private function getSign($params) { ksort($params); //将参数数组按照参数名ASCII码从小到大排序 foreach ($params as $key =&gt; $item) { if (!empty($item)) { //剔除参数值为空的参数 $newArr[] = $key.'='.$item; // 整合新的参数数组 } } $stringA = implode(&quot;&amp;&quot;, $newArr); //使用 &amp; 符号连接参数 $stringSignTemp = $stringA.&quot;&amp;key=&quot;.$this-&gt;key; //拼接key // key是在商户平台API安全里自己设置的 $stringSignTemp = MD5($stringSignTemp); //将字符串进行MD5加密 $sign = strtoupper($stringSignTemp); //将所有字符转换为大写 return $sign; } 统一下单的所需要的数据都准备好了 接下来就是发起我们的请求了 当然因为与微信的数据请求包括返回都是xml格式的 所以我们需要去包装成xml格式 private function setSendData($data) { $this-&gt;sTpl = &quot;&lt;xml&gt; &lt;appid&gt;&lt;![CDATA[%s]]&gt;&lt;/appid&gt; &lt;body&gt;&lt;![CDATA[%s]]&gt;&lt;/body&gt; &lt;mch_id&gt;&lt;![CDATA[%s]]&gt;&lt;/mch_id&gt; &lt;nonce_str&gt;&lt;![CDATA[%s]]&gt;&lt;/nonce_str&gt; &lt;notify_url&gt;&lt;![CDATA[%s]]&gt;&lt;/notify_url&gt; &lt;out_trade_no&gt;&lt;![CDATA[%s]]&gt;&lt;/out_trade_no&gt; &lt;spbill_create_ip&gt;&lt;![CDATA[%s]]&gt;&lt;/spbill_create_ip&gt; &lt;total_fee&gt;&lt;![CDATA[%d]]&gt;&lt;/total_fee&gt; &lt;trade_type&gt;&lt;![CDATA[%s]]&gt;&lt;/trade_type&gt; &lt;sign&gt;&lt;![CDATA[%s]]&gt;&lt;/sign&gt; &lt;/xml&gt;&quot;; //xml数据模板 $nonce_str = $this-&gt;getNonceStr(); //调用随机字符串生成方法获取随机字符串 $data['appid'] = $this-&gt;appid; $data['mch_id'] = $this-&gt;mch_id; $data['nonce_str'] = $nonce_str; $data['notify_url'] = $this-&gt;notify_url; $data['trade_type'] = $this-&gt;trade_type; //将参与签名的数据保存到数组 // 注意：以上几个参数是追加到$data中的，$data中应该同时包含开发文档中要求必填的剔除sign以外的所有数据 $sign = $this-&gt;getSign($data); //获取签名 $data = sprintf($this-&gt;sTpl, $this-&gt;appid, $data['body'], $this-&gt;mch_id, $nonce_str, $this-&gt;notify_url, $data['out_trade_no'], $data['spbill_create_ip'], $data['total_fee'], $this-&gt;trade_type, $sign); //生成xml数据格式 return $data; } xml数据要使用&lt;![CDATA[]]&gt;注释包裹 对于包装好的数据我们可以生成相应的接口去发送请求 如果请求成功 微信会返回我们对应的信息 其中我们需要关注一个 prepay_id 该参数是微信生成的预支付回话标识，用于后续接口调用中使用，该值有效期为2小时 因为接下来我们要做的就是利用这个 prepay_id 再次进行签名然后返回给APP客户端 其中参与签名的字段有appId partnerId prepayId nonceStr timeStamp package 调用支付接口 因为在上一步中返回给我们这个 prepay_id 还有其他的返回数据 我们需要对数据进行解析 这样我们在确认返回成功之后再去对数据进行包装 $postObj = $encpt-&gt;xmlToObject($xml_data); //解析返回数据 if ($postObj === false) { echo 'failed'; exit; // 如果解析的结果为false，终止程序 } if ($postObj-&gt;return_code == 'FAIL') { echo $postObj-&gt;return_msg; // 如果微信返回错误码为FAIL，则代表请求失败，返回失败信息 } else { //如果上一次请求成功，那么我们将返回的数据重新拼装，进行第二次签名 $resignData = array( 'appid' =&gt; $postObj-&gt;appid, 'partnerId' =&gt; $postObj-&gt;mch_id, 'prepayId' =&gt; $postObj-&gt;prepay_id, 'nonceStr' =&gt; $postObj-&gt;nonce_str, 'timeStamp' =&gt; time(), 'package' =&gt; 'Sign=WXPay' ); //二次签名； $sign = $encpt-&gt;getClientPay($resignData); echo $sign; } 这里的xmlToObject是用来解析返回的xml对象的 具体代码如下 public function xmlToObject($xmlStr) { if (!is_string($xmlStr) || empty($xmlStr)) { return false; } $postObj = simplexml_load_string($xmlStr, 'SimpleXMLElement', LIBXML_NOCDATA); $postObj = json_decode(json_encode($postObj)); //将xml数据转换成对象返回 return $postObj; } 这里的我们只需要注意 mch_id 即为 partnerId 时间戳使用time()获取就好 package字段 暂填写固定值Sign=WXPay 在生成签名之后 我们就可以将sign，appId，partnerId，prepayId，nonceStr，timeStamp，package 这些参数一起返回给APP客户端 支付结果返回 支付请求发起后 微信端会触发我们在第一次填写的 notify_url 在这里我们就可以去完成对数据的相应逻辑 如更新订单和用户信息 当然值得注意的是我们还需要执行的就是验签 因为在这个结果返回的时候即想客户端APP返回支付结果 也会向商户后台服务器返回支付结果 我们需要做的就是获取并解析返回的结果 接着对我们的项目执行处理更新 最后再去同步返回给微信 首先我们先获取一下数据并解析成对象 /** * 接收支付结果通知参数 * @return Object 返回结果对象； */ public function getNotifyData() { $postXml = file_get_contents('php://input', 'r'); //接受通知参数； if (empty($postXml)) { return false; } $postObj = $this-&gt;xmlToObject($postXml); if ($postObj === false) { return false; } if (!empty($postObj-&gt;return_code)) { if ($postObj-&gt;return_code == 'FAIL') { return false; } } return $postObj; } 我们在微信回调接口里大致是这样: public function wxpayCallback(Request $request) { $obj = $this-&gt;wxPay-&gt;getNotifyData(); if ($obj-&gt;return_code != &quot;SUCCESS&quot;) { return response('failure'); } else { $request = $this-&gt;rsaCheckV2($obj); if ($request) { //处理完后台逻辑成功 if ($res) { $reply = &quot;&lt;xml&gt; &lt;return_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/return_code&gt; &lt;return_msg&gt;&lt;![CDATA[OK]]&gt;&lt;/return_msg&gt; &lt;/xml&gt;&quot;; echo $reply; // 向微信后台返回结果 } else { return response('failure'); } } else { return response('failure'); } } } 这里的rsaCheckV2就是我们验签的方法: public function rsaCheckV2($obj) { if($obj){ $data = array( 'appid' =&gt; $obj-&gt;appid, 'mch_id' =&gt; $obj-&gt;mch_id, 'nonce_str' =&gt; $obj-&gt;nonce_str, 'result_code' =&gt; $obj-&gt;result_code, 'openid' =&gt; $obj-&gt;openid, 'return_code' =&gt; $obj-&gt;return_code, 'fee_type' =&gt; $obj-&gt;fee_type, 'is_subscribe' =&gt; $obj-&gt;is_subscribe, 'trade_type' =&gt; $obj-&gt;trade_type, 'bank_type' =&gt; $obj-&gt;bank_type, 'total_fee' =&gt; $obj-&gt;total_fee, 'cash_fee' =&gt; $obj-&gt;cash_fee, 'transaction_id' =&gt; $obj-&gt;transaction_id, 'out_trade_no' =&gt; $obj-&gt;out_trade_no, 'time_end' =&gt; $obj-&gt;time_end ); $sign = $this-&gt;getSign($data); // 获取签名 进行验证 if ($sign == $obj-&gt;sign) { return true; }else{ return false; } } } 这样一来我们就可以完成支付完成结果通知后的业务逻辑 查询订单 对于我们的APP客户端依然要向我们发起一个请求，查询订单状态，此时我们需要客户端将订单号传递给我们，然后我们使用订单号，继续向微信发起请求： public function queryOrder(Curl $curl, $out_trade_no) { $nonce_str = $this-&gt;getNonceStr(); $data = array( 'appid' =&gt; $this-&gt;appid, 'mch_id' =&gt; $this-&gt;mch_id, 'out_trade_no' =&gt; $out_trade_no, 'nonce_str' =&gt; $nonce_str ); $sign = $this-&gt;getSign($data); $xml_data = '&lt;xml&gt; &lt;appid&gt;%s&lt;/appid&gt; &lt;mch_id&gt;%s&lt;/mch_id&gt; &lt;nonce_str&gt;%s&lt;/nonce_str&gt; &lt;out_trade_no&gt;%s&lt;/out_trade_no&gt; &lt;sign&gt;%s&lt;/sign&gt; &lt;/xml&gt;'; $xml_data = sprintf($xml_data, $this-&gt;appid, $this-&gt;mch_id, $nonce_str, $out_trade_no, $sign); $url = &quot;https://api.mch.weixin.qq.com/pay/orderquery&quot;; $curl-&gt;setUrl($url); $content = $curl-&gt;execute(true, 'POST', $xml_data); return $content; } 支付宝接口的处理会在以后的博客中继续更新 :bowtie: 注意事项 1.签名的字段参数小写 2.验签时去除sign字段 返回剩余字段进行签名 3.填写正确的商户key 相关文档 GitHub地址 PayApi 微信开发文档 开发者文档 简书 开发微信接口 ","link":"https://GeekGhc.github.io/post/app-wei-xin-zhi-fu-bao-zhi-fu-fu-wu-duan-jie-kou/"},{"title":"Redis在项目中的应用实例","content":"如今网站基本都集成了非常不错的缓存驱动 之前认识了Redis的基本使用 那么这里就结合一个帖子的浏览的应用实例去深入了解这些缓存机制的应用 既然初步了解Redis在Laravel中的应用 那么我们试想这样的一个应用场景 一个文章或者帖子的浏览次数的统计 如果只是每次增加一个浏览量 就到数据库新增一个数据 如果请求来那个太大这对数据库的消耗也就不言而喻了吧 那我们是不是可以有其他的解决方案 这里的解决方案就是 即使你的网站的请求量很大 那么每次增加一个访问量就在缓存中去进行更改 至于刷新Mysql数据库可以自定义为 多少分钟进行刷新一次或者访问量达到一定数量再去刷新数据库 这样数据也是准确的 效率也比直接每次刷新数据库要高出许多了 既然给出了相应的解决方案 我们就开始实施 我们以一篇帖子的浏览为例 我们先去创建对应的控制器 $ php artisan make:controller PostController 再去生成需要用到的 Model $ php artisan make:model Post -m 填写posts的迁移表的字段内容 Schema::create('posts', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;string(&quot;title&quot;); $table-&gt;string(&quot;content&quot;); $table-&gt;integer('view_count')-&gt;unsigned(); $table-&gt;timestamps(); }); 还有就是我们测试的数据的Seeder填充数据 $factory-&gt;define(App\\Post::class, function (Faker\\Generator $faker) { return [ 'title' =&gt; $faker-&gt;sentence, 'content' =&gt; $faker-&gt;paragraph, 'view_count' =&gt; 0 ]; }); 定义帖子的访问路由 Route::get('/post/{id}', 'PostController@showPost'); 当然我们还是需要去写我们访问也就是浏览事件的(在app/providers/EventServiceProvider中定义) protected $listen = [ 'App\\Events\\PostViewEvent' =&gt; [ // 'App\\Listeners\\EventListener', 'App\\Listeners\\PostEventListener', ], ]; 执行事件生成监听 $ php artisan event:generate 之前定义了相关的路由方法 现在去实现一下: public function showPost(Request $request,$id) { //Redis缓存中没有该post,则从数据库中取值,并存入Redis中,该键值key='post:cache'.$id生命时间6分钟 $post = Cache::remember('post:cache:'.$id, $this-&gt;cacheExpires, function () use ($id) { return Post::whereId($id)-&gt;first(); }); //获取客户端IP $ip = $request-&gt;ip(); //触发浏览量计数器事件 event(new PostViewEvent($post, $ip)); return view('posts.show', compact('post')); } 这里看的出来就是以Redis作为缓存驱动 同样的 会获取获取的ip目的是防止同一个ip多次刷新来增加浏览量 同样的每次浏览会触发我们之前定义的事件 传入我们的post和id参数 Redis的key的命名以:分割 这样可以理解为一个层级目录 在可视化工具里就可以看的很明显了 接下来就是给出我们的posts.show的视图文件 &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt; &lt;title&gt;Bootstrap Template&lt;/title&gt; &lt;!-- 新 Bootstrap 核心 CSS 文件 --&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css&quot;&gt; &lt;style&gt; html,body{ width: 100%; height: 100%; } *{ margin: 0; border: 0; } .jumbotron{ margin-top: 10%; } .jumbotron&gt;span{ margin: 10px; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-xs-12 col-md-12&quot;&gt; &lt;div class=&quot;jumbotron&quot;&gt; &lt;h1&gt;Title:{{$post-&gt;title}}&lt;/h1&gt; &lt;span class=&quot;glyphicon glyphicon-eye-open&quot; aria-hidden=&quot;true&quot;&gt; {{$post-&gt;view_count}} views&lt;/span&gt; &lt;p&gt;Content:{{$post-&gt;content}}&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;!-- jQuery文件--&gt; &lt;script src=&quot;//cdn.bootcss.com/jquery/1.11.3/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;!-- 最新的 Bootstrap 核心 JavaScript 文件 --&gt; &lt;script src=&quot;//cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; 初始化我们的事件就是接收一下这些参数即可 class PostViewEvent { use Dispatchable, InteractsWithSockets, SerializesModels; protected $ip; protected $post; /** * PostViewEvent constructor. * @param Post $post * @param $ip */ public function __construct(Post $post, $ip) { $this-&gt;post = $post; $this-&gt;ip = $ip; } /** * Get the channels the event should broadcast on. * * @return Channel|array */ public function broadcastOn() { return new PrivateChannel('channel-name'); } } 最主要的还是编写我们的监听事件: class PostEventListener { /** * 同一post最大访问次数,再刷新数据库 */ const postViewLimit = 30; /** * 同一用户浏览同一post过期时间 */ const ipExpireSec = 300; /** * Create the event listener. * */ public function __construct() { } /** * @param PostViewEvent $event */ public function handle(PostViewEvent $event) { $post = $event-&gt;post; $ip = $event-&gt;ip; $id = $post-&gt;id; //首先判断下ipExpireSec = 300秒时间内,同一IP访问多次,仅仅作为1次访问量 if($this-&gt;ipViewLimit($id, $ip)){ //一个IP在300秒时间内访问第一次时,刷新下该篇post的浏览量 $this-&gt;updateCacheViewCount($id, $ip); } } /** * 限制同一IP一段时间内得访问,防止增加无效浏览次数 * @param $id * @param $ip * @return bool */ public function ipViewLimit($id, $ip) { $ipPostViewKey = 'post:ip:limit:'.$id; //Redis命令SISMEMBER检查集合类型Set中有没有该键,Set集合类型中值都是唯一 $existsInRedisSet = Redis::command('SISMEMBER', [$ipPostViewKey, $ip]); //如果集合中不存在这个建 那么新建一个并设置过期时间 if(!$existsInRedisSet){ //SADD,集合类型指令,向ipPostViewKey键中加一个值ip Redis::command('SADD', [$ipPostViewKey, $ip]); //并给该键设置生命时间,这里设置300秒,300秒后同一IP访问就当做是新的浏览量了 Redis::command('EXPIRE', [$ipPostViewKey, self::ipExpireSec]); return true; } return false; } /** * 达到要求更新数据库的浏览量 * @param $id * @param $count */ public function updateModelViewCount($id, $count) { //访问量达到300,再进行一次SQL更新 $post = Post::find($id); $post-&gt;view_count += $count; $post-&gt;save(); } /** * 不同用户访问,更新缓存中浏览次数 * @param $id * @param $ip */ public function updateCacheViewCount($id, $ip) { $cacheKey = 'post:view:'.$id; //这里以Redis哈希类型存储键,就和数组类似,$cacheKey就类似数组名 如果这个key存在 if(Redis::command('HEXISTS', [$cacheKey, $ip])){ //哈希类型指令HINCRBY,就是给$cacheKey[$ip]加上一个值,这里一次访问就是1 $save_count = Redis::command('HINCRBY', [$cacheKey, $ip, 1]); //redis中这个存储浏览量的值达到30后,就去刷新一次数据库 if($save_count == self::postViewLimit){ $this-&gt;updateModelViewCount($id, $save_count); //本篇post,redis中浏览量刷进MySQL后,就把该篇post的浏览量清空,重新开始计数 Redis::command('HDEL', [$cacheKey, $ip]); Redis::command('DEL', ['laravel:post:cache:'.$id]); } }else{ //哈希类型指令HSET,和数组类似,就像$cacheKey[$ip] = 1; Redis::command('HSET', [$cacheKey, $ip, '1']); } } } 最后可以通过我们的工具查看具体效果 相关链接 Redis 命令 ","link":"https://GeekGhc.github.io/post/redis-zai-xiang-mu-zhong-de-ying-yong-shi-li/"},{"title":"Redis在Laravel中初识","content":"对于缓存系统 如今网站基本都集成了非常不错的缓存驱动 如Redis和Memcached 今天来简单谈谈在Laravel中是这些缓存技术 简介 Redis 是一款开源且先进的键值对数据库。由于它可用的键包含了字符串、哈希、列表、集合 和 有序集合，因此常被称作数据结构服务器。 当然在开始使用在前 在你的环境先下载好Redis 如果是windows文章下面有安装教程链接 如果是Mac或者Linux则可以按照官网上安装 当然为了可视化更好的管理Redis数据的话 这里推荐一个工具就是 Redis 可视化管理工具 在使用 Redis 之前，你必须通过 Composer 安装 predis/predis 扩展包（~1.0） $ composer require predis/predis 如果你需要一些可视化的Redis管理工具 Redis Desktop Manager不失为一种很好的选择 配置 有关缓存的配置都是在config/cache.php里面 而对于缓存存储的则是在config/database.php 在config/database.php里 可以看到关于Redis的相关配置 'redis' =&gt; [ 'client' =&gt; 'predis', 'default' =&gt; [ 'host' =&gt; env('REDIS_HOST', '127.0.0.1'), 'password' =&gt; env('REDIS_PASSWORD', null), 'port' =&gt; env('REDIS_PORT', 6379), 'database' =&gt; 0, ], ], 这些默认的配置对于我们开发来说已经足够了 当然除了这几个配置 PRedis 也可以为其配置其他参数 只需增加相应的配置参数即可 'read_write_timeout' =&gt; 60, 则是定义读取超时的时间 开始使用 就像我们平常使用Redis一样 我们在PRedis也会有一系列的set/get方法 在这里我们可以使用Redis这个facade 话不多说 为了更好的说明 我们可以去创建一个控制器 $ php artisan make:controller RedisController 新建一个方法 以一个简单的实例测试下 public function setInfo() { Redis::set(&quot;name&quot;, &quot;GeekGhc&quot;); dd(Redis::get(&quot;name&quot;)); } 当然在我们的Redis Desktop Manager打开可以查看到对应的数据信息 还有就是我们可以将command传递至服务器 它接收命令的名称作为第一个参数，第二个参数则为值的数组： $values = Redis::command('lrange', ['name', 5, 10]); 当然还有一些其他的Redis的命令用户 在官方文档上都有介绍 包括我们的订阅和发布 通过Cache Facade 当然对于Redis或者Memcached这些缓存方式 我们可以使用Cache这个Facade来管理 比如对于同样的Redis缓存的基本操作 Cache Facade提供了方便而又简洁的方法访问缓存实例 例如对于同样的基本的值的存取 我们可以这样写: public function setInfo() { Cache::Store(&quot;redis&quot;)-&gt;put(&quot;name&quot;,&quot;gavin&quot;,1); dd(Cache::store('redis')-&gt;get(&quot;name&quot;,&quot;def&quot;)); } 当然还有一些获取更新 删除 永久写入这些操作在文档中写的很详细 这里不再多说 值得一提的是 当我们需要全部清空这里的缓存时 提供给我们的方法是： Cache::flush(); 还有一个我们经常用到的就是获取更新 文档上给出的一个事例就是当我们需要从缓存取出所有用户 而当缓存中并没有时 则从数据库中 读取并加入缓存 这样的情景的话我们可以使用一个remember方法(这在之后我们也会用到) $value = Cache::remember('users', $minutes, function () { return DB::table('users')-&gt;get(); }); 我们需要了解的也就是这些 当然还有一些增加缓存驱动 我们得结合具体的应用场景再说 相关链接 windows 下redis安装 Redis-Laravel文档 Redis 可视化管理工具 Redis 命令 ","link":"https://GeekGhc.github.io/post/redis-zai-laravel-zhong-chu-shi/"},{"title":"应用中正则表达式的使用","content":"无论我们学习哪一门语言 我们都会遇到正则表达式的场景 他的使用确实解决了很多实际应用的很多问题 作为一名开发者这应该是每个人都应掌握的技能 简介 正则表达式对我们来说肯定不陌生了 无论你学习哪门语言 肯定会有所用到它 从js里的表单验证匹配到路由的匹配以及很多后台的 形式匹配验证 对于开发人员来讲都是很重要的一门技术 尽管平时一直在用 但发现这个却很值得整理一番 毕竟他的所存在的应用场景太多了 当然这里会推荐一个正则表达式的在线工具RegExr 一个好的工具对于学习往往可以起到事半功倍的效果 过程中也会结合MDN的正则表达式手册所罗列的内容加以说明 认识正则表达式 很多时候我们会去处理一些类似手机号 邮箱的验证 当然一开始我们需要知道的就是字符串的替换 举例来说的话就是如果我们需要打印hello world 在我们输入hello $text$时 我们希望匹配到结果是hello world所以我们的匹配规则也就是对$text$的匹配 我们要知道$符号是匹配字符串的一个结尾 所以对于这样的符号要进行转义 即我们的匹配规则也就是 /\\$text\\$/g 当然这里的 replace的也就是world这个单词 这里符号的转义就是在符号之前加上 \\ 在匹配规则中对大小写是敏感的 所以如果我们想忽略大小写的限制 那么我们可以在后面加上加上一个 i 标识 所以这个时候我们的匹配规则是 /\\$text\\$/ig 这里的i也就是ignore 而g这个标识就是global也就是全局性的匹配 就是说对全文都进行字符串的替换 量词的说明 这个在RegExr的Reference的Quantifiers &amp; Alternation里面已经列举了 其实主要我们常用的就是 ^ 匹配输入的开始。如果多行标志被设置为true，那么也匹配换行符后紧跟的位置 $ 匹配输入的结束。如果多行标示被设置为true，那么也匹配换行符前的位置 * 匹配前一个表达式0次或多次。等价于 {0,} + 匹配前面一个表达式1次或者多次。等价于 {1,} ? 匹配前面一个表达式0次或者1次。等价于 {0,1} . （小数点）匹配除换行符之外的任何单个字符。 如果我想匹配指定范围次数时 可以使用{m,n}在这之间加上想要的范围值 这样一来我们也就可以限定character的长度 这些在匹配元素如空格时就会用到 这些都是限定在他之前的元素的出现次数 元素分类 通常我们所需要面对的就是数字 单词以及一些特殊字符 在这里RegExr在Character classes列出了一些元素 我们可以先列举几个 \\d 匹配数字 等价于[0-9] \\D 匹配匹配一个非数字字符 等价于[^0-9]。 \\w 匹配是一个word即单词 \\s 匹配空格 \\t 匹配一个水平制表符 手机号匹配 对于手机号的匹配的应用其实很多 这时我们可以对用户输入的手机号进行正则匹配 手机号的第一位数字都是 1 所以当然一开始的规则也就是/1/ig 对于手机号码的第2个数字 因为服务商的不同也会有所不同 那么总结下来主要有34578 那么新的匹配规则就是/1[34578]/ig 也就是第二个数是34578里的其中一个 当然我们的手机位数都是11位 已经确定了2位 那么接下来还有9个 这9个也都是数字 所以可以新建规则为 (指明剩下的都是数字且为9位 ) /1[34578][0-9]{9}/ig 这里的[0-9]的匹配也可以使用之前所提出的元素也就是 \\d 也是代表了数字 这样也就可以匹配最基础的手机号码匹配 当然还有其他的规则 完全可以按照从左到有的匹配规则进行匹配 用户名匹配 想象一个场景就是在论坛社区里常常会**@某一个人 那么@**某个人这个字段会加上一个连接 这样就可以查看对方的个人主页 那个这个时候我们需要解决的就是匹配@Jelly这样的语句 所以我们可以定义规则如下` /@(\\w{1,})/ 这里的意思就是**@**开头 后面是一个word character 而\\w包含了数字字母和下划线 这个也就是一个应户名 同时我们回去匹配 1 个或 1个以上 而替换的文字当然就是a标签了: &lt;a href=&quot;/$1&quot;&gt;@$1&lt;/a&gt; 特别的作为一个整体进行匹配时 在两边加上()用来表明这匹配的一个整体 []代表的是或的关系即满足一个即可 邮件匹配 用户在注册登录时大都会用到邮件地址 当然用户也有可能输入一个无效的邮箱信息 那么我们就可以用正则表达式去匹配我们所需要的邮件地址 而去除那些没有用的邮件地址 其实邮件验证的匹配要求会比较严格 所以说实现一个完整的邮件匹配并不容易 但多数场景下借助正则表达式我们就可以满足我们的需求 首先来匹配到我们常用的邮箱地址 在知道我们邮箱地址的基本形式之后 我们可以去匹配一个基本的邮箱 因为我们邮箱肯定会有一个@符号 那么在@符号之前会有几个单词或者数字所以一开始我们定义规则为: /[\\w]+@/g 邮箱当然也会有类似二级域名的形式 如jelly.gavin@sina.com 如果再匹配-符号 所以在\\w匹配时需要匹配到这样的形式 我们之前罗列过小数点可以匹配到除换行符的任何单个字符 那个这里我们需要匹配的是小数点 So 接下来的规则定义就是 /[\\w\\.-]+@/g 接下来如果一个邮箱地址如 gehuachun@outlook.com 那么**@**符号后面也是一个单词加一个点 /[\\w]+@[\\w\\.-]+/g 再者的话就是邮箱地址的后缀 现在邮箱地址的后缀已经有很多种了并不是只有com 所以你认为只是几个字符的话我们就可以使用\\w /[\\w]+@[\\w\\.-]+\\.[\\w]+/g 这样就基本完成了对邮箱的匹配 当然如果需要限定格式的话比如我们只需要匹配Outlook和QQ邮箱 我们只需要提供两个选择就可以 /[\\w]+@(qq|outlook)\\.[\\w]+/ 因为之前已经说过()作为一个整体部分进行判断匹配 而|则标识或的关系 那么我们就可以成功匹配到我们需要的类型邮箱 相关链接 RegExr网址 MDN 正则表达式 菜鸟工具 ","link":"https://GeekGhc.github.io/post/ying-yong-zhong-zheng-ze-biao-da-shi-de-shi-yong/"},{"title":"浅谈CSS常用的预处理器(框架)","content":"CSS预处理器技术已经非常的成熟，而且也涌现出了很多种不同的CSS预处理器语言，而我们接触最多的就是Scss,Stylus,Less。如此之多的CSS预处理器 其终究目的是使得CSS开发更灵活和更强大 简介 CSS 预处理器技术已经非常的成熟，而且也涌现出了越来越多的 CSS 的预处理器框架。 不过这里就来谈谈最为普遍的三款 CSS 预处理器框架 不过最主要还是谈谈Sass和Less 定义说明 CSS预处理器 CSS 预处理器是一种语言用来为 CSS 增加一些编程的的特性，无需考虑浏览器的兼容性问题， 例如你可以在 CSS 中使用变量、简单的程序逻辑、 函数等等在编程语言中的一些基本技巧，可以让你的 CSS 更为简洁，适应性更强，代码更直观等诸多好处 通俗的将就是讲原本的CSS语言抽象出来使他具有一定的程序逻辑 也省去了中间的很多步骤 而这些预处理器无非就是集合了语法、变量、嵌套、混入(Mixin)、继承、导入、函数和操作符等方面的操作处理 Sass Sass是一种动态样式语言，Sass语法属于缩排语法， 比css比多出好些功能(如变量、嵌套、运算,混入(Mixin)、继承、颜色处理，函数等)，这样也使开发者更加的容易阅读 说白了sass就是scss的严格模式 所以说他更像一门编程语言 因为他有一些诸如语言的特性 sass有变量和作用域 变量有全局和局部之分，并且有优先级 sass有函数的概念 进程控制(也就是我们所说的@if @else等) 数据结构 $list类型-数组 $map类型-object 其余的也有string、number、function等类型 Scss SCSS 是 Sass 3 引入新的语法，其语法完全兼容 CSS3，并且继承了 Sass 的强大功能。也就是说，任何标准的 CSS3 样式表都是具有相同语义的有效的 SCSS 文件。另外，SCSS 还能识别大部分 CSS hacks（一些 CSS 小技巧）和特定于浏览器的语法 Scss 和 Sass 的区别 Sass 和 SCSS 也可以说是同一种的，我们平时都称之为 Sass，两者之间不同之处有以下两点： 首先文件扩展名不同，Sass 是以&quot;.sass&quot;后缀为扩展名，而 SCSS 是以&quot;.scss&quot;后缀为扩展名 语法书写方式不同，Sass 是以严格的缩进式语法规则来书写(这也是之前我们所提到的) 不带大括号({})和分号(;)，而 SCSS 的语法书写和我们的 CSS 语法书写方式非常类似。 我们平常所说的Sass文件基本都是以.scss文件 这样也是避免 sass 后缀名的严格格式要求报错。 举个例子来说的话: Sass语法 $primary-color: #eee //定义变量 body color: $primary-color Scss语法 $primary-color: #eee; body { color: $primary-color; } 最后编译的结果是: body { color: #eee; } Less 受SASS的影响较大，但又使用CSS的语法，让大部分开发者和设计师更容易上手，在ruby社区之外支持者远超过SASS 其缺点是比起SASS来，可编程功能不够，不过优点是简单和兼容CSS，个人实际开发的话会更容易上手 反过来也影响了SASS演变到了SCSS的时代 这也就是为什么现在Scss完全兼容Css3的原因了 Stylus 2010年产生，来自Node.js社区，主要用来给Node项目进行CSS预处理支持 在此社区之内有一定支持者，在广泛的意义上人气还完全不如SASS和LESS 简单语法使用 1.变量 你可以在 CSS 预处理器中声明变量，并在整个样式单中使用，支持任何类型的变量 例如颜色、数值(不管是否包括单位)、文本。然后你可以任意引用该变量 不同的是Sass允许使用变量，所有变量以$开头 $mainColor: #0982c1; $siteWidth: 1024px; $borderStyle: dotted; body color: $mainColor; border: 1px $borderStyle $mainColor; max-width: $siteWidth; 而Less则以@开始 @mainColor: #0982c1; @siteWidth: 1024px; @borderStyle: dotted; body { color: @mainColor; border: 1px @borderStyle @mainColor; max-width: @siteWidth; } 所以最后的编译结果就是: body { color: #0982c1; border: 1px dotted #0982c1; max-width: 1024px; } 可以体会得到的是变量的使用 就和一个全局变量的作用差不多 我们在需要修改颜色等样式的值时只需要去修改定义的值就好了 当然变量也是有作用域的 就如上面的是声明在规则块定义外的 如果定义在css规则块内，那么该变量只能在此规则块内使用 $nav-color: #F90; nav { $width: 100px; width: $width; color: $nav-color; } //编译后 nav { width: 100px; color: #F90; } 这里的$width就只能在nav这个规则快里使用 2.嵌套 这个也是我认为非常方便的一个地方 很多时候样式的定义需要一堆的父级类名的限制 最后的结果呢一个样式却堆成很长 又很难后期维护 特别的在我们需要在CSS中相同的 parent 引用多个元素，这将是非常乏味的，你需要一遍又一遍地写 parent。例如 section { margin: 10px; } section nav { height: 25px; } section nav a { color: #0982C1; } section nav a:hover { text-decoration: underline; } 这时我们在Css预处理器里可以这样定义 section { margin: 10px; nav { height: 25px; a { color: #0982C1; &amp;:hover { text-decoration: underline; } } } } 这里最后的编译结果和上面的是一样的 最后的结果也一目了然了 下面的定义我们很清楚的看到父级与子级的关系 修改起来也十分方便 这里使用了一个父选择器的标识符&amp; 当然还有就是群组选择器的嵌套 举例来说的话就是: .container { h1, h2, h3 {margin-bottom: .8em} } 当然这样是等价于: .container h1, .container h2, .container h3 { margin-bottom: .8em } 这样其实也很容易明白 对于选择器的使用其实还有诸如 子组合选择器和同层组合选择器：&gt;、+和~ 这些在文档上都有写到 看一下就能明白了 理解起来和父选择器差不多 还有一个值得看一下的就是属性的嵌套 比如在对于border的定义 如果要反复写border-style border-width border-color以及border-*等也是非常烦人的。在预处理器中，你只需敲写一遍border nav { border: { style: solid; width: 1px; color: #ccc; } } 最后的编译结果看看也能知道就是: nav { border-style: solid; border-width: 1px; border-color: #ccc; } 3. Mixins (混合器) Mixins 有点像是函数或者是C语言的宏，我个人更觉得更像是函数的定义 当你某段 CSS 经常需要在多个元素中使用时 你可以为这些共用的 CSS 定义在一个 Mixin，然后你只需要在需要引用这些 CSS 地方调用该 Mixin 即可 同时你可以传入你的参数 那么那块就会载入你定义在Mixins的样式代码了 使用混合器 在Sass里 @mixin rounded-corners { -moz-border-radius: 5px; -webkit-border-radius: 5px; border-radius: 5px; } 这样你就可以在需要这块代码的地方去引入 notice { background-color: green; border: 2px solid #00aa00; @include rounded-corners; } 所以最终编译的结果就是 .notice { background-color: green; border: 2px solid #00aa00; -moz-border-radius: 5px; -webkit-border-radius: 5px; border-radius: 5px; } 在Less语法里 .error(@borderWidth: 2px) { border: @borderWidth solid #F00; color: #F00; } .generic-error { padding: 20px; margin: 4px; .error(); } .login-error { left: 12px; position: absolute; top: 20px; .error(5px); } 当然混合器中不仅可以包含属性，也可以包含css规则，包含选择器和选择器中的属性 举例来说就是 @mixin no-bullets { list-style: none; li { list-style-image: none; list-style-type: none; margin-left: 0px; } } 其实最后编译的道理是相同的这里就不过多展开说了 混合器传递参数 这时就更像一个function了 @mixin link-colors($normal, $hover, $visited) { color: $normal; &amp;:hover { color: $hover; } &amp;:visited { color: $visited; } } 所以接下来你在引入时需要给这个CSS函数传递必要的参数 就像这样 a { @include link-colors(blue, red, green); } 而在Less里就可以这样定义 .link-colors($normal, $hover, $visited) { color: $normal; &amp;:hover { color: $hover; } &amp;:visited { color: $visited; } } 那么调用的时候就是 a { .link-colors(blue, red, green); } 两者其实都差不多 只不过表示方式不太一样而已 4. 继承 任何css规则都可以继承其他规则，几乎任何css规则也都可以被继承. 通常使用继承会让你的css美观、整洁。因为继承只会在生成css时复制选择器 .menu { border: 1px solid #ddd; } .footer { @extend .menu; } 这样编译后的效果和下面的是一样的: .menu, .footer { border: 1px solid #ddd; } 这样一来.footer继承了.menu的样式定义 那么我们就可以不用使用逗号将两者分开来写了 在Less里我们同样可以这样写: .menu { border: 1px solid #ddd; } .footer { &amp;:extend .menu; } 最后编译出来的结果和之前是一样的 5. 导入 在Sass的import的规则在生成css文件时就把所相关的文件导入进来了 这也就省去了额外的下载请求 使用SASS部分文件 但你在写一些不需要独立生成css文件 而只是为了引入到其他的sass文件 那么这里的sass文件也叫局部文件 因为他不要单独生成 对应的css文件 而是想结合其他的sass文件最终生成css文件 对于这样的文件有个约定就是命名以下划线开头 而当引入这个文件时就只需要提供下划线后面的文件名即可 如你想引入 themes/_night-sky.scss 这个局部文件里的变量 只需在样式表中写@import &quot;themes/night-sky&quot; 这里介绍一个最为常用的场景就是嵌套的引入 举例说明，有一个名为_blue-theme.scss的局部文件: aside { background: blue; color: white; } 如果你需要导入到css文件里 : .blue-theme {@import &quot;blue-theme&quot;} //结果跟你直接在.blue-theme选择器内写_blue-theme.scss文件的内容完全一样。 .blue-theme { aside { background: blue; color: #fff; } 和Sass差不多 在Less中 如果你导入的是less文件 完全可以省略后缀名: @import &quot;theme&quot;; // theme.less @import &quot;style.css&quot;; 总结 不管是Sass，还是Less，Stylus 都可以视为一种基于CSS之上的高级语言，其目的是使得CSS开发更加灵活和强大 Sass的功能比Less强大,因为他更像一门编程语言了，而Less则相对清晰明了,易于上手,对编译环境要求比较宽松。因为Sass需要Ruby环境的支持 编译比不上less.js那么直接 但我想这个并不是什么大问题 相对而言，国内前端团队使用Less的同学会略多于Sass 尽管现在很多的公司都开始转向于用Sass 而个人开发的话Less更为容易上手和方便 其实无论哪种选择 都是可以的 始终记住存在即合理 所以也没必要太纠结选择那种工具 实现自己最为满意的开发方式就ok 相关链接 SASS中文网 Less手册 ","link":"https://GeekGhc.github.io/post/qian-tan-css-chang-yong-de-yu-chu-li-qi-kuang-jia/"},{"title":"轻松部署你的PHP7运行环境","content":"在部署一个服务器时 我们可以有多种选择 比如apache和nginx 在这些选择之中 每个人都有最终的一个目的就是部署好环境 发布自己的项目网站 这里就对Nginx+PHP7+Mysql+phpmyadmin的安装做一个总结 简介 对于有时候服务器的安装部署 每次有的过程忘记总得再把之前的笔记再找出来 现在将整个流程做一个整理 结合自己以前遇到的各种坑和实践经验吧 这样也方便以后少浪费点时间在查找各种笔记 😄 集成环境 如果你嫌这样麻烦这里推荐一个非常实用的集成环境安装(也算是一个彩蛋喽) https://oneinstack.com/ 当然如果你想自己手动安装的话就继续看看下面的文章吧 服务目录 Nginx /etc/nginx Mysql /var/lib/mysql php7.1 /usr/local/php php-fpm /usr/local/bin/php-fpm phpmyadmin /data/www/phpmyadmin 站点根目录 /data/www/ 安装nginx $ sudo yum install nginx 这里你可以选择编译安装或者这种仓库的形式安装 编译安装的可选择性更好 你可以安装到指定的目录 比如我们一般或放在/usr/local/nginx 这里采用的是包的安装 此时nginx安装在/etc/nginx 安装完毕之后 $ nginx -v Nginx服务的一些命令形式 $ systemctl restart nginx $ systemctl stop nginx $ systemctl start nginx 设置开机启动 $ systemctl enable nginx 安装Mysql57 1.下载 mysql57-community-release-el7-8.noarch.rpm 的 YUM 源： $ yum install mysql57-community-release-el7-8.noarch.rpm $ wget http://repo.mysql.com/mysql57-community-release-el7-8.noarch.rpm 2.查看mysql源是否安装成功 $ yum repolist enabled | grep &quot;mysql.*-community.*&quot; 安装 MySQL(一路Y就可以)： $ yum install mysql-community-server 4.启动Mysql $ systemctl start mysqld 5.设置开机启动 $ systemctl enable mysqld $ systemctl daemon-reload 6.接下来就是去修改数据库的密码 mysql安装完成之后，在/var/log/mysqld.log文件中给root生成了一个默认密码。通过下面的方式找到root默认密码，然后登录mysql进行修改： 必须为启动mysql之后 $ grep 'temporary password' /var/log/mysqld.log 有了这个密码去登录mysql $ mysql -u root -p mysql5.7默认安装了密码安全检查插件（validate_password），默认密码检查策略要求密码必须包含： 大小写字母、数字和特殊符号，并且长度不能少于8位。否则会提示ERROR 1819 (HY000): Your password does not satisfy the current policy requirements错误， 所以这里的解决办法就是要么修改的密码满足他的验证规则 如果你想密码不用那么复杂 那么你就可以去关闭这些验证规则 7.在/etc/my.cnf文件添加validate_password_policy配置，指定密码策略 当然你也可以直接关闭验证 $ vim /etc/my.cnf 接着在末尾添加: $ validate_password = off 填写完密码之后就可以登录了 接着设置密码: $ set password = password(&quot;xxxx&quot;); 8.重启我们的mysql $ systemctl restart mysqld 接下来我们就可以直接使用刚设置的密码去登录服务器的mysql了 9.一些命令 启动 MySQL 服务：service mysqld start 关闭 MySQL 服务：service mysqld stop 重启 MySQL 服务：service mysqld restart 查看 MySQL 的状态：service mysqld status &amp; systemctl status mysqld 10、添加远程登录用户 默认只允许root帐户在本地登录，如果要在其它机器上连接mysql，必须修改root允许远程连接， 或者添加一个允许远程连接的帐户(这种最为理想) 这里我们先给所有用户以权限 $ grant all privileges on *.* to 'root'@'%' identified by '123456' with grant option; # root是用户名，%代表任意主机，'123456'指定的登录密码（这个和本地的root密码可以设置不同的，互不影响） $ flush privileges; # 重载系统权限 $ exit; 12.Centos 7 防火开启3306端口 然后编辑系统的开放端口列表，增加3306端口，重启防火墙即可。 vi /etc/sysconfig/iptables # 加上下面这行规则也是可以的 -A INPUT -p tcp -m state --state NEW -m tcp --dport 3306 -j ACCEPT $ iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport 3306 -j ACCEPT 13.配置默认编码为utf8 修改/etc/my.cnf配置文件，在[mysqld]下添加编码配置，如下所示： [mysqld] character_set_server=utf8 init_connect='SET NAMES utf8' 14 文件目录: 存放数据库文件的目录 /var/lib/mysql 默认配置文件路径： /etc/my.cnf 日志文件：/var/log//var/log/mysqld.log 服务启动脚本：/usr/lib/systemd/system/mysqld.service socket文件：/var/run/mysqld/mysqld.pid 安装phpmyadmin 1.官网下载 phpmyadmin 下载完毕之后可以上传到服务器的目录 例如可以放在/root/phpmyadmin/ 2.进入目录 解压文件 $ cd /root/phpmyadmin $ unzip phpMyAdmin-4.7.0-all-languages.zip 3.移动解压后的文件到站点根目录(nginx 配置的root路径为/data/www ) 比如 $ mv phpMyAdmin-4.7.0-all-languages /data/www/phpmyadmin 4.修改文件的拥护者 $ chown root:root /data/www/phpmyadmin 5.这里可能遇到的问题 提示没有发现指定文件 如果不存在/var/mysql则创建 $ sudo mkdir /var/mysql 接着创建一个软连接 $ sudo ln -s /var/lib/mysql/mysql.sock /var/mysql/mysql.sock 如果你找不到你服务器下的文件 可以查找(以上只是我的目录): $ sudo find / -name mysql.sock 需要一个密文 那么在配置文件填入大于32为的字符串就可以了： $cfg['blowfish_secret']=''; 接着可以在/etc/nginx下去创建phpmyadmin.conf 内容是: location /phpMyAdmin { alias /data/www/phpMyAdmin; index index.php; location ~ ^/phpMyAdmin/.+\\.php$ { alias /data/www/phpMyAdmin; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /data/www$fastcgi_script_name; include fastcgi_params; } } 接着你可以在nginx.conf里去包含这个配置文件 location ~* \\.php$ { fastcgi_index index.php; fastcgi_pass 127.0.0.1:9000; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param SCRIPT_NAME $fastcgi_script_name; } include /etc/nginx/phpmyadmin.conf; 接着你可以访问http://example.com/phpmyadmin就可进入phpmyadmin操作数据库了 编译安装PHP7 1.下载 wget -O php7.tar.gz http://cn2.php.net/get/php-7.1.1.tar.gz/from/this/mirror 2.解压php7 $ tar -xvf php7.tar.gz 3.进入php7目录 $ cd PHP-7.1.1 4.下载相关依赖 $ yum install -y libxml2 libxml2-devel openssl openssl-devel bzip2 bzip2-devel libcurl libcurl-devel libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel gmp gmp-devel libmcrypt libmcrypt-devel readline readline-devel libxslt libxslt-devel 5.当然在编译安装之前 需要下载gcc编译 $ yum install -y gcc 6.编译配置 ./configure \\ --prefix=/usr/local/php \\ --with-config-file-path=/usr/local/php/etc \\ --exec-prefix=/usr/local/php \\ --bindir=/usr/local/php/bin \\ --sbindir=/usr/local/php/sbin \\ --includedir=/usr/local/php/include \\ --libdir=/usr/local/php/lib/php \\ --mandir=/usr/local/php/php/man \\ --enable-fpm \\ --with-fpm-user=nginx \\ --with-fpm-group=nginx \\ --enable-inline-optimization \\ --disable-debug \\ --disable-rpath \\ --enable-shared \\ --enable-soap \\ --with-libxml-dir \\ --with-xmlrpc \\ --with-openssl \\ --with-mcrypt \\ --with-mhash \\ --with-pcre-regex \\ --with-sqlite3 \\ --with-zlib \\ --enable-bcmath \\ --with-iconv \\ --with-bz2 \\ --enable-calendar \\ --with-curl \\ --with-cdb \\ --enable-dom \\ --enable-exif \\ --enable-fileinfo \\ --enable-filter \\ --with-pcre-dir \\ --enable-ftp \\ --with-gd \\ --with-openssl-dir \\ --with-jpeg-dir \\ --with-png-dir \\ --with-zlib-dir \\ --with-freetype-dir \\ --enable-gd-native-ttf \\ --enable-gd-jis-conv \\ --with-gettext \\ --with-gmp \\ --with-mhash \\ --enable-json \\ --enable-mbstring \\ --enable-mbregex \\ --enable-mbregex-backtrack \\ --with-libmbfl \\ --with-onig \\ --enable-pdo \\ --with-mysql=mysqlnd \\ --with-mysqli=mysqlnd \\ --with-pdo-mysql=mysqlnd \\ --with-zlib-dir \\ --with-pdo-sqlite \\ --with-readline \\ --enable-session \\ --enable-shmop \\ --enable-simplexml \\ --enable-sockets \\ --enable-sysvmsg \\ --enable-sysvsem \\ --enable-sysvshm \\ --enable-wddx \\ --with-libxml-dir \\ --with-xsl \\ --enable-zip \\ --enable-mysqlnd-compression-support \\ --with-pear \\ --enable-opcache 整理之后可以在服务器里面执行 ./configure --prefix=/usr/local/php --exec-prefix=/usr/local/php --with-config-file-path=/usr/local/php/etc --bindir=/usr/local/php/bin --sbindir=/usr/local/php/sbin --includedir=/usr/local/php/include --libdir=/usr/local/php/lib/php --mandir=/usr/local/php/php/man --enable-fpm --with-fpm-user=nginx --with-fpm-group=nginx --enable-inline-optimization --disable-debug --disable-rpath --enable-shared --enable-soap --with-libxml-dir --with-xmlrpc --with-openssl --with-mcrypt --with-mhash --with-pcre-regex --with-sqlite3 --with-zlib --enable-bcmath --with-iconv --with-bz2 --enable-calendar --with-curl --with-cdb --enable-dom --enable-exif --enable-fileinfo --enable-filter --with-pcre-dir --enable-ftp --with-gd --with-openssl-dir --with-jpeg-dir --with-png-dir --with-zlib-dir --with-freetype-dir --enable-gd-native-ttf --enable-gd-jis-conv --with-gettext --with-gmp --with-mhash --enable-json --enable-mbstring --enable-mbregex --enable-mbregex-backtrack --with-libmbfl --with-onig --enable-pdo --with-mysql=mysqlnd --with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --with-zlib-dir --with-pdo-sqlite --with-readline --enable-session --enable-shmop --enable-simplexml --enable-sockets --enable-sysvmsg --enable-sysvsem --enable-sysvshm --enable-wddx --with-libxml-dir --with-xsl --enable-zip --enable-mysqlnd-compression-support --with-pear --enable-opcach 从配置中看到 --prefix=/usr/local/php \\ 所以最终的php安装目录为 /usr/local/php 配置文件设置 $ --with-config-file-path=/usr/local/php/etc \\ 配置文件放在usr/local/php/etc 7.正式安装 $ make &amp;&amp; make install 8.配置环境变量 $ vi /etc/profile 在最后加上(也就是我们安装php存放的路径): PATH=$PATH:/usr/local/php/bin export PATH 执行命令使得改动立即生效 $ source /etc/profile 10.配置php-fpm $ cp php.ini-production /usr/local/php/php.ini $ cp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.conf cp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.d/www.conf cp sapi/fpm/init.d.php-fpm /usr/local/bin/php-fpm 所以我们php-fpm的位置为usr/local/bin/php-fom 11.配置php.ini 需要着重提醒的是，如果文件不存在，则阻止 Nginx 将请求发送到后端的 PHP-FPM 模块， 以避免遭受恶意脚本注入的攻击。 将 php.ini 文件中的配置项 cgi.fix_pathinfo 设置为 0 vim /usr/local/php/php.ini 定位到 cgi.fix_pathinfo= 并将其修改为如下所示 cgi.fix_pathinfo=0 编辑nginx.conf vim /etc/nginx/nginx.conf 12.php-fpm的一些命令形式 /usr/local/bin/php-fpm [start | stop | reload] 部署ssl证书 server { listen 443 ssl http2 default_server; server_name www.example.com; root /data/www; #站点的根目录 ssl on; ssl_certificate &quot;/usr/ssl/1_www.example.com_bundle.crt&quot;; ssl_certificate_key &quot;/usr/ssl/2_www.example.com.key&quot;; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / { index index.php index.html index.htm; } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 这里的证书和解密后的私钥文件放在/usr/ssl/目录下 每个证书的提供商可能提供的形式不一样 不过最终我们需要的就是颁发的证书和解密后的私钥文件 相关链接文档 Mysql http://www.centoscn.com/mysql/2016/0626/7537.html http://www.linuxidc.com/Linux/2016-09/135288.htm http://www.linuxidc.com/Linux/2016-06/132676.htm PHP http://php.net/manual/zh/install.unix.nginx.php http://www.jb51.net/article/109228.htm http://blog.csdn.net/dglxsong/article/details/52075918 http://blog.csdn.net/u014595668/article/details/50188127 SSL证书 腾讯云的证书配置 ","link":"https://GeekGhc.github.io/post/qing-song-bu-shu-ni-de-php7-yun-xing-huan-jing/"},{"title":"轻松部署你的项目-Deployer","content":"实现轻松部署你的个人项目 这是一个具有模块化、代码回滚、并行任务等功能的 PHP 部署工具。 安装 1.第一种方法: curl -LO https://deployer.org/deployer.phar mv deployer.phar /usr/local/bin/dep chmod +x /usr/local/bin/dep 如果你需要其他的版本 需要升级Deployer 可以使用命令 $ dep self-update 2.composer 安装 $ composer require deployer/deployer --dev 当然你也可以使用全局安装 $ composer global require deployer/deployer 接着使用下面的命令: $ php vendor/bin/dep 3.如果你想使用源代码部署 那么可以 $ git clone https://github.com/deployphp/deployer.git 接着在项目文件夹里运行 $ php ./build 这是会建立 deployer.phar 的 Phar 存档 为了方便使用 这里还是推荐第一种方式安装 如果你不是在MAC环境下 那么你可以使用全局安装 使用 现在我们就可以通过dep这个命令使用Deployer了 在你的项目目录的终端执行: $ dep init 运行此命令后会给出下图的选项，可选择你的项目类型，这里我以Laravel项目为例 ，so 选择 [1] Laravel 初始化完成后，会自动生成 deployer.php 文件。里面包含了基本的部署配置和任务， 且有明确的注释，你可以根据注释在适当的地方添加配置以及任务。 我们设置好相应的Repository 和server 即可以完成部署 在deployer.php我们可以定义一系列的任务 举个官方的列子来说 如果我们想查看当前的目录 我们可以为这个task提供一个说明 desc('My task'); 接着就是定义task: task('pwd', function () { $result = run('pwd'); writeln(&quot;Current dir: $result&quot;); }); 接着我们去命令行执行: $ task pwd 我们会得到如下反馈 ➤ Executing task pwd Current dir: /var/www/domain.com ✔ Ok 现在我们可以开始我们的第一次部署 我们可以根据文件的注释填写相关的信息 如repository, shared_files 在第一次部署后会在服务器上生成下面的目录 releases 包含你部署的项目的版本 shared 包含你部署项目的共享文件或目录（如：Laravel 的 Storage 目录、.env 文件等 ） current 软连接到你当前发布的版本 所以如果你是nginx服务器 那么最后的server root应该这样配置 server { listen 80; server_name domain.org; root /var/www/html/current/public; location / { try_files $uri /index.php$is_args$args; } } release默认保存5个版本 当然你也可以修改成你希望的值 $ set('keep_releases', 10); 如果在部署的过程中出席那问题 你可以像在laravel数据库迁移一样 回滚当前的操作 $ dep rollback 你可以在执行一个任务之前或者之后去运行其他的任务 无非就是before after里的逻辑操作 这个在deployer.php都有实例的 所以这里我们可以定义一系列的处理业务 如重启php-fpm 执行项目的初始化数据 就以laravel项目来说 就可以执行php artisan migrate 这些都可以在部署时自动在Task里定义完成 而你只需要给出这一系列任务的顺序就行 ","link":"https://GeekGhc.github.io/post/qing-song-bu-shu-ni-de-xiang-mu-deployer/"},{"title":"Laravel 实现文章浏览次数统计","content":"在实现文章帖子的浏览次数统计时 我们可能会选择Redis作为缓存计算 当然也可以使用session来进行统计 这里我们再结合事件的监听来计算文章的浏览量 介绍 在有关用户浏览文章或帖子的访问数的统计时 我们可能会考虑到使用缓存机制来实现 即可以使用Redis来存储一篇帖子的浏览数 当然我们也可以有其他的更为简单的处理方式 这里我们可以结合Laravel的Event Listener来实现文章浏览的监听 首先我们可以先去创建一个ArticleController 这样我们就可以去执行文章的一系列业务逻辑 $ php artisan make:controller ArticleController 接着当然是去生成Article Model和他的数据表 $ php artisan make:model Article -m 生成完毕之后 可以去定义一下articles这个table public function up() { Schema::create('articles', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;string('title'); $table-&gt;text('body'); $table-&gt;integer('user_id')-&gt;unsigned(); $table-&gt;integer('last_user_id')-&gt;unsigned(); $table-&gt;integer('view_count')-&gt;default(0); $table-&gt;integer('comment_count')-&gt;default(0); $table-&gt;foreign('user_id') -&gt;references('id') -&gt;on('users') -&gt;onDelete('cascade') -&gt;onUpdate('cascade'); $table-&gt;timestamps(); }); } 其实在这里主要的就是view_count这个field 因为这里就是我们用来记录文章的浏览次数的 最后执行数据表迁移: $ php artisan migrate 当然这些只是前期工作 我们还是得做的 一些具体内容 个人看具体需求而定 创建事件监听 我们需要对文章浏览这个事件进行监听 So 我们需要创建一个文章浏览的事件并进行监听 我们可以单独去生成相应的event和listener当然我们也可以在app/Providers/EventServiceProvider去声明: protected $listen = [ 'App\\Events\\UserRegistered' =&gt; [ 'App\\Listeners\\SendWelcomeEmail', ], 'App\\Events\\ArticleView' =&gt; [ 'App\\Listeners\\ArticleViewListener', ], 'App\\Events\\PostView' =&gt; [ 'App\\Listeners\\PostViewListener', ], ]; 这里给出了两个实例 因为在我之前的项目里是有文章和帖子 当然这并不重要 我们需要关注的就是文章 声明完成之后 我们再去命令行: $ php artisan event:generate 这样我们在app/Events 和 app\\Listeners里面就会有相应的事件监听文件 在app\\Events\\ArticleView.php去完成一下他的构造函数 因为我们需要知道是对哪一篇文章的浏览 class ArticleView { use InteractsWithSockets, SerializesModels; public $article; public function __construct(Article $article) { $this-&gt;article = $article; } /** * Get the channels the event should broadcast on. * * @return Channel|array */ public function broadcastOn() { return new PrivateChannel('channel-name'); } } 之后我们就要去app\\Listeners\\ArticleViewListener.php里去写具体的逻辑业务 首先之前我们说过利用laravel Session机制去实现数量的统计 所以在构造函数中先去注入Session protected $session; /** * Create the event listener. * * @return void */ public function __construct(Store $session) { $this-&gt;session = $session; } 这样其实也很好理解 而具体的处理代码就是在handle里面 这和我们去定义console command命令是一样的 public function handle(ArticleView $event) { $article = $event-&gt;article; //查看是否被浏览过 if(!$this-&gt;hasViewedArticle($article)){ //最近没有浏览 则 浏览数加1 $article-&gt;increment('view_count'); //看过文章之后将保存到Session $this-&gt;storeViewedArticle($article); } } 而这里需要的其他方法则定义如下(都给出了相应的解释) //文章最近是否被浏览过 public function hasViewedArticle($article) { return array_key_exists($article-&gt;id,$this-&gt;getViewedArticle($article)); } //如果浏览过则获取session存入的浏览记录 public function getViewedArticle($article) { return $this-&gt;session-&gt;get('viewed_article', []); } //最近第一次浏览 存入session public function storeViewedArticle($article) { $key = 'viewed_article.' . $article-&gt;id; $this-&gt;session-&gt;put($key, time()); } 整个过程和用户的登录差不多 浏览记录不可能每一次刷新都会增加 而是在相应的一段时间内 所以采用Session来存储当前文章对应的一个时间戳最近一次的浏览记录 如果最近没有浏览 则相应的文章浏览数加一 否则不作为 原理也很简单 主要就是结合事件取监听文章浏览这个动作 当然也有相关的Packages可以使用 这里推荐cjjian的一篇博文可供参考 ","link":"https://GeekGhc.github.io/post/laravel-shi-xian-wen-zhang-liu-lan-ci-shu-tong-ji/"},{"title":"CSS 实现IOS毛玻璃的虚化效果","content":"今天不谈php 偶尔闲下来 就稍谈下了下css的毛玻璃效果 其实这样的效果如果处理的好的话 给用户的体验还是非常不错的 介绍 可能我们最初见到的也就是IOS的毛玻璃的效果 现在很多的软件 例如一些音乐软件 在背静虚化上都有处理 这其实实现起来并不是一件什么 难事，但对于用户的视觉效果还是很有必要的。我们经常会遇到这样的背景虚化过滤效果。 作为一名PHPer 我想一些基本的前端技能还是要有的 所以今天不谈后端 就单纯的说下 磨砂的这种效果实现 最后我会将代码发布到CodePen上 MDN filter说明 我的CodePen笔记地址 CodePen 首先说下CodePen这个平台 可能很多人也知道 但我想更多人最为熟悉的还是 jsfiddle 这个我们平常用来测试页面的网站 很多的demo我们在这里运行以达到自己的想要的效果 我在CodePen上已经用了半年多了 确实体验不错 特别是做前端的话 很多时候你会在这里发现很多设计灵感 说起来这明义上是一个代码笔记的意思 不过其创新程度确实让人眼前一亮 在CodePen里只提供最新版本的jQuery, MooTools, Prototype框架，且默认不使用任何库 这给开发者很大的空间 另外还有一点很重要的就是CodePen的所见即所得 这点对于一名程序员来说是一件非常爽的一件事 当然作为现在很多的平台 我想更多的还是支持每一位开发者更好的更为自由去交流自己的想法 开始 首先当然新建一个index.html文件 为了方便说明 在本地就直接新建一个css文件 并进行引入 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Css 实现磨砂效果&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;style.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;main&quot;&gt; &lt;h1&gt;Hello JellyBean&lt;/h1&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 接下来就是在style.css定义相关的样式 为了最终的效果 这里引入一张图片作为背景 body , .main::before { background: url(&quot;http://i2.muimg.com/567571/63ce92cd55b62c6c.jpg&quot;) 0/cover fixed; } 接着定义一下.main和h1的布局 只需放在你需要的网页个位置 .main { position: relative; top: 12vh; margin: auto; min-height: 75vh; max-height: 440px; width: 80%; max-width: 680px; color: rgba(255, 255, 255, 0.94); box-shadow: 0 0 0 1px rgba(255, 255, 255, 0.2) inset, 0 1px 1px rgba(0, 0, 0, 0.3); text-shadow: 0 1px 1px rgba(25, 25, 25, 0.5); background: rgba(255, 255, 255, 0.06); cursor: pointer; overflow: hidden; } .main &gt; h1 { position: absolute; top: 50%; left: 50%; -webkit-transform: translate3d(-50%, -50%, 0); transform: translate3d(-50%, -50%, 0); font-size: 3rem; margin-top: 0; -webkit-transition: all .2s ease-in-out; transition: all .2s ease-in-out; } 这边只是将这个div相对的放在了屏幕中间而已 效果就是这样的最后 接下来其实就是最只要的 我们前提是要了解下filter这个属性 filter 函数 filter 属性定义了元素(通常是)的可视效果(例如：模糊与饱和度)。 滤镜通常使用百分比 (如：75%), 当然也可以使用小数来表示 (如：0.75) 还有一点需要注意的是 Internet Explorer 不支持 filter 属性。 所以这里我们是在Chrome里进行测试的 filter函数里有几种效果 这几种具体的效果很简单 在MDN里讲的也挺明白 在这我们用到的就是blur blur 提供了图片的高斯模糊的效果 其实filter提供的就是对图像的一个滤镜效果 如果结合图像的透明度和阴影效果 那么我们就可以做出一个磨砂的效果 我们现在用到的blur 的值越大越模糊；如果没有设定值，则默认是0；这个参数可设置css长度值，但不接受百分比值 所以我们在.main里可以去加上这个属性: .main::before { content: ''; position: absolute; top: 0; right: 0; bottom: 0; left: 0; z-index: -1; margin: -20px; -webkit-filter: blur(6px); filter: blur(6px); -webkit-transition: all .2s ease-in-out, -webkit-filter .8s .22s ease-in-out; transition: all .2s ease-in-out, -webkit-filter .8s .22s ease-in-out; transition: all .2s ease-in-out, filter .8s .22s ease-in-out; transition: all .2s ease-in-out, filter .8s .22s ease-in-out, -webkit-filter .8s .22s ease-in-out; } 这里我们给出的 filter:blur(6px) 6px 的模糊度 为了对比效果 我们可以加上一个hover的效果 .main:hover::before { -webkit-filter: blur(0); filter: blur(0); } So 最终的效果: ","link":"https://GeekGhc.github.io/post/css-shi-xian-ios-mao-bo-li-de-xu-hua-xiao-guo/"},{"title":"谈谈怎么实现用户的权限管理","content":"系统的后台管理者用户的信息 其中就包括用户的角色 一个大的系统用户的角色也分很多种 最近在做后台方面的内容 索性就以laravel项目为例 谈谈怎么去实现用户的去权限管理 介绍 用户对于一个系统而言 可能存在着多种身份 举个例子来说该用户可能是会员或者普通用户 该用户对于一篇文章是作者或者是游客 其实最终设计 到的一项就是用户的Permission(也就是我们通常所说的权限) 最近自己也正在做后台方面的内容 在之前也接触了不少关于用户的权限这样的场景 现在就来谈谈在Laravel中如何更好的去实现用户的权限管理 在Laravel中有两种方式去实现: Gates和Policy(策略) 这两种方式的简单理解就是路由和控制器的作用 一个方法我们即可以在路由中直接去实现 也可以通过控制器方法去处理 简单的实例 先举一个简单的实例来说就是一个用户对一篇帖子 如果该用户不是帖子的作者 那么他的权限就会受到限制 反之作为作者可以对帖子有更多的权限 就以Laravel来说 我们先去生成一个Post Model $ php artisan make:model Post -m 在去生成相应的控制器: $ php artisan make:controller PosrController 生成好控制器之后定义好posts table字段信息: Schema::create('posts', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;integer('user_id')-&gt;unsigned(); $table-&gt;string('title'); $table-&gt;string('body'); $table-&gt;timestamps(); $table-&gt;foreign('user_id')-&gt;references('id')-&gt;on('users')-&gt;onDelete('cascade'); }); 再去生成一条测试数据 过程这里就不过多阐述 毕竟和本文内容关系不大 不过最后的结果就是用户id为1的用户创建了id为1的post 当然id为2的用户自然也就是游客的身份了 好了 接下来我们先去尝试着通过Gates的方式去定义用户的权限 在app/providers/AuthServiceProvider.php里去声明： public function boot() { $this-&gt;registerPolicies(); Gate::define('user-post', function ($user, $post) { return $user-&gt;id == $post-&gt;user_id; }); } 这里使用了Gates Facade来定义 其中用户和其帖子的权限就命名为user-post 在此之前 先去定义好我们的路由: Route::resource('posts','PostController'); 为了看见效果 我们去PostController里面的show方法去定义 public function show($id) { Auth::loginUsingId(2); $post = Post::findOrFail($id); if (Gate::denies('user-post', $post)) { abort(403,'Sorry'); }; return $post-&gt;title; } 这里我们登录id为2的用户 那么我们在浏览器会发现该用户是无法查看post的title的 反之id为1的用户可以 因为我们赋予了该用户这个权限 当然我们也是可以指定某一个用户而并不是登录的用户 if (Gate::forUser($user)-&gt;allows('update-post', $post)) { // 指定用户可以更帖子... } 我们也可以使用authorize这个方法去实现 Auth::loginUsingId(1); $post = Post::findOrFail($id); $this-&gt;authorize('user-post',$post); return $post-&gt;title; 其实除了上面的 我们还可以通过blade模板 当编写 Blade 模板时，你可能希望页面的指定部分只展示给允许授权访问给定动作的用户。例如，你可能希望只展示更新表单给有权更新帖子的用户。这种情况下，你可以直接使用 @can 和 @cannot 指令 举例来说的话我们可以在posts/show.blade.php去看下 &lt;body&gt; &lt;h1&gt;{{$post-&gt;title}}&lt;/h1&gt; @can('user-post', $post) &lt;a href=&quot;#&quot;&gt;修改文章&lt;/a&gt; @endcan &lt;/body&gt; 这样就定义了 如果该用户是帖子的作者就可以更新帖子 反之则不然 编写策略(Policy) 之前有说在laravel中可以有两种方式去管理用户的权限 其中有一个就是通过创建策略 当然首先去生成相应的策略(如果你需要基本的CURD操作 可以给一个tag即--model=xxx) $ php artisan make:polocy PostPolicy 创建完毕之后我们就会看见在app/policies下的PostPolicy这个类文件 可以想象的就是对于一个权限他必定包含了一系列的权限操作 那么在这里我们可以接着上面的来去定义一个更新的操作(这里的owns就是在User Model里面的模型判断) public function update(User $user, Post $post) { return $user-&gt;owns($post); } 创建完毕policy之后我们需要像注册一个事件 命令一样的去告诉laravel启用这个Policy 还是来到 app/providers/AuthServiceProvider.php protected $policies = [ 'App\\Model' =&gt; 'App\\Policies\\ModelPolicy', 'App\\Post' =&gt; 'App\\Policies\\PostPolicy', ]; 这样一来在PostController里面 我们可以直接去使用update这个权限协议 Auth::loginUsingId(2); $post = Post::findOrFail($id); if (Gate::denies('update', $post)) { abort(403,'Sorry'); }; return $post-&gt;title; 对于特定的用户 比如对于我们的管理员 那么管理员可以授权所有的权限 那么我们在策略里面就可以提前去声明 public function before($user, $ability) { if ($user-&gt;isSuperAdmin()) { return true; } } 这个before方法会在其他方法之前去执行 那么如果是管理员 则用户会有所有的权限 策略授权动作(Action) 之前我们在控制器里还是使用了Gates Facade去判断用户对一个帖子的权限 而在Laravel内置的User Model里面我们可以通过 can 和 cant来实现 Auth::loginUsingId(2); $post = Post::findOrFail($id); if(Auth::user()-&gt;cant('update',$post)){ abort(403,'Sorry'); } 有的时候不需要执行模型的实例 比如去创建一篇帖子 因为这个模型的实例还没有被创建 那么这个时候我们需要传递一个类名 告诉laravel使用哪种策略就行了 use App\\Post; if ($user-&gt;can('create', Post::class)) { // 执行相关策略中的「create」方法... } 策略通过中间件 Laravel 包含一个可以在请求到达路由或控制器之前就进行动作授权的中间件 Illuminate\\Auth\\Middleware\\Authorize 中间件被指定到 App\\Http\\Kernel 类中 can 键上。 我们用一个授权用户更新博帖子的例子来看看 can中间件的使用： Route::put('/post/{post}', function (Post $post) { // 当前用户可以更新帖子... })-&gt;middleware('can:update,post'); can中间件接受两个参数 第一个是需要授权的动作的名称，第二个是我们希望传递给策略方法的路由参数 当然如果是不需要指定模型实例的话 Route::post('/post', function () { // 当前用户可以创建帖子... })-&gt;middleware('can:create,App\\Post'); 就这样提供一个类名就行了 当然还有使用authorize这样的helper function 之前我们也有提到过 使用方法其实都是一样的 具体的看下文档就知道了 使用数据库来保存用户权限 首先我们可以去创建一个Model Permission $ php artisan make:model Permission -m 定义字段信息 Schema::create('permissions', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;string('name'); $table-&gt;string('label')-&gt;nullable(); $table-&gt;timestamps(); }); 还有一个就是我们的角色(Role)比如会员 普通用户 和管理员 分别担当者不同的角色 对于每一个角色他的Permission当然也就不同了 $ php artisan make:model Role -m 定义字段信息 Schema::create('roles', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;string('name');//角色的名字 $$table-&gt;string('label')-&gt;nullable(); $table-&gt;timestamps(); }); 有了users permissions roles这三张表 就可以描述用户在一个系统的权限 当然这三张表之间是一个多对多的关系 因为一个用户可以有多个角色 一个角色也可以是多个用户所共有的 这样一来我们还需要去生成他们之间的中间表 $ php artisan make:migration create_role_user_table --create=role_user 生成permission_role表 $ php artisan make:migration create_permission_role_table --create=permission_role 在每一个中间表去定义字段信息 Schema::create('role_user', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;integer('role_id')-&gt;unsigned()-&gt;index(); $table-&gt;foreign('role_id')-&gt;references('id')-&gt;on('roles')-&gt;onDelete('cascade'); $table-&gt;integer('user_id')-&gt;unsigned()-&gt;index(); $table-&gt;foreign('user_id')-&gt;references('id')-&gt;on('users')-&gt;onDelete('cascade'); $table-&gt;timestamps(); }); 接着在Model去定义三者之间的关系 当然也就是我们所说的多对多的关系了 在User Model public function roles() { return $this-&gt;belongsToMany(Role::class); } Role Model去定义和Permission的关系 public function permissions() { return $this-&gt;belongsToMany(Permission::class); } Permission Model定义和Role的关系 public function roles() { return $this-&gt;belongsToMany(Role::class); } 为了方便使用我们可以去定义一些方法 在Role Model里更加赋予角色权限 //赋予角色权限 public function givePermission(Permission $permission) { return $this-&gt;permissions()-&gt;save($permission); } 这样一来去tinker里生成一个role实例和permission实例 再去执行 $ $role-&gt;givePermission($permission) 就可以在我们的中间表role_permission里看到生成一条对应关系数据 定义完Role和Permission 现在我们可以去定义User和Role 在app/providers/AuthServiceProvider.php里去声明： public function boot() { $this-&gt;registerPolicies(); foreach ($this-&gt;getPermissions() as $permission){ Gate::define($permission-&gt;name,function($user) use ($permission){ return $user-&gt;hasRole($permission-&gt;roles); }); } } public function getPermissions() { return Permission::with('roles')-&gt;get(); } 这个即使判断用户是否是这样的一个角色 然后就可以判断该用户是否拥有该角色的权限了 这个hasRole方法我们还需要在User Model去声明 public function hasRole($role) { if(is_string($role)){ return $this-&gt;roles-&gt;contains('name',$role);//admin } //如果是collection return !! $role-&gt;intersect($this-&gt;roles)-&gt;count(); } 这里的hasRole主要就是判断$role是否是一个字符串还是一个role的collection Middleware 后台管理 其实这样的应用场景就是你必须是admin这个角色的用户才能访问后台的路由 之前我们也提到过的就是可以通过中间件的方式 来决定用户的权限 首先创建管理员这个middleware $ php artisan make:middleware MustBeAnAdmin 创建完毕之后来到这个middleware 我们主要就是去实现这个handle方法 public function handle($request, Closure $next) { //Auth::user() 用户必须是登陆并且是管理员的身份 if ($request-&gt;user() &amp;&amp; $request-&gt;user()-&gt;isAdmin()) { dd($request-&gt;user()); return $next($request); } return redirect('/'); } 在判断用户是否是管理员时可以在User Model去定义这个function public function isAdmin() { return $this-&gt;hasRole('admin'); } 当然写完我们的handle方法 我们还需要去Kernel.php注册我们这个新的middleware protected $routeMiddleware = [ 'auth' =&gt; \\Illuminate\\Auth\\Middleware\\Authenticate::class, 'auth.basic' =&gt; \\Illuminate\\Auth\\Middleware\\AuthenticateWithBasicAuth::class, 'bindings' =&gt; \\Illuminate\\Routing\\Middleware\\SubstituteBindings::class, 'can' =&gt; \\Illuminate\\Auth\\Middleware\\Authorize::class, 'guest' =&gt; \\App\\Http\\Middleware\\RedirectIfAuthenticated::class, 'throttle' =&gt; \\Illuminate\\Routing\\Middleware\\ThrottleRequests::class, 'admin' =&gt; \\App\\Http\\Middleware\\MustBeAnAdmin::class, ]; 接着我们就可以在PostController使用这个middleware 我们在他的构造函数中去指定 当然我们也是可以直接在路由中去指定的 public function __construct() { $this-&gt;middleware('admin'); } 这样的话如果用户是登陆进来的并且是admin这个角色才能访问到后台路由 不然的话就会跳转到首页 这也就实现了通过middleware来进行后台的管理操作 最后推荐几个我用过的感觉非常不错的针对用户权限的Packages Laravel Permission 目前我的项目就是用的这个Package Laravel Roles ultraware/roles ","link":"https://GeekGhc.github.io/post/tan-tan-zen-me-shi-xian-yong-hu-de-quan-xian-guan-li/"},{"title":"分享vue开发的一些有用的Tips","content":"分享一些开发vue的一些值得学习的项目和一些干货吧 介绍 尽管说开发vueJs时间不长 不过在这段时间里 也收集了不少的干货 当然也是想分享给更多的人 其实最好的还是在github上去关注一些vue的开源项目 这样自己再动手做一些小的project也是可以的 在GitHub上就有列举出有关vue的一些精彩的things 😄 先给出地址https://github.com/vuejs/awesome-vue 集合自己的需求和经历推荐 1.如果你是要部署一些静态网站和项目 出了Github Pages那么其实你还有其他的选择 而且你绝对会喜欢上它 地址: https://www.netlify.com/ Write frontend code. Push it. We handle the rest这就是他给出的标语 你写你的前端项目 剩下的交给他就好了 你注册一个账号 关联你的Github你可以直接导入你的项目 同时也提供了终端 他会帮你去生成最终的部署页面 特别是在我们开发SPA时 这无疑是一个部署项目绝佳的解决方案 UI框架 Element-UI Muse-UI iView 移动端 Mint-UI 资源库 Awesome 看看大牛都在用什么 这里能满足你很多的需求 开源项目 vue2-elm vue2+vuex开发的饿了么 vue-zhihu-daily 知乎日报 vue-wechat vue开发wechat-app vue-163-music vue开发网易云音乐 vue-cnodejs vue开发CnodeJs社区 以上都是一些很不错的开源项目 值得学习 自己的知加也算完工了 这里是我的项目地址 vue-zhijia ","link":"https://GeekGhc.github.io/post/fen-xiang-vue-kai-fa-de-yi-xie-you-yong-de-tips/"},{"title":"Laravel用户之间相互关注","content":"在一个系统或者论坛 用户之间可以相互关注 类似于Github的followers和following这样的应用场景 在laravel我们也可以去实现这样的用户与用户之间的关联 介绍 有关用户之间的相互关注这样的应用场景还是很常见的 每个平台都会有这样类似的需求 就比如Segmentfault和我们的知乎 当然还有最熟悉的Github每个人可以有关注者和粉丝 在我写的社区里也需要用到这样的需求 现在在我开发的类似知加的问答圈 因为以laravel作为后端数据 也同样会应用到这样的功能 索性就谈谈如何在laravel中去实现我们用户之间的互相关注 建立模型表 这里我们去建立一个中间表 可以想象得到的是这张表里包含了两个用户的id 我们可以去创建一个Model $ php artisan make:model Follow -m 创建完我们的表之后 我们去完善下表的字段信息 Schema::create('follows', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;integer('follower_id')-&gt;unsigned()-&gt;index(); $table-&gt;integer('followed_id')-&gt;unsigned()-&gt;index(); $table-&gt;timestamps(); }); 定义完毕之后去迁移下数据表 $ php artisan migrate 定义模型方法 写完我们的数据表 我们是将关注的信息存放在follows这个数据表的 因为这是用户与用户之间的关联 并不是之前的用户与帖子或文章这样的模型关联 其实实现的道理是一样的 //用户关注 public function followers() { return $this-&gt;belongsToMany(self::class,'follows','follower_id','followed_id')-&gt;withTimestamps(); } //用户的粉丝 public function following() { return $this-&gt;belongsToMany(self::class,'follows','followed_id','follower_id')-&gt;withTimestamps(); } //关注用户 public function followThisUser($user) { return $this-&gt;followers()-&gt;toggle($user); } 因为用户与用户之间也是一种多对多的关系 所以我将关注用户的方法写成followThisUser 定义方法路由 接下来就可以去定义相应的方法路由了 这里为了使用方便我定义了一个控制器 $ php artisan make:controller FollowController 首先我们去定义一下我们的路由 Route::post('user/follow','FollowersController@follow'); 如果用户去关注另一个用户的话 只需要去执行follow方法 而这个方法也是一个toggle式的操作 当然我们在执行 $follow = $user-&gt;followThisUser($userId) 这个方法是他会返回一个数组对象 如果是执行attach方法的话 那么$follow['attached']是$userId的值 如果这样的话我们就可以知道followThis这个方法到底是执行了attach还是detach方法了 那么接着我们就可以去增加一个用户的粉丝数这样的操作了 所以你可以在执行完成之后的逻辑是这样的 $follow = user()-&gt;followThisUser($userId); //如果用户关注了另一个用户 if(count($followed['attached'])&gt;0){ //可以去通知用户 修改用户的关注人数等数据 return response()-&gt;json(['followed' =&gt; true]); } 当然如果我们需要拿到一个用户的关注的人和粉丝的话 可以去执行 $user-&gt;following 以及 $user-&gt;followers 这样的话我们就可以拿到对应的用户数据信息了 那么我们定义完用户关注其他用户的操作 那么我们之后也可以去获取一个用户是否关注了这个用户 这个只需要返回一个bool值 我们接着去定义一下路由 Route::get('/{userId}/follow/{followedId}','FollowController@isFollow');//用户是否关注 然后去写一下对应的方法逻辑 //用户是否关注 public function isFollow($userId,$followedId) { $followedUser = User::find($followedId); $followers = $followedUser-&gt;followers()-&gt;pluck('follower_id')-&gt;toArray(); if(in_array($userId,$followers)){ return response()-&gt;json(true); } return response()-&gt;json(false); } 主要就是判断字段的follower_id是否存在我们当前的用户id并且对应的是我们所给的目标用户的id 返回值拿到后我们就可以在视图去判断 一个用户是否已经关注了这个用户 其实最好我们把关注用户这个过程放在一个组件里 这样可重用性也会更好 其实整个实现起来就和我们对一篇帖子进行点赞一样 只不过对象变成了用户与用户之间 ","link":"https://GeekGhc.github.io/post/laravel-yong-hu-zhi-jian-xiang-hu-guan-zhu/"},{"title":"Laravel API结合Dingo API和JWT","content":"在Web开发，Api开发是一项非常重要的技术，这里就以Laravel项目实例来开发API 熟悉下API的具体的流程 介绍 关于API的开发 不得不提的就是可以利用Dingo来构建更加强大的API 这样我们可以更好的去实现API认证和请求 本文基于laravist的Api教程 作为笔记参考 安装 首先当然是去安装页面 根据提供的包进行下载 在laravel项目中就是require这个package &quot;dingo/api&quot;: &quot;1.0.*@dev&quot; 接着在laravel项目的config的app.php去添加服务 'providers' =&gt; [ Dingo\\Api\\Provider\\LaravelServiceProvider::class ] 再去生成相应的配置文件 $ php artisan vendor:publish --provider=&quot;Dingo\\Api\\Provider\\LaravelServiceProvider&quot; 如果需要实现jwt 同样的也是去安装页面 安装这个package &quot;require&quot;: { &quot;tymon/jwt-auth&quot;: &quot;0.5.*&quot; } 添加对应的服务: 'Tymon\\JWTAuth\\Providers\\JWTAuthServiceProvider::class' 当然也是需要去配置一下他的alias 'JWTAuth' =&gt; Tymon\\JWTAuth\\Facades\\JWTAuth::class, 'JWTFactory' =&gt; Tymon\\JWTAuth\\Facades\\JWTFactory::class 生成配置文件 $ php artisan vendor:publish --provider=&quot;Tymon\\JWTAuth\\Providers\\JWTAuthServiceProvider&quot; 生成一个key $ php artisan jwt:generate 使用 1.这个时候我们是在开发的环境下 还需对Dingo进行相应的配置 在.env文件里 API_STANDARDS_TREE=vnd 添加前缀 API_PREFIX=api 填写版本 这个我们之前自己写测试的时候也是提供的v1以此来区别版本 API_VERSION=v1 开启Debug模式 API_DEBUG=true 一开始可以去实现一个jwt的auth认证 在config/api.php里配置 'auth' =&gt; [ 'basic'=&gt;function($app){ return new Dingo\\Api\\Auth\\Provider\\Basic($app['auth']); }, 'jwt'=&gt;function($app){ return new Dingo\\Api\\Auth\\Provider\\JWT($app['Tymon\\JWTAuth\\JWTAuth']); } ], 这样我们即实现了在Dingo的jwt认证 2.既然是auth认证我们就需要先注册刚配置好的认证 即在Kernel文件里添加 'jwt.auth'=&gt; \\Tymon\\JWTAuth\\Middleware\\GetUserFromToken::class, 'jwt.refresh'=&gt;\\Tymon\\JWTAuth\\Middleware\\RefreshToken::class, 3.添加api路由 在laravel 5.2以后的版本我们可以直接放在routes/api.php里 $api = app('Dingo\\Api\\Routing\\Router'); 为了区分开来 我们可以在app目录下新建Api目录然后在新建Controllers和在Https目录一样 在这里用来管理api的控制器 在这个目录下新建一个基本的控制器BaseController &lt;?php namespace App\\Api\\Controllers; use App\\Http\\Controllers\\Controller; use Dingo\\Api\\Routing\\Helpers; class BaseController extends Controller { use Helpers; } 所以这个时候我们再去创建对数据表的api时就可以继承这个表而使用Dingo Api的Helpers 比如在此目录下创建PostsController 这样我们就可以在routes里根据Dingo提供的方法去定义想要的api了 $api = app('Dingo\\Api\\Routing\\Router'); $api-&gt;version('v1', function ($api) { $api-&gt;group(['namespace' =&gt; 'App\\Api\\Controllers'], function ($api) { $api-&gt;get('lessons','PostsController@index'); $api-&gt;get('lessons/{id}','PostsController@show'); }); }); 在PostsController的index返回所有数据 那么再去访问http://localhost:8000/api/lessons 就可以看到所有的数据了 在这里路由的定义就是这样 这于我们之前自己写的路由方式还是不太一样的 因为这是Dingo为我们封装好的路由 当然和之前的一样 我们需要对数据字段进行映射 那么我们可以在Api目录下新建Transformer目录 然后在这个目录下新建PostTransformer &lt;?php namespace App\\Api\\Transformer; use App\\Post; use League\\Fractal\\TransformerAbstract; class PostTransformer extends TransformerAbstract { public function transform(Post $post) { return [ 'title' =&gt; $post['title'], 'content' =&gt; $post['body'], 'is_free' =&gt; (boolean)$ppost['free'] ]; } } 在这里我们是可以使用Dingo API的Transformer即TransformerAbstract 这样写完我们就可以在控制器里去重新返回所有信息 public function index() { $lessons = Post::all(); return $this-&gt;collection($post,new PostTransformer()); } 这里的PostTransformer是 App\\Api\\Transformer\\PostTransformer 当然还有之前的show方法 因为他的返回状态信息之前都是自己写的 其实在Dingo里也有相应的方法 public function show($id) { $lesson = Lesson::find($id); if(! $lesson){ return $this-&gt;response-&gt;errorNotFound('Lesson not found'); } return $this-&gt;item($lesson,new LessonTransformer()); } 结合Jwt的auth认证 在App\\Api\\Controllers目录下新建AuthController并继承之前定义好的BaseController 在jwt的创建token的页面 我们就可以使用它的authenticate方法 public function authenticate(Request $request) { // grab credentials from the request $credentials = $request-&gt;only('email','password'); try { // attempt to verify the credentials and create a token for the user if (! $token = JWTAuth::attempt($credentials)) { return response()-&gt;json(['error' =&gt; 'invalid_credentials'], 401); } } catch (JWTException $e) { // something went wrong whilst attempting to encode the token return response()-&gt;json(['error' =&gt; 'could_not_create_token'], 500); } // all good so return the token return response()-&gt;json(compact('token')); } 为了执行这个方法 可以去路由中定义 $api-&gt;version('v1', function ($api) { $api-&gt;group(['namespace' =&gt; 'App\\Api\\Controllers'], function ($api) { $api-&gt;post('user/login','AuthController@authenticate'); $api-&gt;post('user/register','AuthController@register'); }); }); 这个时候再去查看一下我们的路由的话就会看到新定义的post路由 为了验证请求的结果 我们可以使用postman这个chrome工具 去请求http://localhost:8000/api/user/login 这个时候是会返回{&quot;error&quot;:&quot;invalid_credentials&quot;} 为了能够正确通过我们可以在body部分给出用户邮箱和密码(用户可用thinker创建一个) 这个时候就会正确返回一个token 这个token就是用来保护有jwt认证下的信息 我们可以为Post的数据添加一个middleware $api-&gt;group(['middleware'=&gt;'jwt.auth'],function ($api){ $api-&gt;get('posts',PostsController@index'); $api-&gt;get('posts/{id}','PostsController@show'); }); 所以这个时候如果我们没有之前authenticate返回的token的话 我们是无法访问api/posts和api/post/{id}的 只有加上返回的token我们才能继续访问到之前的数据信息 如/api/posts?token=xxxxxx 既然只有登录的用户才能访问到这些资源 那么我们是不是也可以去拿到登录的用户 在jwt的Authentication里就提供了getAuthenticatedUser这个方法 所以为了查看效果 可以去注册一条路由 $api-&gt;group(['middleware'=&gt;'jwt.auth'],function ($api){ $api-&gt;get('user/me','AuthController@getAuthenticatedUser'); }); 接着在AuthController里去定义这个方法 public function getAuthenticatedUser() { try { if (! $user = JWTAuth::parseToken()-&gt;authenticate()) { return response()-&gt;json(['user_not_found'], 404); } } catch (TokenExpiredException $e) { return response()-&gt;json(['token_expired'], $e-&gt;getStatusCode()); } catch (TokenInvalidException $e) { return response()-&gt;json(['token_invalid'], $e-&gt;getStatusCode()); } catch (JWTException $e) { return response()-&gt;json(['token_absent'], $e-&gt;getStatusCode()); } // the token is valid and we have found the user via the sub claim return response()-&gt;json(compact('user')); } 所以说这时候去访问http://localhost:8000/api/user/me?token=xxx就可以拿到当前登录的用户信息了 相关链接 Dingo/api Dingo/api 配置页面 tymondesigns/jwt-auth ","link":"https://GeekGhc.github.io/post/laravel-api-jie-he-dingo-api-he-jwt/"},{"title":"Laravel API开发初探","content":"在Web开发，Api开发是一项非常重要的技术，这里就以Laravel项目实例来开发API 熟悉下API的具体的流程 介绍 关于API的开发 这在每个开发语言里都会有对应的开发方法 在Python里我们会用django框架去开发我们的API 在Laravel里我们会用Dingo结合JWT认证去开发我们的API 而这里我们就以Laravel项目为例 来开发我们的API 熟悉一下在 项目里是怎么去开发API 本文基于laravist的Api的开发教程 作为笔记参考 初始化数据 因为我们这里是以laravel项目，所以我们可以先去生成一些测试数据 为了方便后面的数据操作 在项目终端目录执行: $ php artisan make:model Post -m 在生成的posts table里去定义数据字段 Schema::create('posts', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;string('title'); $table-&gt;text('body'); $table-&gt;boolean('free'); $table-&gt;timestamps(); }); 接着去定义数据ModelFactory $factory-&gt;define(App\\Post::class, function (Faker\\Generator $faker) { return [ 'title' =&gt; $faker-&gt;sentence, 'body' =&gt; $faker-&gt;paragraph, 'free' =&gt; $faker-&gt;boolean() ]; }); 接着为这个model生成相应的控制器: $ php artisan make:controller PostController 接着迁移我们的数据表 $ php artisan migrate 因为之前已经定义好了ModelFactory 所以我们去生成一些测试数据 $ php artisan tinker; $ namespace App; $ factory(Post::class,40)-&gt;create() 到这里我们的测试数据就已经准备好了 路由定义 为了更好的熟悉api的路由 首先我们可以像往常web路由的定义一样去定义我们这个Post的一系列路由 Route::group(['prefix'=&gt;'api/v1'],function (){ Route::resource('posts','PostController'); }); 其实这个时候去查看一下路由 所看到的路由方式和我们的实际api开发的路由是差不多的 你也可以去看一些平台的api的文档 你会发现也都是遵循这样的规则 说到api的开发规则 现在基本也都遵循了Restful api的开发准则 网上相关的说明教程也很多这里推荐一个阮一峰的一篇文章 RESTful API 设计指南 定义好了路由我们可以尝试着自己去实现下数据的操作 比如数据的全部显示 所以在PostController的index方法中就可以直接返回所有数据(这里只做测试显示 实际开发中是不会这样暴露全部数据的) public function index() { return Post::all(); } 这样的话我们去启动服务访问http://localhost:8000/api/v1/posts这个路由的话理应是可以返回所有的数据的 当然如果是去指定的数据的话 按照api的设计要求就可以在url后面加上对应的id即可 所以这个时候我们在控制器里可以这样写 public function show($id) { $post = Post::findOrFail($id); return $post; } 同样的我们这个时候在请求这条api时就是http://localhost:8000/api/v1/posts/3 字段映射 通常的我们在请求一个服务的api时会附加一些信息 如状态码等 这些信息都是必须的 但是我们的数据库里是没有这些信息数据的 再者就是数据库里的字段我们不会想要把他们全部反馈给用户 比如我们的时间戳等用户并不需要的信息 所以说在这里我们就需要对字段进行映射 以满足我们对数据的请求 在Laravel中我们可以在Response里去完善我们的返回信息 如: public function index() { $posts = Post::all(); return \\Response::json([ 'status_code'=&gt;200, 'data' =&gt; $posts-&gt;toArray() ]); } 在选取本地的数据库字段时我们完全可以自己去实现这个方法 public function transform($posts) { return array_map(function($post){ return [ 'title' =&gt;$post['title'], 'content'=&gt;$post['body'], 'is_free'=&gt;$post['free'] ]; },$posts-&gt;toArray()); } 这里的transform function就是对字段做了映射 所以在获取信息时调用这个方法即可 $posts = Post::all(); return \\Response::json([ 'status_code'=&gt;200, 'data' =&gt; $$this-&gt;transform($posts) ]); 这样我们再去请求所有的数据时只会返回title content和is_free而这三个字段分别对应着数据表的title body和free 当然这里只是对一个Collection进行的映射 那么对一条数据呢 为了更好的重用 我们可以去分离这部分代码 所以我们在app目录下去创建Transformer目录 在这个目录里面去创建一个Transformer的抽象类 &lt;?php namespace App\\Transformer; abstract class Transformer { /** * @param $items * @return array */ public function transformCollection($items) { return array_map([$this, 'transform'], $items); } /** * @param $item * @return mixed */ public abstract function transform($item); } 这里的抽象类我们可以继承之后对相应的数据表的字段进行映射 所以这里我们对posts表的字段进行映射时 我们就去创建PostTransformer类 &lt;?php namespace App\\Transformer; class PostTransformer extends Transformer { /** * @param $item * @return array */ public function transform($item) { return [ 'title' =&gt; $item['title'], 'content' =&gt; $item['body'], 'is_free' =&gt; $item['free'] ]; } } 这样定义完毕之后再去控制器里依赖注入这个PostTransformer即可 请求错误返回 有些时候并不是所有请求都会得到正确的信息返回的 比方说错误的参数和不存在的数据 用户想要请求id为99的这条数据 但实际上并不存在这条数据 所以这时候我们会返回这种错误处理信息 对于这样的场景 我们目前能想象得到的就是判断对应的请求信息 如果正确即返回 否则返回错误信息 比如说常见的404错误 这时在定义show方法时 我们就可以加一个判断处理 public function show($id) { $post = Post::find($id); if(! $post){ return \\Response::json([ 'status_code'=&gt;404, 'message'=&gt;&quot;post not found!&quot; ]); } return \\Response::json([ 'status_code'=&gt;200, 'data' =&gt; $this-&gt;postTransformer-&gt;transform($post) ]); } 为了更好的管理返回信息的处理 我们可以把这部分代码抽离出来 $ php artisan make:controller ApiStatusCoontroller 在这个控制器里主要就是去实现各种状态信息的返回 这样我们在业务代码里只需要调用相应的状态函数即可 &lt;?php namespace App\\Http\\Controllers; use Illuminate\\Http\\Request; use App\\Http\\Requests; class ApiStatusController extends Controller { protected $statusCode = 200;//默认状态码 /** * @return int */ public function getStatusCode() { return $this-&gt;statusCode; } /** * @param int $statusCode */ public function setStatusCode($statusCode) { $this-&gt;statusCode = $statusCode; return $this; } public function responseNotFound($message = 'Not found') { return $this-&gt;setStatusCode(404)-&gt;responseError($message); } public function responseError($message) { return $this-&gt;response([ 'error'=&gt;[ 'status_code' =&gt; $this-&gt;getStatusCode(), 'message' =&gt; $message ], ]); } public function response($data) { return \\Response::json($data,$this-&gt;getStatusCode()); } } 这样一来如果我们需要返回错误处理的话 我们在目前的控制器 比如说PostController继承ApiStatusController 然后在返回错误信息时就可以直接调用 这样和之前其实是一样的 如果是正确的返回则直接调用response方法 public function show($id) { $post = Post::find($id); if(! $post){ return $this-&gt;responseNotFound(); } return $this-&gt;response([ 'status' =&gt; 'success', 'data' =&gt; $this-&gt;postTransformer-&gt;transform($lesson) ]); } ","link":"https://GeekGhc.github.io/post/laravel-api-kai-fa-chu-tan/"},{"title":"Laravel用户之间发送私信","content":"在一个系统里 用户之间可以发送私信 而这至今关系到两个用户 用户对于消息进行回复 这应该和系统之类的消息分开处理 我们可以去定义一个中间表去实现这样的业务 介绍 其实有关站内信的通知 这个在一个社区里是很常见的 在写社区项目时肯定会遇到这样的场景 因为现在又遇到了这样的应用场景 在这里还是写一下关于用户私信的发送 这样类似的消息通知对于用户登录一个系统还是社区 都是十分必要的 而这其中我们又可以细分为系统消息以及用户之间的发送的私信消息 我们需要实现的就是用户之间发送私信 用户登录后可以查看私信消息 并进行回复 而对于系统这样的站内信通知 早在5.3 就提供了notifications:table去实现 这个下次有机会准备再写一下 建立模型表 开始之前还是去新建一个存放消息的表 $ php artisan make:model Message -m 当然我们也可以去生成一个控制器去定义对应的方法 $ php artisan make:controller MessageController 数据表字段信息 在messages表中我们去完善一下字段信息: public function up() { Schema::create('messages', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;unsignedInteger('from_user_id'); $table-&gt;unsignedInteger('to_user_id'); $table-&gt;text('body'); $table-&gt;string('has_read',8)-&gt;default('F'); $table-&gt;timestamp('read_at')-&gt;nullable(); $table-&gt;timestamps(); }); } 这里的字段从字面的意思可以看得出来 这就是私信表的主体 定义模型间的对应关系 首先去定义Message Model与用户的关系 我们在Message Model定义如下: class Message extends Model { protected $table = 'messages'; protected $fillable = ['from_user_id', 'to_user_id', 'body', 'has_read','dialog_id']; public function fromUser() { return $this-&gt;belongsTo('App\\User','from_user_id'); } public function toUser() { return $this-&gt;belongsTo('App\\User','to_user_id'); } } 当然在User Model我们也要去声明和Message Model的关联 //用户---消息 public function messages() { return $this-&gt;hasMany(Message::class,'to_user_id'); } ","link":"https://GeekGhc.github.io/post/laravel-yong-hu-zhi-jian-fa-song-si-xin/"},{"title":"Laravel处理多对多模型关系","content":"在一个社区和论坛里 我们常常会遇到用户收藏帖子或文章这种应用场景 那么我们需要的就是对帖子和用户这两者之间的一种多对多的关系处理 介绍 在实际应用中 比如我们在知乎社区中 我们可以对一个帖子进行收藏 点赞的功能 这个其实就是一种多对多的关系 因为一个用户可以收藏多篇帖子 一篇帖子也可以被多个用户收藏 这边我就以用户收藏一篇帖子举例 还有一种情况就是模型的多态关联 意思就是就比如评论既可以是对帖子 也可以是对文章 也可以是对视频 这个我在之前的社区项目里就遇到过 对于这种应用场景Laravel提供多态关联这样的解决方案 而在这我们只需要处理一种多对多的关系 针对的是用户和帖子之间的关系处理 定义模型信息 1.生成中间关系表 为了定义用户与帖子之间的关系 我生成了一个Collect Model $ php artisan make:model Collect -m 2.定义中间表的字段信息 在生成Collects表后 去定义好表的字段信息 Schema::create('collects', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;integer('user_id')-&gt;unsigned()-&gt;index(); $table-&gt;integer('post_id')-&gt;unsigned()-&gt;index(); $table-&gt;timestamps(); }); 这边因为我们定义的是用户和帖子之间的关系 所以包含了各自的id值 当然其实在多对多的关系里 默认是通过 post_user 这样的中间表来决定模型间的关系的 这个当然也可以自定义 也就是这里的collects表 当然users table 和 posts table 根据具体的业务需求来定义即可 定义完毕之后 去生成我们的表 $ php artisan migrate 定义好模型表的信息 接下来就可以去各自的Model声明对应关系了 3.声明模型关系 在User Model 里面声明 //用户----帖子(收藏) public function collect() { return $this-&gt;belongsToMany(Post::class,'collects')-&gt;withTimestamps(); } //收藏帖子 public function collectThis($post) { $this-&gt;collect()-&gt;toggle($post); } 这里的collects是声明了用户和帖子之间的多对多的关系 其中两者的中间关系表就是我们定义好的collects table collectThis 方法是用户收藏帖子的操作 同样的点赞的功能我们也是用到这个方法 在多对多的模型附加和卸载时 是使用attach 和 detach方法 而toggle则是一个开关式的操作 接下来就去定义Post Model里的关系 //帖子----用户 public function user() { return $this-&gt;belongsTo('App\\User');//$post-&gt;user } //帖子----最后更新用户 public function last_user() { return $this-&gt;belongsTo('App\\User'); } //帖子---用户(收藏) public function collected() { return $this-&gt;belongsToMany(User::class,'collects')-&gt;withTimestamps(); } 这里边同时定义了一个预加载(eager load) 也就是last_user这样的一个关系 即代表帖子最后更新的用户 其实到这里就已经定义好了多对多的关系 那么接下来我们可以去生成一个控制器 写下我们收藏帖子这个方法 $ php srtisan make:controller CollectController 生成完后我们的控制器 我们去定义收藏这个操作的方法 //用户收藏帖子 public function store(Request $request) { $user = User::find($request-&gt;get(&quot;userId&quot;)); $collect = $user-&gt;collectThis($request-&gt;get(&quot;postId&quot;)); return json_encode([&quot;isCollect&quot; =&gt; true, &quot;status&quot; =&gt; &quot;true&quot;]); } 这边我们可以对任意的用户去收藏帖子 因为在collectThis 这个function 里我们已经定义好了 //收藏帖子 public function collectThis($post) { $this-&gt;collect()-&gt;toggle($post); } 其实这个方法不一定要这样写在Model里面 也可以直接在控制器方法体里面去执行这个function 但是为了更加直观和更好的重用 这样写还是很有必要的 其实我们也可以去tinker测试一下 是不是可以成功执行收藏 $ php artisan tinker; $ namespace App; 接着获取一个user $ $user = User::find(2) $ $user-&gt;collectThis(23) 这样之后再去数据库里 我们就可以看到数据成功生成了 也就是id为2的用户收藏了id为23的这篇帖子 当再次执行 $ $user-&gt;collectThis(23) 我们数据就可以撤销了 因为收藏和点赞本来就是一个开关式的操作逻辑 ","link":"https://GeekGhc.github.io/post/laravel-chu-li-duo-dui-duo-mo-xing-guan-xi/"},{"title":"Laravel模态关联处理发布帖子","content":"在一个社区或者技术圈中用户肯定需要去发表自己的帖子文章 而如果你以Laravel作为开发框架的话 他提供的模型关联就是通过定义各个对象的关系来处理这种应用场景 介绍 最近在完成自己的SPA项目时 涉及到用户的帖子发布 其实这个应用场景之前在学习laravel时就遇到过 自己在写社区这个项目时 模型之间的关联显然比我目前的需求更为复杂 在这里 我想实现的就只是用户在社区圈发布自己的帖子 这样的关系就只是帖子和用户之间 而社区这个场景的话 里面有文章 帖子 视频 特别是在评论和标签这些的关联 当然在这个项目里我肯定还是回去定义帖子 评论 用户之间的关联 而在社区里就评论而言 评论可以是对帖子 文章 视频 这也就涉及到模型间的多态关联 在这里我还是去实现模型间的一对多的关联 因为完全可以达到我的需求了 定义表字段信息 当然定义模型之间的关联之前还是得去定义模型的字段信息: 首先在User Eloquent Model里还是按照实际场景去定义 在这里我是这样的定义的: Schema::create('users', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;string('name'); $table-&gt;string('email')-&gt;unique(); $table-&gt;string('password'); $table-&gt;string('avatar');//保存用户头像 $table-&gt;string('confirm_code',64);//邮箱确认激活code $table-&gt;smallInteger('is_confirmed')-&gt;default(0);//判断用户呢是否已经激活他的邮箱 $table-&gt;integer('followers_count')-&gt;default(0); $table-&gt;integer('followings_count')-&gt;default(0); $table-&gt;rememberToken(); $table-&gt;timestamps(); }); 这个就是User Model的定义 接着去定义Post Model Schema::create('posts', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;text('body'); $table-&gt;text('html_body'); $table-&gt;integer('user_id')-&gt;unsigned();//发表帖子的用户 $table-&gt;integer('last_user_id')-&gt;unsigned();//更新帖子的用户 $table-&gt;integer('comment_count')-&gt;default(0);//评论数 $table-&gt;integer('vote_count')-&gt;default(0);//点赞数 $table-&gt;string('is_first',8)-&gt;default('F'); $table-&gt;timestamps(); }); 这里包含了一个帖子的基本信息 当然还有包括点赞 评论功能 这些我们都要去定义实现 建立模型间的关系 我们可以知道一个用户是对应着多个帖子 那么也就是一对多的关系 其实无论是文章还是和用户之间 我们更多的看到的还是一个一对多的关系 那么我们可以在User Model 去声明一下与帖子的关系: //用户----帖子 public function posts() { return $this-&gt;hasMany(Post::class);//$user-&gt;posts() } 下面就是去在Post Model声明与用户的关系: //帖子----用户 public function user() { return $this-&gt;belongsTo('App\\User');//$post-&gt;user() } //帖子----最后更新用户 public function last_user() { return $this-&gt;belongsTo('App\\User'); } 这里我声明两个关系 一个是帖子的发表用户 一个是帖子最近更新用户 当一个帖子的发布必然包含这个帖子的所有者 然后如果需要知道帖子最近被更新的用户的信息 那么我们就需要去声明这个last_user这个eager load 检验关系模型 声明完这两个模型之间的关系我们可以尝试着去生成几条测试数据 在这之前我已经完成好Model Factory 为了检验模型间的关系 我们可以去tinker去查看一下有没有拿到对应的帖子 这里我在之前已经生成了几条数据 就拿来用了 下面是Post database table: 这里是windows环境下 所以我用的还是Navicat Premium 管理数据库 因为支持多种数据库 用起来还是相当方便的 如果是Mac环境下的话 Sequel Pro 肯定是一个非常好的选择 接着去项目终端执行: $ php artisan tinerk $ namespace App; 接着去拿到id为6的user $ $user = User::find(6); 现在我们就可以看下是不是可以拿到这三条post数据: $ $user-&gt;posts 执行完后你就可以看到 成功的拿到了三条数据 这就是帖子和用户之间的关联 之后我们还会实现点赞 评论的模型之间的关联 ","link":"https://GeekGhc.github.io/post/laravel-mo-tai-guan-lian-chu-li-fa-bu-tie-zi/"},{"title":"vue-cli集成quill富文本编辑器","content":"在前端vue项目中我们会需要文本编辑器 当然我们用的比较多的是百度的Ueditor 当然我们也有其他非常好的解决方案 具体的editor组件代码我已经放在我的gist 集成Quill-editor 1.集成quill 这里是我项目里集成的文本编辑器的地址 Vue-Quill-Editor 在已经使用我们的脚手架工具初始化我们的vue项目后 我们在终端执行 $ npm install vue-quill-editor --save 接着去项目中的main.js引用 import VueQuillEditor from 'vue-quill-editor' Vue.use(VueQuillEditor) 这和我们使用vue-router是一样的 为了方便使用我在 src/components/common新建了一个QEditor.vue 接着就是去初始化我们的editor组件 data(){ return{ content: '', editorOption: { modules: { toolbar: [ [{ 'header': [1, 2, 3, 4, 5, 6, false] }], ['bold', 'italic', 'underline','strike'], [{ 'list': 'ordered'}, { 'list': 'bullet' }], [{ 'color': [] }, { 'background': [] }], ['image', 'link','code-block'] ] }, } } }, 这里的toolbar是定义自己需要的功能组件 你可以根据自己的需要来定义 具体的可以看文档定义: https://quilljs.com/docs/modules/toolbar/ 在安装初始化之后 这里是监听了change的方法 onEditorChange({ editor, html, text }) { this.content = html }, 这里的html返回的就是html文本内容 而text则是不包含html标签的纯文本内容 2.完成好后就可以去引入我们的editor组件 这里我在Start.vue里去使用的话 import QEditor from 'components/common/QEditor' 接着就是去声明我们的组件 components:{ 'qeditor':QEditor, } 最后的效果的话就是这样的(部分样式可以自己去修改) 部分资料 gist地址 Api文档 Web前端开发资源库 ","link":"https://GeekGhc.github.io/post/vue-cli-ji-cheng-quill-fu-wen-ben-bian-ji-qi/"},{"title":"vuelidate结合Laravel后端数据注册验证","content":"在前端vue项目使用基于vuelidarte验证 结合后端数据库和api数据 这样才是一个完整的注册登录以及验证流程 介绍 在实现vuelidate基本的验证之后 我们需要去真正实现项目中的注册登录以及我们的验证流程 有的具体的代码我也会放到我的gist上面 后端api以及数据准备 对于前端的请求 以laravel作为后端项目需要对数据进行处理和相应的反馈 我们可以先去创建User Model在项目终端: $ php artisan make:model User -m 生成Model后就去定义好字段信息: public function up() { Schema::create('users', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;string('name'); $table-&gt;string('email')-&gt;unique(); $table-&gt;string('password'); $table-&gt;string('avatar');//保存用户头像 $table-&gt;string('confirm_code',64);//邮箱确认激活code $table-&gt;smallInteger('is_confirmed')-&gt;default(0);//判断用户呢是否已经激活他的邮箱 $table-&gt;integer('followers_count')-&gt;default(0); $table-&gt;integer('followings_count')-&gt;default(0); $table-&gt;rememberToken(); $table-&gt;timestamps(); }); } 我们也可以尝试创建对应的Factory然后可以生成测试数据: $factory-&gt;define(App\\User::class, function (Faker\\Generator $faker) { static $password; return [ 'name' =&gt; $faker-&gt;name, 'email' =&gt; $faker-&gt;unique()-&gt;safeEmail, 'avatar' =&gt; $faker-&gt;imageUrl(256, 256), 'confirm_code' =&gt; str_random(48), 'password' =&gt; $password ?: $password = bcrypt('secret'), 'remember_token' =&gt; str_random(10), ]; }); 写好模型工厂后我们可以试着生成几个测试数据 项目终端: $ php artisan tinker; $ namespace App; $ factory(User::class,4)-&gt;create() 现在我们已经准备好测试数据 当然你也可以使用注册过后的数据 开始注册之前先去生成对应的Controller 我的laravel后端项目是5.4 而在5.4里面新增了控制器与模型的绑定 在项目终端执行: $ php artisan make:controller UserController --model=User 生成控制器后就去路由定义我们的路由方法: Route::group(['prefix'=&gt;'user','middleware'=&gt;['api','cors']], function () { Route::post('/register','UserController@store'); }); 而UserController里面的具体逻辑就是平常的注册逻辑: public function store(Request $request) { $data = [ 'avatar' =&gt; '/images/avatar/default.png', 'confirm_code' =&gt; str_random(48), ]; $user = User::create(array_merge($request-&gt;get('user'),$data)); return json_encode([&quot;user_id&quot;=&gt;$user-&gt;id,&quot;status&quot;=&gt;&quot;success&quot;]); } 这样就可以完成注册的后端处理逻辑 vue基于后端api数据进行数据检验 我们的template内容基本还是那样的判断方式 只不过在这里我对字段的唯一性的针对用户名和邮箱 所以我的具体validations是这样的: validations: { newUser:{ name: { required, minLength: minLength(4), async isUnique (value) { if (value === '') return true const response = await fetch(`http://localhost:8000/api/unique/name/${value}`) return Boolean(await response.json()) } }, email: { required, email, async isUnique (value) { if (value === '') return true const response = await fetch(`http://localhost:8000/api/unique/email/${value}`) return Boolean(await response.json()) } }, password: { required, minLength: minLength(6) }, confirm_pwd: { required, sameAsPassword: sameAs('password') } } }, 当然这只是对字段检验的要求 后端的检验路由方法: Route::group(['prefix'=&gt;'unique','middleware'=&gt;['api','cors']], function () { Route::get('/name/{value}','ValidateController@ValidateName'); Route::get('/email/{value}','ValidateController@ValidateEmail'); }); 这里我是单独生成了一个Controller去实现方法逻辑 对于用户名的检验: public function ValidateName($name) { $res = User::where(&quot;name&quot;,$name)-&gt;count(); if($res){ return response()-&gt;json(false); } return response()-&gt;json(true); } 当然对于邮箱的检验也是一样的 这些完成后我们再去前端完成我们的注册方法应该就差不多可以了 但其实并不是 因为注册的前提的是每个字段都符合要求 这样才可以去进行注册这个逻辑 所以在注册按钮添加方法: &lt;div class=&quot;control-group&quot;&gt; &lt;button class=&quot;btn btn-primary btn-lg btn-block btn-login-register&quot; @click=&quot;register($v.newUser)&quot; &gt;立即注册 &lt;/button&gt; &lt;/div&gt; 这里的$v.newUser其实就是所有被检验数据的集合 因为我的data是这样被声明的: data(){ return{ newUser: { name:'', email:'', password:'', confirm_pwd:'' }, } }, 这样完成好后 最终的注册逻辑被我放在了register function里面 register:function(value){ value.$touch();//验证所有信息 if(!value.$error){ this.axios.post('http://localhost:8000/api/user/register',{user:this.newUser}).then(response =&gt; { console.log(&quot;data = &quot;+response.data.status) }) } } 这里的value.$error是对所有字段的$error的或的结果 只有所有字段的$error为false时 才能通过检验进行下一步的注册 value.$touch() 其实是执行的设置其$dirty为true这些在文档上也都有介绍 这样我们去走一下整个的注册流程 用户注册 我们已经注册过了以为jason的用户 数据检验 资料链接 vuelidate网址 gist地址1 gist地址2 ","link":"https://GeekGhc.github.io/post/vuelidate-jie-he-laravel-hou-duan-shu-ju-zhu-ce-yan-zheng/"},{"title":"vuelidate-vueJs验证解决方案","content":"基于vueJs的轻量级的验证解决方案 在我们的登录注册以及表单提交时会是一个不错的选择 介绍 在后端项目里 比如我们的Laravel框架 对于表单验证有自己的一套validation机制 他将验证集成在FormRequest 我们只需要在我们的方法中依赖注入我们自己实例化后的验证类 当然也可以直接去在方法里去验证表单数据 而在我们的前端的项目里 也就是在我们的vue项目里 也有比较好的验证解决方案 也就是这的vuelidate 1.安装 和我们安装前端包一样 在项目终端执行: $ npm install vuelidate --save 安装完成后和我们去使用vuex一样 在main.js去引入声明这个package: import Vue from 'vue' import Vuelidate from 'vuelidate' Vue.use(Vuelidate) 当然你也可以在需要用到验证的组件里去引用一个相对小的版本: import { validationMixin } from 'vuelidate' var Component = Vue.extend({ mixins: [validationMixin], validation: { ... } }) 如果你偏好通过require这样的形式 你也可以这样引入: const { validationMixin, default: Vuelidate } = require('vuelidate') const { required, minLength } = require('vuelidate/lib/validators') 2.使用 其实使用起来真的很方便 下面举例来说就是在我的项目里的使用 1.注册验证 在用户注册时 我们通常的需要处理的表单字段就是name,email,password,confirm_pwd 首先我在Register.vue这个组件文件中把基本的样式结构写好 这取决于每个人 接着是我们对表单数据的验证: 这里是对用户名和邮箱的验证 就像之前提到的 我们先引入我们的验证规则: import { required,minLength,between,email } from 'vuelidate/lib/validators' 因为我是对一个新用户的注册 所以我定义一个data data(){ return{ newUser: { name:'', email:'', password:'', confirm_pwd:'' }, } }, 接着去定义我们的验证字段的规则: validations: { newUser:{ name: { required, minLength: minLength(2) }, email: { required,email } } }, 定义这些验证规则之后 下面是我的html部分内容 &lt;div class=&quot;control-group&quot; v-bind:class=&quot;{ 'form-group--error': $v.newUser.name.$error }&quot;&gt; &lt;label class=&quot;control-label&quot;&gt;用户名&lt;/label&gt; &lt;el-input placeholder=&quot;请输入你的用户名&quot; v-model.trim=&quot;newUser.name&quot; @input=&quot;$v.newUser.name.$touch()&quot; &gt; &lt;/el-input&gt; &lt;/div&gt; &lt;span class=&quot;form-group__message&quot; v-if=&quot;!$v.newUser.name.required&quot;&gt;用户名不能为空&lt;/span&gt; &lt;span class=&quot;form-group__message&quot; v-if=&quot;!$v.newUser.name.minLength&quot;&gt;用户名不能太短&lt;/span&gt; &lt;div class=&quot;control-group&quot; v-bind:class=&quot;{ 'form-group--error': $v.newUser.email.$error }&quot;&gt; &lt;label class=&quot;control-label&quot;&gt;邮箱&lt;/label&gt; &lt;el-input placeholder=&quot;请输入你的邮箱&quot; v-model.trim=&quot;newUser.email&quot; @input=&quot;$v.newUser.email.$touch()&quot; &gt; &lt;/el-input&gt; &lt;/div&gt; &lt;span class=&quot;form-group__message&quot; v-if=&quot;!$v.newUser.email.required&quot;&gt;邮箱不能为空&lt;/span&gt; &lt;span class=&quot;form-group__message&quot; v-if=&quot;!$v.newUser.email.email&quot;&gt;请填写正确的邮箱格式&lt;/span&gt; 每个人可以都不一样 官方文档上也给出了demo 我们先这样举例 值得注意的是我们需要去知道他的$v.name里面的内容 也就是 $invalid $dirty $error $pending $each 这个value 特别的注意 $error里的解释：It is a shorthand to $invalid &amp;&amp; $dirty 也就是一个与的组合 你可以去试着改变这两者的值 再去看$error的值 当然还有两个重要的方法: $touch $reset 上面也有实例 说简单点就是设置这个以及子节点的$dirty 为true或者false 而设置这个$dirty 再结合 $invalid就可以判断验证成功与否 $error 是由$dirty和$invalid共同决定的 在这里的验证规则流程是这样的 如果$error为true那么form-group会添加一个form-group--error这个class 只有在$error为true时 如果你不符合任意一个验证规则 例如不符合required 那么就会提示验证失败 如果验证错误就给出错误提示 这是我的错误样式: .form-group__message{ display: none; font-size: .95rem; color: #CC3333; margin-left: 10em; margin-bottom: 12px; } .form-group--error+.form-group__message { display: block; color: #CC3333; } .form-group--error input, .form-group--error input:focus, .form-group--error input:hover, .form-group--error textarea { border-color: #CC3333; } 2.密码验证 密码验证其实和上面的差不多 只不过他的验证规则时通过 sameAs 来进行验证的 3.组合验证 除了上面这些还有一个组合验证 也就是如果任意一个不符合验证规则就不通过 这个还是挺常用的 我们可以在验证字段这样去组合: validations: { flatA: { required }, flatB: { required }, forGroup: { nested: { required } }, validationGroup: ['flatA', 'flatB', 'forGroup.nested'] } 如果任意一个就是FlatA flatB forGroup其中一个不符合验证规则 那么$v.validationGroup.$error就是false 4.异步验证 特别是在验证唯一性的时候 我们肯定会遇到这样的一个场景: 就比如我们的邮箱 如果已经注册过这个邮箱了 那么再用这个邮箱去注册显然不是我们想要的 还有就是我们登录时我们需要去核对我们的用户的密码 这边我给出的实例就是对于用户名的注册 如果已经注册了就会提示已经注册过 完全支持async/await语法。它与Fetch API结合使用也很出色 那么我们可以通过后端API提供的结果可以进行判断 我们可以去增加我们唯一性的验证: name: { required, minLength: minLength(4), async isUnique (value) { if (value === '') return true const response = await fetch(`http://localhost:8000/api/unique/name/${value}`) return Boolean(await response.json()) } }, 这里我现在本地测试 通过Laravel作为后端来提供的数据校验 实际项目中的话可以再结合数据库 //用户验证路由 Route::group(['prefix'=&gt;'unique','middleware'=&gt;['api','cors']], function () { Route::get('/name/{value}',function(Request $request,$value){ if($value===&quot;gavin&quot;){ return response()-&gt;json(false); } return response()-&gt;json(true); }); }); 如果我们去注册 gavin这个用户就会提示该昵称已经被注册 因为在用户名我增加了isUnique验证 &lt;span class=&quot;form-group__message&quot; v-if=&quot;!$v.newUser.name.isUnique&quot;&gt;用户名已经被注册&lt;/span&gt; 显示结果应该是这样的: 5.自定义验证 同样的我们不仅可以使用它提供给我们的验证规则 我们也可以自定义验证规则并与之前的进行组合 官方给出了一个简单实例: export default value =&gt; { if (Array.isArray(value)) return !!value.length return value === undefined || value === null ? false : !!String(value).length } 相关网址 github地址 package官网 ","link":"https://GeekGhc.github.io/post/vuelidate-vuejs-yan-zheng-jie-jue-fang-an/"},{"title":"使用VueJs开发SPA应用","content":"对于近年来火热的SPA 我们可以有多种解决方案 现在我们可以尝试着用vueJs去开发我们的SPA应用 因为最近自己正在开发类似知加这样的一款应用 利用好自己已学的vueJs和SPA相关的知识 😝 介绍 因为最近也在学习和开发自己的SPA应用 项目开始也有一段时间了 整个过程实现起来还是会有点问题的 当然写代码这也是一个过程啦 1.集成UEditor 1.其实有很多这样的文本编辑器 例如我找到的两款: UEditor 也就是我准备引入的富文本编辑器 Quill 个人感觉还是比较好看的 😝 但我发现好像还是百度的UEditor用的比较多点 其实自己也想到就是文档资料找起来比较方便 这样处理问题也很快 当然后面的话我还是想再去集成其他的文本编辑器 尽管这款编辑器功能还是很强大的 但是我发现我并没有这么多的需求 顺便也多一种体验嘛 (功能和样式选项都是可以配置的 在下面我也会给出我的一些配置项) 官方配置文档 2.开始集成UEditor 来到我们的官网的下载http://ueditor.baidu.com/website/download.html 我下载的是 PHP最新版本的 因为后端处理还是会交给PHP去处理 下载完成后我将文件放在了根目录的static文件夹下 然后去components文件夹下我在公共目录common(这是我自己创建的)创建了Ueditor.vue组件 内容期初是这样: &lt;template&gt; &lt;div id=&quot;ueditor&quot;&gt; &lt;div ref=&quot;editor&quot; id=&quot;editor&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;/template&gt; &lt;script&gt; import '../../../static/ueditor/ueditor.config.js' import '../../../static/ueditor/ueditor.all.js' import '../../../static/ueditor/lang/zh-cn/zh-cn.js' export default{ data(){ return{ edtior: null } }, mounted() { this.editor = UE.getEditor(this.$refs.editor.id); }, beforeDestroy() { if(this.editor) { this.editor.destroy(); } }, components:{ } } &lt;/script&gt; 当然这个时候是引用不到的 还得去配置一下UEDITOR_HOME_URL(文件路径) 这个实在集成进来的UEditor下面的ueditor.config.js里面: // var URL = window.UEDITOR_HOME_URL || getUEBasePath(); var URL = window.UEDITOR_HOME_URL || &quot;/static/ueditor/&quot;; 最后可以在页面看到成功显示: 如果显示加载文件路径问题 基本上是UEDITOR_HOME_URL路径的配置出错了 3.配置富文本编辑器 先去配置下前端的内容(后端的话主要涉及到图片的上传等) 这里我集成了JellyBool在github上的一个Package https://github.com/JellyBool/simple-ueditor 对样式图标进行了一定的优化(你可以直接拿过来替换你的ueditor文件目录就行 当然还是要去配置一下前面提到的文件路径) 最后的效果大概是这样的(这里以集成到我的项目里为例) 我个人在学习Laravel时就是在JellyBool这位大神的站点Laravist学习的 给了我很大的触动也学习到了不少的知识和想法 真的觉得对于技术和学习是一件你只要愿意花时间花精力 你就会体会到你想要的充实感和快乐 Fighting!!! 😆 接下来即使最基本的登录注册过程 这个在之后我会给出了解决方案 项目总是一步一步来进行 期间肯定会遇到各种的坑 但肯定也会学习到很多 这篇也算是开个好头吧 登录注册 ","link":"https://GeekGhc.github.io/post/shi-yong-vuejs-kai-fa-spa-ying-yong/"},{"title":"git-flow开发使用姿势","content":" master分支 最为稳定功能比较完整的随时可发布的代码，即代码开发完成，经过测试，没有明显的bug，才能合并到 master 中。请注意永远不要在 maste 分支上直接开发和提交代码，以确保 master 上的代码一直可用 develop分支 用作平时开发的主分支，并一直存在，永远是功能最新最全的分支，包含所有要发布 到下一个 release 的代码，主要用于合并其他分支，比如 feature 分支； 如果修改代码，新建 feature 分支修改完再合并到 develop 分支。所有的 feature、release 分支都是从 develop 分支上拉的。 feature分支 这个分支主要是用来开发新的功能，一旦开发完成，通过测试没问题（这个测试，测试新功能没问题），我们合并回develop 分支进入下一个 release 。 release分支(预发布分支) 用于发布准备的专门分支。当开发进行到一定程度，或者说快到了既定的发布日，可以发布时，建立一个 release 分支并指定版本号(可以在 finish 的时候添加)。开发人员可以对 release 分支上的代码进行集中测试和修改bug。（这个测试，测试新功能与已有的功能是否有冲突，兼容性）全部完成经过测试没有问题后，将 release 分支上的代码合并到 master 分支和 develop 分支。 hotfix分支(bug修复分支) 用于修复线上代码的bug。当需要修复线上bug时从 master 分支上拉。完成 hotfix 后，打上 tag 我们合并回 master 和 develop 分支 这样保证了开发分支和线上分支代码肯定会一致的。 使用姿势 初始化分支 以git-flow工作流方式(phpstrom需要下载flow插件) 默认会初始化 master develop 两个主干分支。如果已有了不同分支，初始化的时候，可能需要手动指定 maste 分支 跟 develop 分支。(develop命名可稍有差异) 开发分支feature 1.分支的名称都是以 feature/*-20180102 打头，不需要做修改 如feature/register-20180102 即为用户注册的功能分支 2.基于develop分支，可以有多个功能分支进行开发 3.feature分支做完后，必须合并回develop 分支，合并完分支后一般会删除这个 feature 分支（也就是 finish 一般由测试进行，或者经过测试允许），也可以视情况保留 发布分支release 1.分支名称以 release/*-2080102 为例 2.release分支基于develop创建； 当完成一阶段的功能开发需要上线 这时创建基于develop的发布分支release，一旦创建了release分支，不能再从 develop 分支合并新的改动到 release 分支 但不会影响功能分支合并到develop,可以基于release分支进行测试和bug修改，测试不用在另外创建用于测试的分支 3.release 发布的时候，合并到 master 和 develop 分支，同时打tag，视情况删除release分支，通常应该删除掉 维护修复分支(hotfix、bugfix命名可不同) 1.分支名称以 hotfix/* 为例 如hotfix/homepage 即完成首页bug修复 2.hotfix 分支基于 master 分支创建，开发完毕后合并到 master 和 develop 分支，同时创建 tag(创建tag视团队情况而定) 3.这是唯一可以直接从 master 分支 fork出来的分支。 开发功能时基于开发分支的规范 bug修复时基于维护分支的规范即可 主干分支提交时 拉取后再提交 命令行使用 创建develop分支 git branch develop git push -u origin develop 创建开发新的feature git checkout -b 分支名称 develop # 推送功能分支到远程(可选) git push -u origin 分支名称 功能feature开发完成 # 先拉取远程develop分支 git pull origin develop git checkout develop git merge —-no-ff 分支名称 //合并开发的功能分支到develop git push origin develop git branch -d 分支名称//删除本地的功能分支 git push origin —-delete 远程分支名称//删除远程的开发的功能分支 通常名称一样 如没提交则不删 创建release分支 git checkout -b 分支名称 develop //从develop创建release分支(预发布分支) 完成release分支 // 合并release到master分支 git checkout master git merge —-no-ff 分支名称 git push //合并release分支到develop(因为可能会在release做一些测试和上线前的修复) git checkout develop git merge —-no-ff 分支名称 git push // 删除release分支 (可暂时保留 由于打上tag 一般都会做删除) git branch -d 分支名称 git branch origin —-delete 远程分支名称//删除远程的开发的功能分支 通常名称一样 如没提交则不删 git tag -a tag tag名称 master git push tags 创建hotfix分支 git checkout -b 分支名称 master 完成hotfix分支 git checkout master git merge —-no-ff 分支名称 git push git checkout develop git merge —-no-ff 分支名称 git push git branch -d 分支名称 git tag -a tag tag名称 master git push tags 注意事项 1.所有开发分支从 develop 分支拉取新的开发分支。 2.所有 hotfix 分支从 master 拉新的bug修复分支。 3.所有在 master 上的提交都必要要有 tag，方便回滚。 4.只要有合并到 master 分支的操作，都需要和 develop 分支合并下，保证同步 如维护分支的合并。 5.master 和 develop 分支是主要分支，主要分支每种类型只能有一个，辅助分支release和hotfix分支需要时创建 release预发布分支可保留 6.—-no-ff合并模式 合并分支删除后 有历史 看得出合并记录 即有merge commit记录 ","link":"https://GeekGhc.github.io/post/git-flow-kai-fa-shi-yong-zi-shi/"},{"title":"Webhook 自动部署","content":"在使用Git来管理自己的项目的时候，我们可以通过Coding的Webhook来进行一些自动的代码部署，这样每一次提交代码后就可以自动部署 省去了很多的麻烦 也提高了效率 介绍 在实现服务器的自动化的部署时 我们就可以利用到之前讲ssh免登陆服务器和免密码推送代码的那篇博文了 这里推荐两篇文章 这也是JellyBool教主所推荐的 但整个部署过程还是得自己亲自去实验体会下这个过程 至少我之前还是遇到了 不少的坑 现在有时间稍微整理一下这些内容 权当笔记和经验 密钥部署 1.当然首先还是登陆服务器 填写你的服务器ip $ ssh root@xxx.xxx.xxx.xxx 2.接着执行 填写你自己的邮箱 $ ssh-keygen -t rsa -C &quot;ghcz10@outlook.vom&quot; 一直enter下去即可 和我们在本地生成密钥是一样的 最后就是这样的 生成完毕之后我们可以去目录看一下 也就是之前存放登录公钥的文件夹下面 $ cd ~/.ssh 查看一下具体的文件 $ ls 你会看到下面的文件 authorized_keys id_rsa id_rsa.pub 3.接下来就是创建目录 $ sudo mkdir /var/www/.ssh $ sudo chown -R root:root /var/www/.ssh/ 这里的root:root是我的用户组 如果你想知道你当前的用户组的话ll一下就行了 在安正超的文章是这样的 sudo chown -R apache:apache /var/www/.ssh/ 因为他是在apache环境 我这里的服务器是 nginx 所以说视具体环境而定 接着我们需要生成一个部署公钥 之前生成的公钥是用户公钥 是进行git push等认证用户的 4.所以接下来我们去生成一个部署公钥 $ sudo -Hu root ssh-keygen -t rsa 这里我的还是root用户组 接着你会在这个/var/www/.ssh目录下有个id_rsa.pub这个部署公钥 所以我们现在可以去Coding部署这个公钥 首先拿到这个公钥 去执行 $ sudo cat /var/www/.ssh/id_rsa.pub # 查看生成的密钥内容，复制全部 5.复制完这个公钥 我们就可以去Coding 新建一个私有项目 并在部署公钥填写我们已经复制好的公钥 6.准备钩子文件 在你的www目录建立一个目录hook, 里面放上一个php文件index.php $ sudo -Hu root touch /var/www/hook/index.php &lt;?php error_reporting(1); $target = '/var/www'; // 生产环境web目录 $token = 'ispace'; $wwwUser = 'root'; $wwwGroup = 'root'; $json = json_decode(file_get_contents('php://input'), true); if (empty($json['token']) || $json['token'] !== $token) { exit('error request'); } $repo = $json['repository']['name']; $cmd = &quot;sudo -Hu www cd $target &amp;&amp; git pull&quot;; shell_exec($cmd) 确保你的hook文件可以访问：http://example.com/hook/index.php 这样钩子准备完成。 7.3.修改git配置 git config --global user.name &quot;jellybean&quot; git config --global user.email &quot;gehuachun@outlook.com&quot; # 邮箱请与conding上一致 部署公钥 1.添加用户公钥 复制上面的~/.ssh/id_rsa.pub的内容到个人设置页https://coding.net/user/setting/keys添加即可 2.复制/var/www/.ssh/id_rsa.pub的内容并添加到Coding.net公钥(这个在之前已经添加完成): 选择项目 &gt; 设置 &gt; 部署公钥 &gt; 新建 &gt; 粘贴到下面框并确认 3.添加hook 选择项目 &gt; 设置 &gt; WebHook &gt; 新建hook &gt; 粘贴你的hook/index.php所在的网址。比如:http://example.com/hook/index.php, 令牌可选，建议写上。 稍过几秒刷新页面查看hook状态，显示为绿色勾就OK了。 初始化 1.我们需要先在服务器上clone一次，以后都可以实现自动部署了： sudo -Hu www git clone https://git.coding.net/yourname/yourgit.git /home/wwwroot/website.com/ --depth=1 这个时候应该会要求你输入一次Coding的帐号和密码 ！！注意，这里初始化clone必须要用www用户 2.往Coding.net提交一次代码测试： 在本地clone的仓库执行： $ git commit -am &quot;test hook&quot; --allow-empty $ git push OK，稍过几秒，正常的话你在配置的项目目录里就会有你的项目文件了。 相关文章链接 利用WebHook实现PHP自动部署Git代码-Nginx 使用PHP脚本远程部署git项目-Apache 当然还有教主的视频课程 Laravist Coding Webhook 自动部署Git项目 ","link":"https://GeekGhc.github.io/post/webhook-zi-dong-bu-shu/"},{"title":"ssh 免密码登录服务器","content":"对于我们程序员来说 GitHub再熟悉不过了 在平常的工作项目中 对于项目的提交我们不可能每次都需要填写一次密码 当然也包括登录自己的服务器 所以我们可以使用ssh生成密钥 这样就可以省去这样的步骤 介绍 对于我们程序员来说 GitHub再熟悉不过了 在平常的工作项目中 对于项目的提交我们不可能每次都需要填写一次密码 这当然也包括登录自己的服务器 所以我们可以使用ssh生成本地的密钥 只需要向服务器或者Github给出我们的密钥 这样就可以不用每次都去填写密码 服务器的登录 在本地的目录终端执行 $ ssh-keygen -t rsa 由于我之前已经生成过 你如果是第一次的话一直enter下去 这里指定的加密算法是rsa 之后还会有一些保存私钥的路径和密码(可以为空) 最后就是生成了公钥 如果你是Mac环境的话执行 $ cd ~/.ssh 就可以看到你刚刚生成的私钥 当然如果你是Windows环境下的话还是进入(也就是管理员的目录下) $ cd ~/.ssh 而我们需要拿到的是id_rsa.pub这个公钥里面的内容 如果是服务器的话 我们需要将这个文件也放到服务器下的.ssh目录里: $ scp id-rsa.pub root@domain.com:~/.ssh/id_rsa.pub 如果服务器没有.ssh这个目录 创建一个即可 之前已经将id_rsa.pub文件放到服务器的目录下了 填写服务器的密码即可 我们再次登录服务器 我们进入到.ssh这个目录 $ cd ~/.ssh 接下来我们将这个文件保存到authorized_keys $ cat id_rsa.pub &gt;&gt; authorized_keys 这个时候退出服务器再次登录 OK 完美的登录了 Github 密钥部署 在本地的目录终端执行 $ ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 填写好你Github邮箱 如果你比较熟悉git的话之前肯定配置过git直接下一步 这样的话我们就可以在~/.ssh下有两个文件 id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 这个和之前服务器是差不多的 我们需要的就是id_rsa.pub这个公钥 我们登录Github 打开Account settings &gt;&gt; SSH Keys页面 然后，点Add SSH Key，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容 为什么GitHub需要SSH Key呢？因为GitHub需要识别出你推送的提交确实是你推送的，而不是别人的，而Git又支持SSH协议 所以，GitHub只要知道了你的公钥，就可以确认只有你自己才能推送 下面的话我们在Github新建仓库之后 只需要根据提示添加相关的仓库 接着执行 $ git push -u origin master 以后我们再去推送代码时只需要执行 $ git push 这样是不是很方便 😄 ","link":"https://GeekGhc.github.io/post/ssh-mian-mi-ma-deng-lu-fu-wu-qi/"},{"title":"ElementUI 集成组件化开发","content":"Element，一套为开发者、设计师和产品经理准备的基于 Vue 2.0 的组件库，提供了配套设计资源，使用起来还是很方便的。 介绍 寒假期间正在学习用脚手架来进行vue的组件化开发 之前的话因为是期初阶段 每一个组件用起来就感觉把一个个的盒子去拆分 特别是在打包时多多少少会有一些尴尬的事情 在接触也就是我要介绍的element组件解决方案后这个项目进行起来就很舒服了 其实用起来真的很是方便 因为本身就是基于vue 2.0开发的 知道最近在写自己的项目时想想还是记录一下 确实是一个很好的解决方案 1.安装 我们可以采取CDN以及npm安装的方式 因为我是集成在用脚手架搭建的vue项目里的 我自然选择了npm安装的方式: $ npm i element-ui -S 安装完成后会在package.json里看到已经成功安装: &quot;dependencies&quot;: { &quot;axios&quot;: &quot;^0.15.3&quot;, &quot;element-ui&quot;: &quot;^1.2.3&quot;, &quot;vue&quot;: &quot;^2.1.10&quot;, &quot;vue-axios&quot;: &quot;^1.2.2&quot;, &quot;vue-router&quot;: &quot;^2.2.0&quot;, &quot;vuex&quot;: &quot;^2.1.3&quot; }, 2.组件引入 1.在引入我们的Element时 我们可以选择完整引入和按需引入(如果你想省些事的话就完整引入吧) 完整引入时只需要在main.js里import后use声明下 这和我们去引用其他类似vue-axios是一样的 这些在官方文档里都有介绍 为实现按需引入我们需要去安装babel-plugin-component这个plugin: $ npm install babel-plugin-component -D 安装完毕后我们需要去修改一下我们的babel配置文件.babelrc: { &quot;presets&quot;: [ [&quot;es2015&quot;, { &quot;modules&quot;: false }] ], &quot;plugins&quot;: [[&quot;component&quot;, [ { &quot;libraryName&quot;: &quot;element-ui&quot;, &quot;styleLibraryName&quot;: &quot;theme-default&quot; } ]]] } 接下来就是引入我们需要的组件 具体的文档也有介绍 3.项目里使用 使用起来的话其实就和其他的UI框架差不多(也是支持响应式的) 不过你可以在组件里定义一些element特性 这里的UI框架诸如我们经常用到的BootStrap以及SemanticUI之类的 使用的话关注一些表单验证 组合的属性使用 布局 其余和我们结合文档还是很容易使用的 其他的组件UI推荐 MuseUI-https://museui.github.io/#/index IView-https://www.iviewui.com/ Mint Ui(移动端)-http://mint-ui.github.io/#!/en 相关材料链接 element组件网址 ","link":"https://GeekGhc.github.io/post/elementui-ji-cheng-zu-jian-hua-kai-fa/"},{"title":"vuex开发SPA应用","content":"在使用vueJs开发SPA应用时 官方也提供了我们解决方案就是vuex 他就是使用状态管理机制去实现数据的更新 1.安装vuex 文档:http://vuex.vuejs.org/en/ 来到我们的github上面打开我们的文档开始安装: $ npm install vuex --save 当然来到我们的main.js去引入vuex import Vuex from 'vuex' Vue.use(Vuex) 2.处理业务 1.其实最主要的我们是去理解官方所示的流程图 我们可以先从提供的demo开始: 还是去main.js文件加入官方提供的demo: const store = new Vuex.Store({ state: { count: 0 }, mutations: { increment (state) { state.count++ } } }) 还是以任务管理系统为应用场景 这时我们在去定义我们的vuex(这时最后的业务代码): const store = new Vuex.Store({ state: { todos:[], newTodo: { id: null, title: '', } }, mutations: { get_todos_list (state,todos) { state.todos = todos }, delete_todo(state,index){ state.todos.splice(index,1); }, add_todo(state,todo){ state.todos.push(todo); } }, actions:{ getTodos(store){ Vue.axios.get('http://localhost:8000/api/todos').then(response=&gt;{ store.commit('get_todos_list',response.data); }); }, removeTodo(store,payload){ console.log('pindex = '+payload.index); Vue.axios.delete('http://localhost:8000/api/todo/'+payload.todo.id+'/delete').then(response=&gt;{ store.commit('delete_todo',payload.index); }); }, saveTodo(store,todo){ Vue.axios.post('http://localhost:8000/api/todo/create',{'title':todo.title}).then(response=&gt;{ console.log(response.data); store.commit('add_todo',response.data); }); store.state.newTodo = {id: null, title: '', completed: false} } } }) 需要说明的是: **mutations**可以理解成在有了后端数据后去处理我们前端的数据内容 **actions**则是去实现方法的具体业务逻辑 他负责去与我们后端的数据进行交互 在每个component可以dispatch一个action 比如触发了一个更新操作 理解起来即使component也可以说是一个视图里去dispatc一个action , action里面就是和后端去进行交互 , 得到或者更新数据之后再commit 一个mutation 这样一来mutation就会去改变了state里面的数据 , 而一旦我们state里面的数据发生改变它就会去渲染我们的Vue Components 还有要说明的就是前后端数据肯定是统一的 那么在vuex里 后端数据的操作就是通过action去实现的 而前端的数据就是通过mutation去改变state里面的 数据 这样前后端就统一起来了 而state里面的数据是每个组件都可以访问 这和我们通常的全局变量的功能差不多 当我们的项目越来越大的时候 我们的store也会变得十分臃肿 维护起来也十分困难 那么我们可以将store分成几个module 每个module有相当于一个store 因为它有自身的 state、mutation、action、getters 举个例子来说就是 我们在App.vue里面去触发一个action: export default { name: 'app', mounted(){ this.$store.dispatch('getTodos') }, } 就是在生成页面时dispatch了一个action: getTodos getTodos(store){ Vue.axios.get('http://localhost:8000/api/todos').then(response=&gt;{ store.commit('get_todos_list',response.data); }); }, getTodos去从后端拿到了所有的任务数据 然后它commit了一个get_todos_list的mutation: get_todos_list (state,todos) { state.todos = todos }, get_todos_list将从后端得到的数据赋予给了我们state里面的todos: state: { todos:[], newTodo: { id: null, title: '', } }, state的todos的内容改变后就会去渲染我们的Vue Components 我们就可以看到页面的数据信息发生改变 这样也就完成用户和数据的交互过程 当然除了Actions ， Mutations 还有Getters 这里的Getters会从store的对象的state派生出一些其他的状态 这个理解起来可以是是对原本state的进行一个再生和过滤 比如我们需要其中的一个状态的数据长度(这也是官方给出的一个实例) getters: { // ... doneTodosCount: (state, getters) =&gt; { return getters.doneTodos.length } } 由于我们声明了这个getter 而我们是可以获取Getters 的 store.getters 对象： 因此我们在需要时可以以这种方式去得到这样的派生出来的一种数据 store.getters.doneTodos // -&gt; [{ id: 1, text: '...', done: true }] 最后还有一个核心的概念就是Modules 举个例子来说 项目里有很多的Service那么我们完全可以将这些Service分离开来 在index.js里我们就可引入其他的Modules import Vue from 'vue' import Vuex from 'vuex' import PlayService from './PlayService' import ApiService from './ApiService' import NotifyService from './NotifyService' Vue.use(Vuex) export default new Vuex.Store({ modules: { PlayService, ApiService, NotifyService } }) 这里只是其中的一个实例 对于vuex的使用 最好还是官方推荐的架构形式 以我的一个项目为例来说吧 就是将各个工作模块区分开来 我们可以直接去引用我们index.js作为入口 具体的怎么实现 我可能需要重新在一篇文章中去写到 参考资料: http://vuex.vuejs.org/en/ ","link":"https://GeekGhc.github.io/post/vuex-kai-fa-spa-ying-yong/"},{"title":"vue-router处理前端路由","content":"vue-router是Vue.js官方的路由插件，我们可以用它去处理我们前端路由,比较适合用于构建单页面应用 在构建前端vue-cli项目我们就可以利用他去实现路由的处理 1.介绍 在标准的SPA应用中 我们的前端路由都是交给我们的前端框架去处理的 vueJs官方推荐的路由处理就是我们的vue-router 2.安装 1.在命令行执行 $ npm install vue-router 安装完毕之后我们只需要import进来就可以使用 import VueRouter from 'vue-router' Vue.use(VueRouter) 如果我们初始化项目时就已经安装了vue-router 那么就会默认的在src/router下生成了index.js文件 这样我们的路由就可以在里面定义(我觉得这样去定义更加清晰 当然定义的方法都是同理的) 这样我们的路由就会交给我们的vue-router去处理 如果这个时候我们的浏览器的访问时是http://localhost:8080/#/这样的 就说明我们已经使用了vue-router 3.要点说明 以下说明我个人结合文档和自己在写项目时的体会 如有不到位还请见谅 1.路由的命名 路由的命名采取name去定义 这样在路由跳转时 可以直接提供这个路由的name 就拿官方文档的例子来说 const router = new VueRouter({ routes: [ { path: '/user/:userId', name: 'user', component: User } ] }) 这边定义了一个名字为user的路由 这样我们在链接到一个路由时就可以这样给出 &lt;router-link :to=&quot;{ name: 'user', params: { userId: 123 }}&quot;&gt;User&lt;/router-link&gt; 这里的params是以一个参数传递的 也就是userId的值 那么这样就可以跳转到名为user这个路由 而:to这样的定义和router.push是一样的 这个在下面会说到 2.首先谈一下vue-router的路由嵌套 其实官方解释起来也很简单 例如我们在实现/user展示的User这个component 而``/user/post/:id`展示的是用户的一个帖子 那么我们在之前我们路由的渲染是放在App这个入口component 我们在里面是这样定义的 &lt;template&gt; &lt;div id=&quot;app&quot;&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;/div&gt; &lt;/template&gt; 这里的 &lt;router-view&gt; 是最顶层的出口 同时我们也是可以在User component去包含 &lt;router-view&gt; 这样就相当于他的子路由就会在这里被渲染 那么我们在路由的定义的时候就可以去定义它的子路由 const router = new VueRouter({ routes: [ { path: '/user', component: User, children: [ // 当 /user 匹配成功， // Post 会被渲染在 User 的 &lt;router-view&gt; 中 { path: '', component: Post }, // ...其他子路由 ] } ] }) 3. router.push、 router.replace 和 router.go 这三者如果说区别的话 我们从字面的解释也可以看出一二 对于router.push就可以想象成一个堆栈的组合 那么我们在路由导航时定义的 &lt;router-link :to=&quot;...&quot; name=&quot;&quot;&gt;&lt;/router-link&gt; 这样的作用就等同与router.push 这样我们在访问每一个组件回退到上一个页面 其实最终的效果就是这样的 当然我们也会用到router-replace就是一个替换 他不像router-push会将之前的路由记录保存 对于router-go 作用就是js里的window.history.go(n) 我们在一开始写js跳转页面的时候肯定用到过 4. 导航钩子 这个其实说起来就是一个中间件的作用 这个在laravel我们会经常接触到 那么在vue-router里 这样的钩子可以是全局的 也可以是由单个组件 也可以是一个组件 那么就谈一下全局的路由钩子好了 其余的官方文档将的也很清楚了 首先我们在route的index.js(也就是初始化路由的index文件)去注册这样的全局性的钩子 const router = new VueRouter({ ... }) router.beforeEach((to, from, next) =&gt; { // ... }) 这三个参数的作用就是(发现文档的解释还是挺容易理解的): to: Route: 即将要进入的目标 路由对象 from: Route: 当前导航正要离开的路由 next: Function: 一定要调用该方法来 resolve 这个钩子。可以说执行了一个类似回调函数 5. 导航的数据获取 首先明白一点的是导航的数据获取可以在导航完成之前也可以是导航完成之后 这个主要取决于你所想要的用户体验 1.导航完成后获取数据 结合官方给出的例子来说就是在导航结束后 我们可以在组件created这个时间钩子去获取路由中的数据 比如在展示一个用户主页的需求时 在导航之后获取到用户的id这样就可以去服务器去请求这个id用户的数据信息 官方给出的实例是展示一个post的信息 和展示用户是一样的 在组件created这个钩子去调用fetchData方法 在fetchData方法里去请求post的数据 2.在导航完成前获取数据 在导航转入新的路由前获取数据。我们可以在接下来的组件的 beforeRouteEnter 钩子中获取数据，当数据获取成功后只调用 next 方法 我们之前也有说过钩子可以是组件级的 这样一来我们在组件的路由钩子中我们就可以去获取路由的数据 这里我们在获取数据的期间 用户也会停留在当前的界面 这个时候我们可以加一些进度条 缓冲的效果之类的 6. 路由的懒加载 官方给出的说法就是我们在打包项目时 我们可以将每个组件打包成异步加载 这样你的路由被访问的时候就高效多了 我们需要做的就是将每个组件定义成异步组件 在配置路由时依然和之前的一样 不同的是在路由的定义上 就拿我项目里的几条路由来说就是这样去定义: const Profile = r =&gt; require.ensure([], () =&gt; r(require('components/users/Profile')), 'profile') const Account = r =&gt; require.ensure([], () =&gt; r(require('components/users/Account')), 'account') 这边就定义了两个异步组件 然后我们爱配置的时候还是之前的配置 export default new Router({ mode: 'history', history: true, routes: [ {path: '/profile', name: 'profile', component: Profile}, {path: '/account', name: 'account', component: Account}, ] }) 4.结合简单业务处理 这里我们还是以一个任务管理系统为应用场景 1.下面就是定义我们的路由 前端的路由定义是交给我们的component去处理 而我们熟悉的框架路由是交给我们的视图去处理 其实可以这么理解SPA中不同的component就代替了我们传统框架中每个不同的视图 const routes = [ { path: '/', component: Todos }, { path: '/todo/:id', component: Todo ,name:'todo'} ] 就是说我们的首页就是我们的Todos这个component(之前也说了SPA应用中每个视图就是一个component) 当然我们得去引进我们的Todos和Todo component: import Todos from './components/Todos' import Todo from './components/Todo' 定义完我们的路由后我们就需要去实例化我们的: //实例化我们的router const router = new VueRouter({ routes // （缩写）相当于 routes: routes }) 2.在我们的laravel返回我们的todos数据: Route::get('/todos',function(){ $todos = Todo::all(); return $todos; })-&gt;middleware('api','cors'); 有了数据之后我们需要将数据交给我们的component 当然component也会交给我们的vue-router去处理 在vue-router官网上是这样给出的: &lt;router-view&gt;&lt;/router-view&gt; 我们之前已经定义过每个路由的指向(即每个路由都需要访问到的component) 我们可以这么理解: 当访问到一个路由时 那么这个router-view就会去访问到我们指向的component 这个router-view就是填充我们需要展示的component的 就比如访问到首页时就展示的是Todos这个component的内容 当访问到http://localhost:8000/todo/2 就会展示id为2的Todo的内容 因为我们需要渲染我们后端传过来的数据 我们需要将我们的数据传入: &lt;router-view :todos=&quot;todos&quot;&gt;&lt;/router-view&gt; 另外我们可以想象得到的是一个视图是由很多组件构成 而有个组件下面会有很多的子组件 而这个父组件(应该可以这么说吧) 就代表了我们这个视图 3.接下来就是去实现我们另一条路由 也就是/todo/:id这条路由了 其实实现起来的效果就是当我们点击某任务时就会跳转到这个任务的详情 也就是去展示这个任务的数据 我们可以先去完成好我们每个任务也就是Todo component的内容(大致就是去展示它的具体内容): &lt;template&gt; &lt;div class=&quot;todo&quot;&gt; &lt;div class=&quot;loading&quot; v-if=&quot;loading&quot;&gt; Loading... &lt;/div&gt; &lt;div v-if=&quot;error&quot; class=&quot;error&quot;&gt; {{ error }} &lt;/div&gt; &lt;div v-if=&quot;todo&quot; class=&quot;content&quot;&gt; &lt;h2&gt;{{ todo.title }}&lt;/h2&gt; &lt;/div&gt; &lt;/div&gt; &lt;/template&gt; 这是官方给出的一个实例我们加以修改让它去展示我们任务的名称就行 而在script里我们也要给出每个任务的数据(依然使用我们的vue-axios): &lt;script&gt; export default { data () { return { loading: false, todo: null, error: null } }, created () { // 组件创建完后获取数据， // 此时 data 已经被 observed 了 this.fetchData() }, watch: { // 如果路由有变化，会再次执行该方法 '$route': 'fetchData' }, methods: { fetchData () { this.error = this.todo = null this.loading = true this.axios.get(&quot;http://localhost:8000/api/todo/&quot;+this.$route.params.id).then(response=&gt;{ this.todo = response.data; console.log(response.data); this.loading = false }) } } } &lt;/script&gt; 在这边this.$route.params.id就可以获取到我们路由的id值 当然在后端我们是可以通过这个url去获取我们指定id的任务信息的: Route::get('/todo/{id}',function($id){ $data = Todo::find($id); return $data; })-&gt;middleware('api','cors'); 这些都定义好之后 我们就可以去定义我们的链接了: 我们可以去参照我们的官方文档router-link 我们在Todos component里面就可以去定义我们的链接: &lt;router-link :to=&quot;{ name: 'todo', params: { id: todo.id }}&quot;&gt;{{todo.title}}&lt;/router-link&gt; 这里的name我们是可以在定义路由时给出的: const routes = [ { path: '/', component: Todos }, { path: '/todo/:id', component: Todo ,name:'todo'} ] 这个就和我们laravel里面通过use的路由命名是一个意思 这样我们就可以对应到我们的这个路由了 这个时候我们把去查看页面的链接时就会发现是诸如这样的http://localhost:8080/#/todo/2 我们点击链接之后就会跳转到我们Todo这个component的内容页面 当然这个时候我们会发现url都是http://localhost:8080/todo/2这样的形式 如果你想去掉# 我们可以在定义路由时在这样去定义 const router = new VueRouter({ mode: 'history', history: true, routes // （缩写）相当于 routes: routes }) 参考资料: https://router.vuejs.org/zh-cn/ ","link":"https://GeekGhc.github.io/post/vue-router-chu-li-qian-duan-lu-you/"},{"title":"Laravel Composer Package 开发简明教程","content":"在Laravel中有Package Development 扩展包是添加功能到 Laravel 的主要方式。扩展包可以包含许多好用的功能。在开发扩展包之前 我们需要了解Service Providers 这样我们只需要在composer.json里去包含这个package并进行相应的配置即可 介绍 在Laravel中就有Laravel Composer Package开发的相关介绍 这其中需要运用 Service Providers 当然对于我们Laravel开发人员 来说 开发一个扩展包还是很值得学习的 现在就来开发一个消息通知的扩展包 扩展包的地址:https://github.com/GeekGhc/LaraFlash 整个packages参照 Jeffrey Way的Flash Packages 新建包 在生成好的Laravel项目中新建packages目录(和app同级) 接着在packages目录下新建包目录 packages/geekghc/laraflash 我们需要去laravel项目下去声明包的命名空间: &quot;autoload&quot;: { &quot;classmap&quot;: [ &quot;database&quot; ], &quot;psr-4&quot;: { &quot;App\\\\&quot;: &quot;app/&quot;, &quot;GeekGhc\\\\LaraFlash\\\\&quot;:&quot;packages/geekghc/laraflash/src/&quot; } }, 声明完毕之后别忘了去执行重新生成autoload文件 $ composer dump-autoload 我们需要新建src目录来存放我们的源文件 接着因为我们是开发一个扩展包 之后还需要进行测试开发 所以我们去生成一个composer.json文件 $ composer init 填写完基本信息之后 在packages/geekghc/laraflash目录下就会生成一个composer.json文件: 我先给出 { &quot;name&quot;: &quot;geekghc/flash&quot;, &quot;description&quot;: &quot;flash for laravel&quot;, &quot;license&quot;: &quot;MIT&quot;, &quot;authors&quot;: [ { &quot;name&quot;: &quot;GeekGhc&quot;, &quot;email&quot;: &quot;ghcgavin@sina.com&quot; } ], &quot;minimum-stability&quot;: &quot;dev&quot;, &quot;require&quot;: { &quot;php&quot;: &quot;&gt;=5.5.9&quot;, }, &quot;require-dev&quot;: { &quot;phpunit/phpunit&quot;: &quot;~5.7&quot;, &quot;mockery/mockery&quot;: &quot;0.9.*&quot; }, &quot;autoload&quot;: { &quot;psr-0&quot;: { &quot;GeekGhc\\\\LaraFlash&quot;: &quot;src/&quot; }, &quot;files&quot;: [ &quot;src/GeekGhc/LaraFlash/function.php&quot; ] } } 完成好composer.json后 我们可以去src/GeekGhc/LaraFlash目录下新建一个Flash.php &lt;?php namespace GeekGhc\\LaraFlash; use Illuminate\\Support\\Facades\\Facade; class Flash extends Facade { public static function getFacadeAccessor() { return 'laraflash'; } } 我们这里继承了Facade类，用Facades可以访问IoC容器中注册的类 这样我们就可以去调用注册的类 同时我们需要去新建一个Service Provider $ php artisan make:provider FlashProvider 将生成的 app/Providers/FlashProvider.php 文件移动到我们的 packages/geekghc/laraflash/src/GeekGhc/LaraFlash/ 目录下面， 并注册 FlashProvider 到 config/app.php 在FlsahProvider里面我们去写一下之后我们需要绑定的类: public function register() { $this-&gt;app-&gt;bind( 'GeekGhc\\LaraFlash\\SessionStore', 'GeekGhc\\LaraFlash\\LaravelSessionStore' ); $this-&gt;app-&gt;singleton('laraflash',function(){ return $this-&gt;app-&gt;make('GeekGhc\\LaraFlash\\FlashNotifier'); }); } 这边我们就绑定了封装好的SessionStore 之后我们去配置一下视图的路径 public function boot() { $this-&gt;loadViewsFrom(__DIR__.'/../../views','laraflash'); $this-&gt;publishes([ __DIR__.'/../../views'=&gt;base_path('resources/views/vendor/laraFlash'), ]); } 这里我们就发布了我们的视图文件 在执行 $ php artisan vendor:publish 我们就可以在laravel项目的resources/views/vendor/laraFlash去自定义最后的样式效果 在config/app.php去注册我们的服务 'providers' =&gt; [ GeekGhc\\LaraFlash\\MyFlashProvider::class, ]; 为了方便使用 可以再去添加一个alias 'aliases' =&gt; [ 'LaraFlash'=&gt;GeekGhc\\LaraFlash\\Flash::class, ]; 接着我们可以去实现flash的主要功能服务 每个包的功能都根据需求而来 这里也不多做介绍 最后的目录结构是这样的 | |—— packages | |—— geekghc | |—— laraflash | |—— src 源文件 | |—— GeekGhc 源文件 | |—— LaraFlash | |—— Flash.php | |—— FlashNotifier.php | |—— function.php | |—— FlashProvider.php | |—— SessionStore.php | |—— LaravelSessionStore.php | |—— views 视图文件 | |—— tests 测试目录 | |—— vendor 测试需要的包 | |—— .gitignore | |—— composer.json | |—— composer.lock | |—— phpunit.xml | |—— readme.md 这样的话 我们就在本地写好了扩展包 我们其实可以去创建一个控制器去测试我们这个包是否正常 在视图home.blade.php我们就可以去包含views里面的视图文件 @include('laraflash::notification') 或者 @include('laraflash::header-notification') 接着在控制器去使用类似这样的形式: LaraFlash::success('Message') LaraFlash::info('Message') LaraFlash::error('Message') LaraFlash::warning('Message') 包的具体使用还是去github看一下具体使用就知道了 最后的效果大概就是这样的: 显示正常之后我们就可以去发布我们的package了 在github先创建一个仓库 当然我这里的就是创建了LaraFlash这个远程仓库 紧接着我们去推好我们的代码到github 接着我们需要去仓库的setting =&gt; Intergrations&amp;services添加Packagist服务(填写好用户名和Token) 添加完毕之后去Packagist Submit这个仓库(提供远程仓库的地址) 在github进入packagist测试通过之后就ok了 因为我们之前定义的dev版本 如果后期有人提出了一些issues你去修改了自己的package 那么我们会去增加别的tag 也是就是说你修改package之后 再去添加一个tag: $ git tag 2.0 -a 填写说明信息后 推送这个tag: $ git push --tags 这样一来我们就发布了v2.0这个版本 这就是我们发布扩展包大概流程 博客文章地址Laravel Package 参考资料 Laravel Composer Package 开发简明教程 Laravel 的扩展插件开发指南 Laravel Composer Package实战 Laravel Package Development ","link":"https://GeekGhc.github.io/post/laravel-composer-package-kai-fa-jian-ming-jiao-cheng/"},{"title":"Laravel console command","content":"Laravel 为我们提供了不同的命令行创建方式，这样一来，我们可以向定义路由一样创建自定义的命令行 这在我们项目发布的初始化的时候 非常的方便和必要 开始 在**laravel(5.3)**项目里routes文件夹下包含了需要的路由的定义 当然还有就是定义我们的command 也就是console.php这里面的定义和路由的 定义其实还是非常的象的 在console.php里我们是可以看到: Artisan::command('inspire', function () { $this-&gt;comment(Inspiring::quote()); })-&gt;describe('Display an inspiring quote'); 这里就是定义了一个inspire 命令 我们可以去命令行去执行: $ php artisan inspire 这里其实就会随机返回给我们一句名言警句 在这里去我们就可以自定义一个简单的命令 我们去简单声明一下 Artisan::command('hello', function () { $this-&gt;comment(&quot;Hello!!&quot;); }); 现在再去命令行执行 $ php artisan 那么我们就会看到有php artisan hello这个命令了 我们而是可以是去传递参数的 比方说是这样: Artisan::command('hello {name}', function () { $this-&gt;comment(&quot;Hello &quot;.$this-&gt;argument('name')); })-&gt;describe('Say Hello to somebody'); 如果想去为这条命令生成一个说明的话可以直接在后面加上describe(就像之前的那样) 和很多的命令一样 如果对命令的参数或其他有疑问的话 就可以借助help这个tag去查看 $ php artisan help hello 自定义 Artisan 命令 之前基本熟悉了artisan 命令的作用 那么现在我们可以自己去自定义一个命令来实现更多的业务需求 我们在命令行执行: $ php artisan make:command InstallProject 特别的在5.1之前的版本是执行php artisan make:cnsole xxxx 生成的文件就在App\\Console\\Command下 其中我们需要注意的是 protected $signature = 'command:name'; 定义了我们需要在命令行输入的命令 举个例子来说如果是一个Blog项目 那么我们就可以这样去定义 protected $signature = 'blog:install'; 定义完命令之后 下面的handle function就负责我们最后的输出 这里先做一个小的测试 就是去打印一些信息提示: public function handle() { $this-&gt;info(&quot;This is test data&quot;); } 会后我们会需要去Kernel文件去注册我们刚定义好的command protected $commands = [ \\App\\Console\\Commands\\InstallProject::class ]; 写好之后再去执行: $ php artisan 就会看到我们刚定义好的命令了 我们可以去执行一下这个命令是可以看到打印出来的数据的 $ php artisan blog:install 和之前在路由里面去定义我们的命令一样 我们同样可以去传递参数 protected $signature = 'blog:install {name}'; 在去修改一下handle方法 public function handle() { $this-&gt;info(&quot;This is test data and name is &quot;.$this-&gt;argument('name')); } 如果需要可选参数的话就和路由的要求一样 只需要在name去声明 protected $signature = 'blog:install {name?}'; 实际项目应用 1.更多的时候我们可能需要在项目初始化时需要用户手动去提交一些信息 比如用户名 邮箱等等 这样的应用场景的话我们就需要一种交互式输入 先来一个简单的栗子 😄 public function handle() { $name = $this-&gt;ask('请输入你的名字?'); $email = $this-&gt;ask('请输入你的邮箱?'); $data = [ 'name' =&gt; $name, 'email' =&gt; $email, ]; $this-&gt;register($data); } 然后就可以去定义这个register function public function register($data) { $this-&gt;info($data['name']); } 这样的话就可以拿到用户输入的数据的话 这样看的话好像用处不到 其实在拿到这组数据之后我们就可以进行验证 并去创建一个用户数据 特别是对一个后台 这样去创建一个管理员还是有必要的 其实这样的交互式输入还有很多 比如给用户的选择 具体的看下文档就明白了 2.调用其他命令 其实这个实现起来也很容易 也就是去调用call这个方法 public function handle() { $this-&gt;call('key:generate'); $this-&gt;call('migrate'); $this-&gt;info('数据表创建成功'); } 当然前提是数据库是对应的你对应的数据库 如果在homestead下开发 这些也就不用管他了 ","link":"https://GeekGhc.github.io/post/laravel-console-command/"},{"title":"VueJs处理后端Laravel Api","content":"用vueJs实现组件化时，我们可以在项目中引入后端API的概念，在这里我们就是去处理利用Laravel提供的后端API去实现我们通常说的前后端分离 1.后端提供API 在用vue-cli构建好我们的项目 并且实现vueJs组件化.下面我们可以去尝试从后端的提供的API来获取我们前端需要的数据 因为我们的数据不可能都是预先写好在我们的组件里的 而后端的话就用我熟悉的Laravel项目作为Api的提供者 这样一来也就实现了我们通常所说的前后端分离 假设这样的一个应用场景就是任务管理系统(想来想去感觉还是这个应用场景比较容易理解): 我们需要： Todo component(每一个任务 当然也可以执行删除和已完成的状态) Todos component(所有任务) TodoForm component(添加任务) 在后端我们可以执行 $ php artisan make:model Todo -m 去生成我们的任务表 然后在数据库里生成一定的测试数据 其中todos表的结构是这样的: Schema::create('todos', function (Blueprint $table) { $table-&gt;increments('id'); $table-&gt;string('title'); $table-&gt;boolean('completed')-&gt;default(false); $table-&gt;timestamps(); }); 定义好表的内容结构之后 我们去完成一些我们的Factory: $factory-&gt;define(App\\Todo::class, function (Faker\\Generator $faker) { return [ 'title' =&gt; $faker-&gt;paragraph, ]; }); 当然不要忘了在Todo model里去声明我们的fillable: class Todo extends Model { protected $fillable = ['title']; } 完成之后去用我们的tinker去生成大概10条数据(这里就不作介绍) 生成完毕我们的数据之后我们需要在routes的api.php里给出我们的数据: Route::get('/todos',function(){ $todos = Todo::all(); return $todos; })-&gt;middleware('api'); 有了数据之后我们在前端VueJs项目的组件里就要去获取我们提供的api里的数据 这里就是我们的App.vue里需要我们后端提供的数据: export default{ computed: { //todos就需要我们后端传过来的数据 todos () { ... } }, } 这样才算完成我们前后端的一个流程 😆 2.前端去处理后端数据 1.后端的数据已经准备好 这时我们的vueJs项目就需要发起http请求去得到我们的数据 vueJs之前官方推荐的是使用vue-resource 但是后来作者有给我们推荐了vue-axios 使用这个package的话我们去执行: 如果你只需要在vueJs项目区使用axios的话就使用这个package 这个在之前的package做了一定的vue兼容 https://github.com/imcvampire/vue-axios $ npm install --save axios vue-axios 安装完成之后我们就可以在我们的main.js去添加配置: import axios from 'axios' import VueAxios from 'vue-axios' //去声明下这个package Vue.use(VueAxios, axios) 现在我们就可以去App.vue里的mounted使用我们刚刚引入的package： export default { name: 'app', mounted(){ this.axios.get('http://localhost:8000/api/todos').then(response =&gt; { //获取我们后端api的数据(response.data) ... }) }, components: { Hello } } 至于this.axios.get()这个用法其实和我们vue-resource的tihs.$http.get()的用法是差不多的 而这个package在github上也给出了用法 这个时候我们发现并不能成功获取到我们的数据 是因为涉及到跨域这个问题 当然在laravel也有package给出了解决方案 https://github.com/barryvdh/laravel-cors 这个package就是去解决laravel作为后端api跨域的问题 安装的话参照github上的步骤就行了 安装完毕之后我们在路由上需要去添加我们的cors这个middleware: Route::get('/todos',function(){ $todos = Todo::all(); return $todos; })-&gt;middleware('api','cors'); 添加完毕之后再去刷新我们的浏览器 我们发现数据已经成功获取到了 他会返回相应的我们之前生成的10条测试数据的object 2.拿到我们的数据后我们就去在vueJs项目中去展示 我们去初始化我们的todos为一个空的数组 在拿到后端数据后再赋予给我们的todos: export default { name: 'app', data(){ todos:[] }, mounted(){ this.axios.get('http://localhost:8000/api/todos').then(response =&gt; { this.todos = response.data }) }, components: { Hello } } 已经拿到我们后端的API数据 并且已经存放在todos里 接下来怎么去展示就是看业务的要求了 到这我们其实就实现了以vueJs作为前端项目 laravel作为后端API 的前后端分离这种开发模式了 这样的话我们只需要后端去提供API 前端去请求我们的API去获取所需要的数据并在前端进行渲染展示 我们后端只需要去关心前端需要什么的数据并去提供这个API 而我们的前端则是要去关心怎么渲染这些数据 当然这只是获取到我们所有的数据 我们的业务要求肯定远不止这些 比如我们对数据的CURD这些其实我们都可以通过类似的 形式去实现 但总的来说我们前后端的业务就是这样的一个处理流程 ","link":"https://GeekGhc.github.io/post/vuejs-chu-li-hou-duan-laravel-api/"},{"title":"Vue使用脚手架进行组件化开发","content":"Vuejs 脚手架工具是 Vuejs 官方提供的一个命令行操作工具，我们可以初始化一个 Vuejs 项目来进行组件化开发 1.vue-cli安装 1.首先确保你已经正确安装了nodejs环境以及git 然后可以全局方式安装vue-cli： vue-cli github地址https://github.com/vuejs/vue-cli 当然对node 和 npm 版本也有一定要求 $ npm install -g vue-cli 安装完毕之后我们可以在命令行执行一个vue的命令: $ vue 2.初始化项目 $ vue init &lt;template-name&gt; &lt;project-name&gt; 现在我们开始创建我们的项目: $ vue init webpack my-project 这里的webpack是指我们生成的project是使用webpack构建工具 当然也会有其他的选择 这在github上已经给出了: webpack - A full-featured Webpack + vue-loader setup with hot reload, linting, testing &amp; css extraction. webpack-simple - A simple Webpack + vue-loader setup for quick prototyping. browserify - A full-featured Browserify + vueify setup with hot-reload, linting &amp; unit testing. browserify-simple - A simple Browserify + vueify setup for quick prototyping. simple - The simplest possible Vue setup in a single HTML file 在生成项目时我们都选择默认 同时也需要注意一下几点: 特别的在Runtime + Compiler和 Runtime-only时我们还是选择Runtime + Compiler模式 在选择是否需要安装vue-router时 选择N为的是一开始我们更容易去学习和理解 然后会问你是否需要ESLint, 因为我们是刚开始我们选择N(因为ESLint语法要求比较严格) unit tests我们也不需要了 接下来我们进入到我们的项目my-peoject执行npm install以及我们的 $ npm run dev 如果你在npm install下载的时候发生错误 大都需要你升级nodejs和npm到最新的版本 之前我们也提到过就是在官方给出就是对版本也有一定要求 执行完npm run dev会启动一个服务器 并运行在我们的8080端口: 用phpstrom打开我们的项目 可以看到我们的文件目录(而我们最需要关注的就是我们的src目录) 2.vue-cli 项目结构 1.src目录是我们的组件存放等文件的目录 在main.js里定义了我们vueJs的一个实例: import Vue from 'vue' import App from './App' import router from './router' /* eslint-disable no-new */ new Vue({ el: '#app', router, template: '&lt;App/&gt;', components: { App } }) router是我在初始化项目时确认安装的 当然这在之后也可以进行安装引入 我们在接下来会用到这个(不过我觉得初始化时不用去安装vue-router 这样刚接触时会更容易理解) 除此之外我们可以看到这个vue实例包含了一个App template 而这个template就在我们的同级目录下的App.vue 来到我们的App.vue 可以看到在上面其实就是定义了一个template 下面是一个script标签用来指明这个template 然后就是我们的style标签定义的样式 可以想象得到的是因为我们我们是使用的webpack这个构建工具 所以template这个是交由我们的html loader去处理的 而script里的内容是由script loader style标签里的内容是交由我们的style loader去处理的 在App.vue这个template里引入了在同级目录components下的Hello组件 它里面的内容就是我们一开始8080端口所看到的内容 在这之后我们就可以往用vue-cli构建的项目里面去添加我们需要的组件内容了 假使我们需要添加一个Todo组件我们就可以在src/components下去创建Todo.vue 他的具体内容当然是根据自己的业务去写了 每个组件的组成形式也就是我们之前提到的Hello.vue这样的(学习过vueJs的应该很容易就能理解这些文件的内容了) 用vue-cli去构建我们的vueJs组件化开发大概就是这样 当然在这之后我们可以去利用vueJs官方推荐的vue-router 去构建我们的SPA(单页面应用) ","link":"https://GeekGhc.github.io/post/vue-shi-yong-jiao-shou-jia-jin-xing-zu-jian-hua-kai-fa/"},{"title":"Webpack基本使用","content":"webPack作为目前非常受欢迎的前端构建工具,相比于gulp和require构建工具自然有他存在的优势 1.webpack安装 1.首先确保你已经正确安装了nodejs环境。然后可以全局方式安装webpack: $ npm install -g webpack 我们可以检查一下webpack版本 $ webpack -v 这样就完成了对全局的安装 2.接下来我们可以在根目录下创建一个index.html并引入一个app.js 接着可以在根目录下的js文件夹下创建part-one.js和part-two.js以及我们的entry.js 其中entry.js里我们引入其他两个js文件 require('./part-one.js') require('./part-two.js') 当然这个时候如果我们引入entry.js是没有用的 require这种写法是类似nodejs里服务端里的写法 我们终端运行 $ webpack js/entry.js app.js 这里的app.js就相当于我们的入口文件 这个时候我们在index.html里引入我们的app.js就可以成功执行到js文件夹下的part-one.js和part-two.js里的业务逻辑了 和npm的package.json composer的composer.json bower的bower.json 一样 webpack也有自己的相应的配置文件 执行： $ touch webpack.config.js 2.webpack配置使用 1.在webpack配置文件里进行相应的配置 这是一个简单的配置: module.exports = { devtool: &quot;sourcemap&quot;, entry: &quot;./js/entry.js&quot;, output: { filename: &quot;app.js&quot; } } sourcemap 指定了生成文件间的对应关系 entry.js 指明了入口文件 app.js 指明了最后打包生成的文件 回到我们的命令行我们就可以直接执行 $ webpack 这样就生成了我们需要的app.js文件 当然还有我们的map文件 在这里指明了文件之间的映射关系 当然这个时候我们需要更多的业务更多的需求 如我们需要引入jquery这样的库 其实这时候也和gulp差不多 我们在命令行中执行: $ npm init 来生成我们的package.json 接下来我们开始引入jquery 命令行中执行 $ npm install jquery --save-dev 接下来我们就可以在我们的js文件里使用jquery了 var $ = require('jquery'); $(&quot;p&quot;).css(&quot;background-color&quot;,&quot;yellow&quot;); 2.webpack Loader机制 1.和其他构件工具一样webpack也可以将我们的静态文件进行打包 而有所不同的是 webpack采用loader机制将静态文件打包到一个js文件再通过不同loader进行加载使用 这样的话我们在项目里只需要加载一个js文件就可以达到应用的目的了 2.下面开始下载我们所需的loader 首先我们可以下载我们对样式处理的loader $ npm install css-loader style-loader --save-dev 在我们先前的配置文件webpack.config.json里我们就需要声明loader对应的处理 module: {//在配置文件里添加JSON loader loaders: [ { test: /\\.css$/, loader: &quot;style!css&quot; } ] }, 配置里其实也就是通过正则表达式来匹配.css的文件 并且用css loader和style loader来进行处理 当然我们这时需要在入口文件entry.js里包含我们的css文件， 假使我们在根目录先的css文件夹创建了一个style.css， 我们需要在entry.js文件里包含进这个css文件 require(&quot;../css/style.css&quot;) 来到命令行执行: $ webpack 我们就会看到style.css里的样式已经成功应用到我们的页面上了 3.将js交给我们的babel来进行处理 下载相关的loader： $ npm install babel-core babel-loader babel-plugin-transform-runtime babel-preset-es2015 babel-preset-stage-0 babel-runtime --save-dev 下载完成之后在webpack.config.json里进行配置来进行babel编译 babel: { presets: ['es2015','stage-0'], plugins: ['transform-runtime'] } 接着就是添加对js文件的loader module: {//在配置文件里添加JSON loader loaders: [ { test: /\\.css$/, loader: &quot;style!css&quot; }, { test: /\\.js$/, loader:&quot;babel&quot;, //忽略掉node_modules exclude: /node_modules/ }, { test:/\\.vue$/, loader:&quot;vue&quot;, } ] }, 在这里也给出了对vue文件的loader处理 因为接下来我们还是要借助这个来进行vueJs的组件化开发 这样一来我们就可以在我们的js文件里使用babel语法 同样的我们也是在entry.js里去引入我们的jquery 这时我们就可以通过import进来 这在vueJs组件化开发时也是很常见的 import $ from 'jquery' //如果你要引入Vue的话 import Vue from 'vue'; 3.webpack 进行Vue的组件化开发 1.下载相关的package $ npm install vue vue-loader vue-html-loader vue-style-loader --save-dev 安装完毕后 我们去配置我们的loader(这在之前已经给出了) { test:/\\.vue$/, loader:&quot;vue&quot;, } 2.创建我们的vue组件 在根目录的js/components文件夹下创建hheading.vue 具体内容: &lt;template&gt; &lt;div&gt; {{ message }} &lt;/div&gt; &lt;/template&gt; &lt;script&gt; export default{ data(){ return{ message:'hello vueJs' } }, } &lt;/script&gt; 写完后我们还需要在之前的入口文件entry.js里包含进来这个vue组件 import Heading from &quot;./components/heading.vue&quot; //初始化一个vue 需要在视图文件里指定我们的app new Vue({ el:'#app', components:{Heading} /* * 当然我们也可以在初始化之前这样注册 * Vue.component('Heading',require('./components/heading.vue')) */ }) 这时在视图里我们就可以指定我们的组件了: &lt;div id=&quot;app&quot;&gt; &lt;Heading&gt;&lt;/Heading&gt; &lt;/div&gt; 命令行再次执行 $ webpack 当然我们的vue由于是通过npm安装的 我们他会给出一个错误就是Failed to mount component 如果我们需要使用常规模式 这需要我们通过一个script标签进行引入或者添加一个配置 resolve: { alias: { 'vue$': 'vue/dist/vue.js' } }, 再去执行webpack就可以看到hello vueJs了 说明我们成功引入了vue组件 4.webpack hot reload(热加载) 1.webpack 有用的flag webpack --display-modules: 你可以看到各个modules的情况 webpack --display-modules --display-reasons: 除此之外我们还可以清楚看出每个module的包含情况 webpack -p: 这会对打包的文件进行优化和压缩 特别的这对我们线上部署是很有用的 webpack -w(webpack --watch): 这会执行一个watch的状态 不用我们每次修改文件之后再回来执行webpack 这和gulp watch是一个道理 2.webpack在watch机制上引进了hot reload机制 在引入hot reload 机制后我们不仅不需要再次执行webpack 每次修改文件后也不用刷新我们的浏览器 这对每个开发人员来说肯定是非常好的事情 开始安装: $ npm install webpack-dev-server -g 安装完毕之后理论上我们就可以使用了: 这时候我们并不是去执行webpack 而是去命令行执行: $ webpack-dev-server --inline --hot webpack-dev-server会启动一个web服务器 默认端口是8080而--hot则是代表我们去执行一个热加载 如果没有启动成功 你需要考虑下有没有其他程序占用这个8080端口 这个就和我们最常见的80端口被占用是一样的 在这之后如果你去修改你的诸如js文件 css文件 vue文件 那么浏览器会同时执行了修改 这真的是件非常cool的事情 5.webpack 插件配置 1.针对线上还是线下环境的处理 和很多框架一样(如我们的laravel框架 会在.env读取对是否是线上还是线下的变量)在开发和上线是两种不同的工作环境 而在webpack里面是根据env.NODE_ENV进行判断的 这里会有production和local两个选择 我们可以这样获取(这就和我们框架中debug模式是否开启是一样的) var debug = process.env.NODE_ENV !== &quot;production&quot;; 有了我们的debug变量我们就可以获取到是否是线下环境 然后在我们的webpack.config.json可以这样写： module.exports = { devtool: debug?&quot;sourcemap&quot;:null, ... } 这个就是如果是线下环境我们就生成一个sourcemap如果是线上环境就不需要 2.plugins(插件)配置 我们在配置我们的插件的时候我们就需要把我们的插件放到我们的webpack.config.json一个plugins声明当中： module.exports = { devtool: debug?&quot;sourcemap&quot;:null, ... //这些是你需要一些plugins plugins:debug ? [] : [ new webpack.optimize.DedupePlugin(), new webpack.optimize.OccurenceOrderPlugin(), ... ] } 这里是webpack的插件说明和相应的配置https://github.com/webpack/docs/wiki/list-of-plugins 你可以去了解去使用你需要的plugins 下面提供一个webpack官方提供的analyse功能 http://webpack.github.io/analyse/ 他会根据你的js文件分析你的项目的package和文件之间的关系 这会需要我们上传一个json文件 我们可以在项目中去生成这个json文件: $ webpack --profile --json &gt; status.json 这会将我们webpack整个profile生成一个status.json 到时我们上传这个status.json文件就可以分析我们整个项目的结构了 :bowtie: ","link":"https://GeekGhc.github.io/post/webpack-ji-ben-shi-yong/"},{"title":"Git在开发中基本应用","content":"Git版本管理在实际开发应用中对代码管理是一个非常且必备的技能 无论是对于个人开发还是团队开发都是一个值得学习的管理方案 前言 Git作为一个版本管理工具 对于程序员来说应该是相当熟悉 无论是对于个人开发者还是对于团队的开发都是必不可少的工具和技能 就比如这个jekyll比博客系统也是通过Git部署到Github上的 对于代码管理国内国外都有比较好的平台和服务 当然最为广泛肯定就是Github了 国内的话Coding的用户也是很多的 无论是对于哪个平台 满足我们的需求的就是最好的 Git安装配置 Git的安装的话其实网上找到自己的环境 下载之后安装就行了 1.基本配置 安装完成后我们需要去进行简单的配置 在终端执行 $ git config 我们可以看到下面的几个flag --global --system --local 其实这三个也就是对应着3个不同的层级吧 对于用户,系统,具体项目 当然我们在自己的开发环境中当然是配置--global`主要就是对我们当然的用户而言 是一个全局性的配置 接着我们配置一下我们的user.name 和user.email这个我们就用我们Github账号的用户名和邮箱 $ git config --global user.name &quot;JellyBean&quot; 这里的用户名填写自己的用户名就行了 接着就是配置user.email： $ git config --global user.email = &quot;gehuachun@outlook.coom&quot; 这里还是填写你自己邮箱账户就可以了 你也可以去查看自己的用户名和邮箱 $ git config --global user.name 这个时候应该就是你之前配置的用户名了 当然其实我们所配置的信息 你也可以直接去Home目录下查看(Windows环境也是一样 因为我们是配置的是global) $ cat ~/.gitconfig 这个时候你就可以看到刚才你所配置的具体的信息了 这个配置文件里面就是对git的具体的配置 其实对于Git的配置还有很多 具体的等遇到什么样的需求 如果说是对每一个项目的配置 我们在进入项目中执行 $ git init 我们就可以在生成的**.git**目录下的config里面看到对于这个项目的git的具体配置 Git的基本管理流程 在我们开发一个项目时 我们可以在项目里 $ git init 初始化项目的git管理 这样就会在项目里生成对应的**.git**目录 而里面就是具体的配置 在一开始我们可以在Github上新建一个项目仓库 然后我们就可以进行之后的代码上传部署 而在上传之前 我们得明白Git的工作流程 首先我们并不是一次性就提交我们的代码 这边其实可以分成三块 一个就是我们工作的项目块 也就是存放我们编写的代码 在中间会有一个类似缓存区这样的一个区域块 最后才是最后提交的一个HEAD区域 这里就是你最后提交的结果 只有在这个里面才能提交到远程仓库中 开始时我们可以去创建一个文件 $ echo &quot;&lt;?php phpinfo();&quot; &gt;&gt; index.php 这里我们生成了一个index.php的文件 我们去查看一下git状态 $ git status 我们就可以看到git会提示我们有一个文件需要进行跟踪 因为我们还没有加入到缓存区 所以开始我们可以去将代码加入到缓存区 $ git add . 在执行完毕之后我们再去执行git status 就看到已经没有之前的提示 说明git已经跟踪我们刚才所添加的index.php文件了 这里的git add .是将目录下的所有文件全都加入到缓冲区 .起到一个通配符的作用 当然也有* 例如我们需要将src目录下的php文件写入缓存区 $ git add src/*.php 接下来就是写入到HEAD区域块中 $ git commit -m &quot;first commit&quot; 这个也就可以将文件放入到HEAD区域块中 之后我们才可以进行文件上传提交 在提交之前我们需要去加入我们的远程仓库的 这个在创建远程仓库的时候就可以看到 所以这个时候我们就可以提交文件 $ git push origin master 如果不想要每次提交都需要输入密码的话 可以去生成一个ssh key这个对于Github和Coding都是一样的 包括我们在自己的服务器上去部署代码的时候我们也是可以去在服务器里生成对应的key这样加入到Github后我们就可以直接提交而不用输入密码了 其实大概的基本流程就是这样 .gitignore说明 在创建远程仓库时会有这个选择 当然对于每个项目的创建这个.gitignore是不同的 我们自己也可以进行相应的配置 他的作用明义上就是忽略掉不必要的文件 这里的不必要并不是真的不必要 这些文件就是包含一些敏感信息数据的文件 比如Laravel中的.env文件 因为里面包含了很多开发者的私密数据 这些肯定不会暴露给别的人 还有的就是我们可以通过npm install去下载的包文件 以及通过gulp去生成的文件 因为这些文件我们完全可以在部署的时候去生成 不然的话整个项目就太繁重了 上传的效率也不是很高 而且显得很没有必要 Git分支的应用 首先提出这样的一个场景: 我们在项目里面已经上线了部分的功能 我们正在开发其他的功能板块 这个时候你发现自己原先的项目里面出现了一个很明显的Bug 如果这个时候我们去修复这个Bug我们正在开发的功能板块还没有完成所以这个时候我们是不能推送我们的代码的 这个时候的矛盾就是我们需要去修复之前项目里的Bug但是我们又不好把半成品的功能板块推送上去 所以这个时候我们在开发我们新的功能板块时 需要去创建新的分支 $ git branch new-branch 这里我们就创建了一个新的分支new-branch 我们可以通过执行命令查看 $ git branch 这样就会罗列出当前项目里面所有的分支了 我们看到的matser就是当前分支 因为它前面会有一个*号 创建好了分支 我们可以进入我们的分支 $ git checkout new-branch 这样我们就可以进入到刚生成的分支里了 当然我们在创分支时 可以创建并进入该分支 $ git checkout -b new-branch 这个其实就是上面两个步骤的结合 其实这里的分支可以这么理解 就是你上线了项目的部分功能 已经投入使用 那么在线下 你需要去添加新的功能板块时就去创建一个分支 在这个新的分支里去实现功能代码逻辑 一旦之前已经上线的项目出现了Bug那么你s首先在新的分支里去提交增加的文件 然后切换到master分支去修复Bug而不会影响到新的分支 因为新的分支里正在实现新的功能板块还没有完成 并且已经commit了 修复完Bug后你就可以去提交以完成当前项目的Bug修复这个不会推送新分支里的代码 之后你完成了新的功能再将新的分支合并到master分支 这样再一起推送到远程仓库 这样就不会出现问题了 即使之前的项目出现Bug我们也可以在不影响正在开发的新的功能板块 我们可以去在命令行执行 $ git log 我们会发现只有master分支下的提交信息 而不会有新分支下的提交信息 在完成新的功能板块 切换到mater分支后我们就可以去合并我们新的分支 $ git merge new-branch 在完成新的功能后如果没有什么需要修复更改的我们就可以去删除这个分支 $ git branch -d new-branch Git合并冲突 我们在合并其他分支时很有可能会同时修改了同一段代码 这样两个分支的代码片段不一致 这样就会产生合并冲突 当我们去合并新分支时 $ git merge new-branch 会给出一个冲突提示 我们可以查看一下冲突的具体情况 如发生冲突在test.txt文件中 $ git diff test.txt 我们就可以看到冲突的具体的情况 而我们需要去手动解决这些冲突 因为两个分支的片段取舍完全取决于我们 修改完冲突之后 我们再去提交推送就可以了 顺便说一下的就是git reset --soft xxx和git reset --hard xxx的区别 xxx是git log显示的每条提交记录的hash值 其中soft和hard的区别就是一个会撤销提交记录 而另一个不仅会撤销提交记录代码数据也会退回到之前的时间点 git stash解决修复Bug 还是这个前提 当你正在专注于功能的开发时 有人突然告诉你要你去修改上线的Bug而这个Bug可能和你正在开发的其他模块相关 Oh No~ 又是这个问题 好吧git stash就可以将我们目前的工作区域暂存起来 这样我们就可以去做其他事 比如去修复下蛋疼的Bug 这个我们还是去创建一个新的分支去开发新的功能模块 这个和我们之前有所不同的是 如果新的分支的内容没有commit 他是会影响到master分支的 我们可以试着在新的分支里去写一些新的代码块 然后我们切换到matser分支 这个时候我们git status去看一下git当前的状态 我们会发现 我们是可以看到在新的分支里面的文件还没有添加到缓存区这样的信息的 所以我们还是切换到新的分支 去执行 $ git stash 这个时候我们可以在新的分支里去查看一下当前的状态 $ git status 我们会发现这里是没有需要commit的内容的 我们再回到mater分支 我们再次执行 $ git status 我们发现也看不到需要提交的内容 这个看起来是不是正是我们所需要的结果 这个时候我们就可以在matser分支里做我们想做的事 比如去修复一下需要修改的Bug 修复完后 我们还得去新的分支继续我们的工作 我们需要去拿回我们之前stash的内容 这个理解起来大概就是你正在办公桌忙着今天的工作 突然经理过来有给你一个新的紧急任务 没办法 这个任务十分紧急 你还是得马上去处理 所以我们只得把手上的工作先放到一边 先把经理交给我们的事先解决 解决完成之后 好嘛 今天的工作还没做完 我们又得重新拿起今天的工作任务再拿过来开始做 当然在拿回我们今天的工作任务时 会有和git stash相反的操作 就是git stash apply和git stash pop 这两者之间的区别就是我们可以通过执行 $ git stash list 当然如果是git stash apply 我们是可以看到stash的一条记录 这条记录包含了一个stash id标记着每条stash记录 但是如果是执行git stash pop的话不仅会拿回我们stash的数据 还会将这条stash记录删除 这个有点像数据结构里面的栈的top和pop的区别 一个是只是返回数据 一个不仅返回数据而且会删除掉栈顶元素 哈哈 这样理解起来还是可以接受的吧 😆 当然如果你只是执行git stash apply后想删除掉那条stash记录的话 我们去drop掉那条记录 $ git stash drop 所以这个时候你去执行git stash list是看不到之前的那条记录的 git rebase的应用 首先我们在master分支里去git add提交下信息然后去生成一个新的分支 $ git checkout -b new-branch 我们在切换到新的分支后 我们去执行 $ git log 这样的话我们是可以看到在master上的提交信息的 如果这个时候我们在新的分支上修改和增加代码 我们去提交后 我们再次git log这样一来我们会多出一条信息 切换到master分支 我们像往常一样merge新的分支 再去执行git log 我们会看到会多出一条merge的信息 这实际上是因为我们在中途新增加了一条新的分支 所以我们才会有这么一条merge信息 往往团队成员多的时候 我们想整个开发过程就像在一条历史线上 每一个commit都在一个时间轴上 这样也可以保证了提交记录的整洁 git 发布版本 我们正开发第三方插件供别人使用时 我们可能过程在别人提出的issue后对扩展包进行改进或者功能的增加 那么我们会针对过程发布应用 包的版本也就是tag 我们在执行git push后将自己的代码提交到仓库 这个时候我们的代码已经可以使用 难么我们可以就可以为现在的代码提供一个版本也就是打上一个标签 在push完成之后 我们再去执行: $ git tag 1.0 -a 执行完毕后会我们就可以在vim里去写我们这次的commit message(发布说明) 添加完毕之后再去执行: $ git push --tags 这样一来我们就把我们这次的版本推送上去了 也就是1.0的版本 后期再有改善的话还是这样 这样就很清楚这个扩展包的还是生命流程 如果需要删除一个tag的话 我们需要将本地和远程的tag都删除掉 首先我们可以查看一下本地当前分支的tag情况 $ git tag 接着如果我们需要删除其中的一个1.0的tag时 $ git tag -d 1.0 删除远程的tag的话(可以理解为推送一个空的tag): $ git push origin --delete tag 1.0 这样我们就删除了tag为1.0的这个分支了 必要时也可以去选择适应哪一个版本的包 ","link":"https://GeekGhc.github.io/post/git-zai-kai-fa-zhong-ji-ben-ying-yong/"},{"title":"gulpJs使用技巧介绍","content":"gulpjs是一个前端构建工具，与gruntjs相比，gulpjs无需写一大堆繁杂的配置参数，API也非常简单，学习起来 1.gulp安装 1.首先确保你已经正确安装了nodejs环境。然后可以全局方式安装gulp： $ npm install -g gulp 我们可以检查一下gulp版本 $ gulp -v 这样就完成了对全局的安装 2.如果想在安装的时候把gulp写进项目package.json文件的依赖中，则可以加上--save-dev： $ npm install --save-dev gulp 其中--save-dev和--save的区别这里也有清楚的解释 这其实在composer安装依赖包是一样的 一个存在require一个存在require-dev 2.开始使用gulp 1.和其他的构建工具一样gulpjs也需要一个相应的配置文件gulpfile.js 执行 $ touch gulpfile.js 2.首先是一个简单的egulpfile.js内容: var gulp = require('gulp'); gulp.task('default',function(){ // 将你的默认的任务代码放在这 }); 3.运行gulp $ gulp 要运行gulp任务，只需切换到存放gulpfile.js文件的目录，然后在命令行中执行gulp命令就行了，gulp后面可以加上要执行的任务名，例如gulp task1，如果没有指定任务名，则会执行任务名为default的默认任务 这里默认的名为 default 的任务（task）将会被运行，但是这个任务并未做任何事情。 如果想要单独执行特定的任务，请输入 gulp &lt;task&gt; &lt;othertask&gt; 3.gulp API使用 gulp只有五个方法:task run watch src dest 1.gulp.src(globs[, options]) globs参数是文件匹配模式(类似正则表达式),他的类型是String或Array,用来匹配文件路径(包括文件名)，当然这里也可以直接指定某个具体的文件路径。当有多个匹配模式时，该参数可以为一个数组。 options为可选参数。通常情况下我们不需要用到。 我们这里简单可以理解为这个方法就是读取你需要操作的文件的 Gulp内部使用了node-glob模块来实现其文件匹配功能。 * 匹配文件路径中的0个或多个字符，但不会匹配路径分隔符，除非路径分隔符出现在末尾 ** 匹配路径中的0个或多个目录及其子目录,需要单独出现，即它左右不能有其他东西了。如果出现在末尾，也能匹配文件。 ? 匹配文件路径中的一个字符(不会匹配路径分隔符) [...] 匹配方括号中出现的字符中的任意一个，当方括号中第一个字符为^或!时，则表示不匹配方括号中出现的其他字符中的任意一个，类似js正则表达式中的用法 !(pattern|pattern|pattern) 匹配任何与括号中给定的任一模式都不匹配的 ?(pattern|pattern|pattern) 匹配括号中给定的任一模式0次或1次，类似于js正则中的(pattern|pattern|pattern)? +(pattern|pattern|pattern) 匹配括号中给定的任一模式至少1次，类似于js正则中的(pattern|pattern|pattern)+ *(pattern|pattern|pattern) 匹配括号中给定的任一模式0次或多次，类似于js正则中的(pattern|pattern|pattern)* @(pattern|pattern|pattern) 匹配括号中给定的任一模式1次，类似于js正则中的(pattern|pattern|pattern) 当有多个匹配规则时 可以传入数组 如: //使用数组的方式来匹配多种文件 gulp.src(['js/*.js','css/*.css','*.html']) 除此之外 数组还可以进行排除的匹配(ps:数组的第一个元素不能进行排除模式) gulp.src([*.js,'!a*.js']) // 匹配所有js文件，但排除掉以a开头的js文件 gulp.src(['!a*.js',*.js]) //不会排除任何文件，因为排除模式不能出现在数组的第一个元素中 2.gulp.dest(path[, options]) 简单的说**gulp.dest()**是用来写文件的 path为写入文件的路径 options为一个可选的参数对象，通常我们不需要用到 gulp的运行流程大致是这样的: gulp的使用流程一般是这样子的：首先通过gulp.src()方法获取到我们想要处理的文件流， 然后把文件流通过pipe方法导入到gulp的插件中，最后把经过插件处理后的流再通过pipe方法导入到gulp.dest()中， gulp.dest()方法则把流中的内容写入到文件中，这里首先需要弄清楚的一点是， 我们给gulp.dest()传入的路径参数，只能用来指定要生成的文件的目录，而不能指定生成文件的文件名， 它生成文件的文件名使用的是导入到它的文件流自身的文件名，所以生成的文件名是由导入到它的文件流决定的， 即使我们给它传入一个带有文件名的路径参数，然后它也会把这个文件名当做是目录名，例如： var gulp = require('gulp'); gulp.src('script/jquery.js') .pipe(gulp.dest('dist/foo.js')); //最终生成的文件路径为 dist/foo.js/jquery.js,而不是dist/foo.js 通过指定gulp.src()方法配置参数中的base属性，我们可以更灵活的来改变gulp.dest()生成的文件路径。 当我们没有在gulp.src()方法中配置base属性时，base的默认值为通配符开始出现之前那部分路径，例如： gulp.src('app/src/**/*.css') //此时base的值为 app/src gulp.src()的bade属性可以在options里指定 gulp.src('client/js/**/*.js', { base: 'client' }) .pipe(minify()) .pipe(gulp.dest('build')); // 写入 'build/js/somedir/somefile.js' gulp.src(script/lib/*.js, {base:'script'}) //配置了base参数，此时base路径为script //假设匹配到的文件为script/lib/jquery.js .pipe(gulp.dest('build')) //此时生成的文件路径为 build/lib/jquery.js 我们可以这样理解: 上面我们说的gulp.dest()所生成的文件路径的规则，其实也可以理解成，用我们给gulp.dest()传入的路径替换掉gulp.src()中的base路径，最终得到生成文件的路径。 3.gulp.task(name[, deps], fn) name 为任务名(请不要在名字中使用空格) deps 是当前定义的任务需要依赖的其他任务，为一个数组。当前定义的任务会在所有依赖的任务执行完毕后才开始执行。如果没有依赖，则可省略这个参数 fn 为任务函数，我们把任务要执行的代码都写在里面。该参数也是可选的。 gulp中执行多个任务，我们的项目里肯定会有处理类似css js images fonts 这样的静态文件的几个任务 可以通过任务依赖来实现。例如我想要执行one,two,three这三个任务，那我们就可以定义一个空的任务，然后把那三个任务当做这个空的任务的依赖就行了： //只要执行default任务，就相当于把css,images,scripts这三个文件任务执行了 gulp.task('default',['css','images','scripts']); 如果任务相互之间没有依赖，任务会按你书写的顺序来执行，如果有依赖的话则会先执行依赖的任务。 在处理所依赖的任务是异步的这样的应用场景也是有几种解决方案的： 依赖任务异步执行 4.gulp.watch(glob [, opts], tasks) 或 gulp.watch(glob [, opts, cb]) **gulp.watch()**用来监视文件的变化，当文件发生变化后，我们可以利用它来执行相应的任务，例如文件压缩等。 glob 为要监视的文件匹配模式，规则和用法与gulp.src()方法中的glob相同。 opts 为一个可选的配置对象，通常不需要用到 tasks 为文件变化后要执行的任务，为一个数组 每当监视的文件发生变化时，就会调用这个函数,并且会给它传入一个对象，该对象包含了文件变化的一些信息，type属性为变化的类型，可以是added,changed,deleted；path属性为发生变化的文件的路径 gulp.watch('js/**/*.js', function(event){ console.log(event.type); //变化类型 added为新增,deleted为删除，changed为改变 console.log(event.path); //变化的文件的路径 }); gulp.watch('js/**/*.js', function(event) { console.log('File ' + event.path + ' was ' + event.type); }); 4.gulp 插件使用 gulp 插件库 1.自动加载 gulp-load-plugins 安装: npm install --save-dev gulp-load-plugins 在使用gulp插件时都需要require进来 而这个插件很好的解决了这个问题 gulp-load-plugins并不会一开始就加载所有package.json里的gulp插件，而是在我们需要用到某个插件的时候，才去加载那个插件。 因为gulp-load-plugins是通过你的package.json文件来加载插件的，所以必须要保证你需要自动加载的插件已经写入到了package.json文件里，并且这些插件都是已经安装好了的 下面这是一段一段很方便使用其他插件的load-plugins代码(其实就是匹配到package.json里的插件): var plugins = require(&quot;gulp-load-plugins&quot;)({ pattern: ['gulp-*', 'gulp.*'], replaceString: /\\bgulp[\\-.]/ }); 这样就可以通过 plugins.name()来使用我们的插件 举一个简单的使用gulp-rename这个插件的例子 gulp.task('one',function () { gulp.src(paths.styles.src+'/one.css') .pipe(plugins.rename('new.css')) //而不用声明类似var rename = require('gulp-rename') .pipe(gulp.dest(paths.styles.dest)); }); 2.重命名gulp-rename 安装：npm install --save-dev gulp-rename var rename = require('gulp-rename'); //最后将src/styles/one.css 生成到 assets/styles/new.css gulp.task('one',function () { gulp.src('src/styles/one.css') .pipe(rename('new.css')) .pipe(gulp.dest('asstes/styles')); }); 3.js文件压缩 gulp-uglify 安装：npm install --save-dev gulp-uglify var gulp = require('gulp'), uglify = require(&quot;gulp-uglify&quot;); gulp.task('minify-js', function () { gulp.src('src/scripts/*.js') // 要压缩的js文件 .pipe(uglify()) //使用uglify进行压缩 .pipe(gulp.dest('assets/js')); //压缩后的路径 }); 4.文件合并 gulp-concat 安装：npm install --save-dev gulp-concat var gulp = require('gulp'), concat = require(&quot;gulp-concat&quot;) uglify = require(&quot;gulp-uglify&quot;); //如果src/scripts下有one.js two.js three.js 那么最后合并到assets/js/all.js gulp.task('concat', function () { gulp.src('src/scripts/*.js') //要合并的文件 .pipe(uglify()) //使用uglify进行压缩 .pipe(concat('all.js')) // 合并匹配到的js文件并命名为 &quot;all.js&quot; .pipe(gulp.dest('assets/js')); }); 5.less和sass的编译 安装：npm install --save-dev gulp-less npm install --save-dev gulp-sass var gulp = require('gulp'), less = require(&quot;gulp-less&quot;); gulp.task('compile-less', function () { gulp.src('src/less/*.less') .pipe(less()) .pipe(gulp.dest('assets/css')); }); 当然还有其他非常有用插件 gulp 插件库 相关资料参考于: gulp Api文档 前端构建工具gulp ","link":"https://GeekGhc.github.io/post/gulpjs-shi-yong-ji-qiao-jie-shao/"}]}